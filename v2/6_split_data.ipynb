{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444aaf44-4153-4de4-8240-eb64e8d9769e",
   "metadata": {},
   "source": [
    "# Create Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb485cc1-fb1b-4711-9c0f-15bd2895d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black==22.6.0\n",
      "boto3==1.24.61\n",
      "dask==2022.8.0\n",
      "dask-ml==2022.5.27\n",
      "distributed==2022.8.0\n",
      "nb-black==1.0.7\n",
      "openpyxl==3.0.10\n",
      "pandas==1.4.3\n",
      "s3fs==0.4.2\n",
      "scikit-learn==1.1.2\n",
      "ipykernel                 6.15.1             pyh210e3f2_0    conda-forge\n",
      "CPU times: user 32.2 ms, sys: 19.1 ms, total: 51.3 ms\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip3 freeze | grep -E 'boto3|s3fs|scikit-learn|distributed|dask==|dask-m|black==|jupyter-server|pandas|openpyxl'\n",
    "!conda list -n spark | grep -E 'ipykernel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5ede44-eaa0-4209-9652-e85ae11c2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e1ff41-c5cc-4b80-8fb6-5e970e8b8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import boto3\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split as sk_train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a47d23f-5aca-4157-8f66-667e4fbcdf38",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358b6aa-36c9-4838-9713-17b8ee277dd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Objective\n",
    "This notebook will split the processed data into training, validation and test splits that can be used to train a machine learning model for twitter sentiment classification. Since `dask-ml` provides a convenient method (`train_test_split()`, [link](https://ml.dask.org/modules/generated/dask_ml.model_selection.train_test_split.html#dask-ml-model-selection-train-test-split)) for creating data splits.\n",
    "\n",
    "### ML Model Development\n",
    "A random sample of the training split will be further divided into three smaller splits in order to support training a NLP (transformers) model to predict sentiment. This NLP model will be used to label the processed tweets data with sentiment. The NLP model will be used to label the data (i.e. to extract the sentiment) used during ML model development. The ML model will be trained using this labeled data and then deployed.\n",
    "\n",
    "### ML Model Usage in Production\n",
    "In production, the deployed ML model will be used to predict sentiment of incoming tweets on-demand. These predictions will be served to customers.\n",
    "\n",
    "### ML Model Drift Monitoring\n",
    "After the same fraction of inference predictions have been made as the size of the validation of test splits used during ML model development, the following will be performed\n",
    "- all tweets predicted during inference are labeled\n",
    "  - manually\n",
    "  - using the **previously trained NLP model**\n",
    "- deployed ML model predictions, made during inference, will be scored against these labels (previous bullet point) in order to determine if ML model performance has\n",
    "  - drifted (the ML training pipeline will be triggered)\n",
    "    - scores are not within some threshold of the scores on the test split during ML model development\n",
    "    - predictions made by the **previously trained NLP model** will be served to the customer\n",
    "      - the other option here is to serve the same (poorly scoring) predictions made by the ML model to the customers\n",
    "    - updated training, validation and testing splits will be created using *all available data*\n",
    "    - a new ML model will be trained using *all available data* and will then be deployed to production\n",
    "  - not drifted\n",
    "    - scores are within some threshold of the scores on the test split during ML model development\n",
    "    - the currently used ML model will continue to serve inference\n",
    "\n",
    "*All available data* here will include\n",
    "- the original data used for ML model development\n",
    "- the new data used to make inference with the originally trained ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da003d84-d869-412e-b32e-87e95d72c710",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0114977-12b2-4d51-9b43-2182f5367d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folder = \"/datasets/twitter/kinesis-demo/\"\n",
    "\n",
    "# processed data\n",
    "processed_data_dir = \"data/processed\"\n",
    "processed_file_name = \"processed_text\"\n",
    "\n",
    "# train-test split\n",
    "test_split_frac = 0.125\n",
    "\n",
    "# sampling data\n",
    "nlp_sample_size = 0.3333\n",
    "sampled_fname = \"sampled_data.csv.zip\"\n",
    "nlp_cols = [\"id\", \"created_at\", \"text\"]\n",
    "\n",
    "# inference\n",
    "inference_start_date = \"2022-01-10 00:00:00\"\n",
    "\n",
    "upload_to_s3 = True\n",
    "create_nlp_splits = True\n",
    "cleanup_local_files = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44af9bd9-bb26-402a-a106-3d37bdc81f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = os.getenv(\"AWS_S3_BUCKET_NAME\", \"sagemakertestwillz3s\")\n",
    "session = boto3.Session(profile_name=\"default\")\n",
    "s3_client = session.client(\"s3\")\n",
    "\n",
    "dtypes_dict = {\n",
    "    \"id\": pd.StringDtype(),\n",
    "    \"geo\": pd.StringDtype(),\n",
    "    \"coordinates\": pd.StringDtype(),\n",
    "    \"place\": pd.StringDtype(),\n",
    "    \"contributors\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"is_quote_status\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"quote_count\": pd.Int32Dtype(),\n",
    "    \"reply_count\": pd.Int32Dtype(),\n",
    "    \"retweet_count\": pd.Int32Dtype(),\n",
    "    \"favorite_count\": pd.Int32Dtype(),\n",
    "    \"favorited\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"retweeted\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"source\": pd.StringDtype(),\n",
    "    \"in_reply_to_user_id\": pd.StringDtype(),\n",
    "    \"in_reply_to_screen_name\": pd.StringDtype(),\n",
    "    \"source_text\": pd.StringDtype(),\n",
    "    \"place_id\": pd.StringDtype(),\n",
    "    \"place_url\": pd.StringDtype(),\n",
    "    \"place_place_type\": pd.StringDtype(),\n",
    "    \"place_name\": pd.StringDtype(),\n",
    "    \"place_full_name\": pd.StringDtype(),\n",
    "    \"place_country_code\": pd.StringDtype(),\n",
    "    \"place_country\": pd.StringDtype(),\n",
    "    \"place_bounding_box_type\": pd.StringDtype(),\n",
    "    \"place_bounding_box_coordinates\": pd.StringDtype(),\n",
    "    \"place_attributes\": pd.StringDtype(),\n",
    "    \"coords_type\": pd.StringDtype(),\n",
    "    \"coords_lon\": pd.StringDtype(),\n",
    "    \"coords_lat\": pd.StringDtype(),\n",
    "    \"geo_type\": pd.StringDtype(),\n",
    "    \"geo_lon\": pd.StringDtype(),\n",
    "    \"geo_lat\": pd.StringDtype(),\n",
    "    \"user_name\": pd.StringDtype(),\n",
    "    \"user_screen_name\": pd.StringDtype(),\n",
    "    \"user_followers\": pd.Int32Dtype(),\n",
    "    \"user_friends\": pd.Int32Dtype(),\n",
    "    \"user_listed\": pd.Int32Dtype(),\n",
    "    \"user_favourites\": pd.Int32Dtype(),\n",
    "    \"user_statuses\": pd.Int32Dtype(),\n",
    "    \"user_protected\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"user_verified\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"user_contributors_enabled\": pd.StringDtype(),\n",
    "    \"user_location\": pd.StringDtype(),\n",
    "    \"retweeted_tweet\": pd.StringDtype(),\n",
    "    \"tweet_text_urls\": pd.StringDtype(),\n",
    "    \"tweet_text_hashtags\": pd.StringDtype(),\n",
    "    \"tweet_text_usernames\": pd.StringDtype(),\n",
    "    \"num_urls_in_tweet_text\": pd.Int32Dtype(),\n",
    "    \"num_users_in_tweet_text\": pd.Int32Dtype(),\n",
    "    \"num_hashtags_in_tweet_text\": pd.Int32Dtype(),\n",
    "    \"text\": pd.StringDtype(),\n",
    "    \"contains_wanted_text\": pd.BooleanDtype(),\n",
    "    \"contains_wanted_text_case_sensitive\": pd.BooleanDtype(),\n",
    "    \"contains_multi_word_wanted_text\": pd.BooleanDtype(),\n",
    "    \"contains_crypto_terms\": pd.BooleanDtype(),\n",
    "    \"contains_religious_terms\": pd.BooleanDtype(),\n",
    "    \"contains_inappropriate_terms\": pd.BooleanDtype(),\n",
    "    \"contains_video_games_terms\": pd.BooleanDtype(),\n",
    "    \"contains_misc_unwanted_terms\": pd.BooleanDtype(),\n",
    "    \"contains_non_english_terms\": pd.BooleanDtype(),\n",
    "    \"text_trimmed\": pd.StringDtype(),\n",
    "    \"text_stripped\": pd.StringDtype(),\n",
    "    \"text_processed\": pd.StringDtype(),\n",
    "    \"words\": pd.StringDtype(),\n",
    "    \"num_words\": pd.Int32Dtype(),\n",
    "}\n",
    "\n",
    "proc_text_zip_fname = f\"{processed_file_name}.zip\"\n",
    "\n",
    "val_split_frac = test_split_frac / (1 - test_split_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3995b943-2fec-4c28-b11d-94e448757b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_cols(df_cols, cols_to_use):\n",
    "    \"\"\"Highlight a list of columns in a DataFrame.\"\"\"\n",
    "    # copy df to new - original data is not changed\n",
    "    df = df_cols[cols_to_use].copy()\n",
    "    # select all values to yellow color\n",
    "    df.loc[:, :] = \"background-color: yellow\"\n",
    "    # return color df\n",
    "    return df\n",
    "\n",
    "\n",
    "def download_file_from_s3(\n",
    "    s3_bucket_name: str,\n",
    "    path_to_folder: str,\n",
    "    data_dir: str,\n",
    "    fname: str,\n",
    "    aws_region: str,\n",
    "    prefix: str,\n",
    ") -> None:\n",
    "    \"\"\"Download file from .\"\"\"\n",
    "    dest_filepath = os.path.join(data_dir, fname)\n",
    "    s3_filepath_key = s3_client.list_objects_v2(\n",
    "        Bucket=s3_bucket_name,\n",
    "        Delimiter=\"/\",\n",
    "        Prefix=prefix,\n",
    "    )[\"Contents\"][0][\"Key\"]\n",
    "    start = datetime.now()\n",
    "    print(\n",
    "        f\"Started downloading processed data zip file from {s3_filepath_key} to \"\n",
    "        f\"{dest_filepath} at {start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}...\"\n",
    "    )\n",
    "    s3 = boto3.resource(\"s3\", region_name=aws_region)\n",
    "    s3.meta.client.download_file(\n",
    "        s3_bucket_name,\n",
    "        s3_filepath_key,\n",
    "        dest_filepath,\n",
    "    )\n",
    "    duration = (datetime.now() - start).total_seconds()\n",
    "    print(f\"Done downloading in {duration:.3f} seconds.\")\n",
    "\n",
    "\n",
    "def extract_zip_file(dest_filepath: str, data_dir: str) -> None:\n",
    "    \"\"\".\"\"\"\n",
    "    start = datetime.now()\n",
    "    print(\n",
    "        \"Started extracting filtered data parquet files from \"\n",
    "        f\"processed data zip file to {data_dir} at \"\n",
    "        f\"{start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}...\"\n",
    "    )\n",
    "    zip_ref = zipfile.ZipFile(dest_filepath)\n",
    "    zip_ref.extractall(data_dir)\n",
    "    zip_ref.close()\n",
    "    duration = (datetime.now() - start).total_seconds()\n",
    "    print(f\"Done extracting in {duration:.3f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c850312-4502-4ee7-8f94-9bfb7a1322f2",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07be3d1-0e0c-4df1-ad57-4cb1bf9cbbba",
   "metadata": {},
   "source": [
    "We will start by downloaded the processed and filtered `.zip` file from S3 and extracting all the contained `.parquet` files into a `.parquet.gzip` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6e6661-fc45-421e-8a00-0ad4961a6734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.03 ms, sys: 254 µs, total: 1.29 ms\n",
      "Wall time: 797 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists(os.path.join(processed_data_dir, proc_text_zip_fname)):\n",
    "    download_file_from_s3(\n",
    "        s3_bucket_name,\n",
    "        path_to_folder,\n",
    "        processed_data_dir,\n",
    "        proc_text_zip_fname,\n",
    "        session.region_name,\n",
    "        f\"{path_to_folder[1:]}processed/{os.path.splitext(proc_text_zip_fname)[0]}\",\n",
    "    )\n",
    "    extract_zip_file(\n",
    "        os.path.join(processed_data_dir, proc_text_zip_fname),\n",
    "        f\"{processed_data_dir}/{os.path.splitext(proc_text_zip_fname)[0]}.parquet.gzip\",\n",
    "    )\n",
    "proc_files = glob(f\"{processed_data_dir}/*.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de4c25-93fe-49c9-9e17-052bdd9f6447",
   "metadata": {},
   "source": [
    "Find the number of individual `.parquet.gzip` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc570dbe-52d9-42f0-be9c-5d0e5ad79fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "proc_files_all = glob(f\"{processed_data_dir}/*.parquet.gzip/*.gz.parquet\")\n",
    "print(len(proc_files_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa7314-4451-4f05-afb4-53b57b700578",
   "metadata": {},
   "source": [
    "Use Dask to load the `.parquet.gzip` file (consisting of multiple `.parquet` files) into a single Dask DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02d444a-06c4-484a-b692-d5661cb9c024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TRYING dask.delayed with pd.read_parquet (.sort_values() errored out)\n",
    "# # %%time\n",
    "# from collections import OrderedDict\n",
    "# from dask import delayed\n",
    "# delayed_dfs = [\n",
    "#     delayed(pd.read_parquet)(f).astype(dtypes_dict).sort_values(by=[\"created_at\"])\n",
    "#     for f in proc_files_all\n",
    "# ]\n",
    "# ddf = (\n",
    "#     dd.from_delayed(delayed_dfs)\n",
    "#     .set_index('created_at')\n",
    "#     # .sort_values(by=[\"created_at\"])\n",
    "#     .reset_index(drop=True)\n",
    "#     .repartition(npartitions=len(proc_files_all))\n",
    "# )\n",
    "# print(ddf.npartitions)\n",
    "# with pd.option_context(\"display.max_columns\", None):\n",
    "#     display(ddf.head())\n",
    "# with pd.option_context(\"display.max_colwidth\", None, \"display.max_rows\", None):\n",
    "#     display(ddf.dtypes.rename(\"dtype\").to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb7720b-e352-4d9d-8258-ccad26f97712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TRYING dask.concat with pd.read_parquet (.sort_values() errored out)\n",
    "# # %%time\n",
    "# ddf = dd.multi.concat(\n",
    "#     [\n",
    "#         dd.from_pandas(\n",
    "#             pd.read_parquet(f).astype(dtypes_dict).sort_values(by=[\"created_at\"]),\n",
    "#             npartitions=1\n",
    "#         )\n",
    "#         for f in proc_files_all\n",
    "#     ], axis=1, interleave_partitions=False\n",
    "# ).sort_values(by=[\"created_at\"]).repartition(npartitions=len(proc_files_all))\n",
    "# print(ddf.npartitions)\n",
    "# with pd.option_context(\"display.max_columns\", None):\n",
    "#     display(ddf.head())\n",
    "# with pd.option_context(\"display.max_colwidth\", None, \"display.max_rows\", None):\n",
    "#     display(ddf.dtypes.rename(\"dtype\").to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd70cd79-4059-4e9d-bfa0-308b7aedce30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed data from *.parquet.gzip files into Dask DataFrame with 8 partitions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>geo</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>contributors</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>source</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>source_text</th>\n",
       "      <th>place_id</th>\n",
       "      <th>place_url</th>\n",
       "      <th>place_place_type</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_full_name</th>\n",
       "      <th>place_country_code</th>\n",
       "      <th>place_country</th>\n",
       "      <th>place_bounding_box_type</th>\n",
       "      <th>place_bounding_box_coordinates</th>\n",
       "      <th>place_attributes</th>\n",
       "      <th>coords_type</th>\n",
       "      <th>coords_lon</th>\n",
       "      <th>coords_lat</th>\n",
       "      <th>geo_type</th>\n",
       "      <th>geo_lon</th>\n",
       "      <th>geo_lat</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_listed</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_statuses</th>\n",
       "      <th>user_protected</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>user_contributors_enabled</th>\n",
       "      <th>user_joined</th>\n",
       "      <th>user_location</th>\n",
       "      <th>retweeted_tweet</th>\n",
       "      <th>tweet_text_urls</th>\n",
       "      <th>tweet_text_hashtags</th>\n",
       "      <th>tweet_text_usernames</th>\n",
       "      <th>num_urls_in_tweet_text</th>\n",
       "      <th>num_users_in_tweet_text</th>\n",
       "      <th>num_hashtags_in_tweet_text</th>\n",
       "      <th>text</th>\n",
       "      <th>contains_wanted_text</th>\n",
       "      <th>contains_wanted_text_case_sensitive</th>\n",
       "      <th>contains_multi_word_wanted_text</th>\n",
       "      <th>contains_crypto_terms</th>\n",
       "      <th>contains_religious_terms</th>\n",
       "      <th>contains_inappropriate_terms</th>\n",
       "      <th>contains_video_games_terms</th>\n",
       "      <th>contains_misc_unwanted_terms</th>\n",
       "      <th>contains_non_english_terms</th>\n",
       "      <th>text_stripped</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>text_trimmed</th>\n",
       "      <th>words</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-30 17:35:58</td>\n",
       "      <td>1476608009669201922</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[[]]</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Radio Justice 📻🎙⚖</td>\n",
       "      <td>justiceputnam</td>\n",
       "      <td>2599</td>\n",
       "      <td>1395</td>\n",
       "      <td>84</td>\n",
       "      <td>93424</td>\n",
       "      <td>193934</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2009-07-14 05:10:36</td>\n",
       "      <td>Rogue River, Oregon #homebase</td>\n",
       "      <td>no</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NASA: It wasn't a strike, it was just a work s...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NASA: It wasn't a strike, it was just a work s...</td>\n",
       "      <td>nasa  it wasn t a strike  it was just a work s...</td>\n",
       "      <td>NASA: It wasn't a strike, it was just a work s...</td>\n",
       "      <td>[NASA:, It, wasn't, a, strike,, it, was, just,...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-30 17:36:01</td>\n",
       "      <td>1476608024521453573</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[[]]</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Fabricio F. Costa</td>\n",
       "      <td>ffalconi</td>\n",
       "      <td>1623</td>\n",
       "      <td>5002</td>\n",
       "      <td>303</td>\n",
       "      <td>23</td>\n",
       "      <td>30126</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2009-03-06 05:49:14</td>\n",
       "      <td>San Francisco Area</td>\n",
       "      <td>no</td>\n",
       "      <td>https://t.co/9NeCWrTrIp</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NASA just dropped an exciting update about the...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NASA just dropped an exciting update about the...</td>\n",
       "      <td>nasa just dropped an exciting update about the...</td>\n",
       "      <td>NASA just dropped an exciting update about the...</td>\n",
       "      <td>[NASA, just, dropped, an, exciting, update, ab...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-30 17:36:03</td>\n",
       "      <td>1476608030330724357</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[[]]</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Dr. James O'Donoghue</td>\n",
       "      <td>physicsJ</td>\n",
       "      <td>148322</td>\n",
       "      <td>1081</td>\n",
       "      <td>1251</td>\n",
       "      <td>22069</td>\n",
       "      <td>21049</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2010-12-22 22:01:32</td>\n",
       "      <td>Between Mount Fuji and Tokyo</td>\n",
       "      <td>no</td>\n",
       "      <td>https://t.co/pncsLEDASF#WebbFliesAriane|https:...</td>\n",
       "      <td>Webb|WebbFliesAriane|VA256|JWST</td>\n",
       "      <td>RealtraSpace|Arianespace|ariane5|ESA_Webb|NASA...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Great footage from  camera of NASA/ESA/CSA J...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Great footage from camera of NASA/ESA/CSA Jame...</td>\n",
       "      <td>great footage from camera of nasa esa csa jame...</td>\n",
       "      <td>Great footage from  camera of NASA/ESA/CSA Jam...</td>\n",
       "      <td>[Great, footage, from, camera, of, NASA/ESA/CS...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-30 17:36:04</td>\n",
       "      <td>1476608036873682952</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[[]]</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Beyond Blue Aerospace</td>\n",
       "      <td>beyondblueaero</td>\n",
       "      <td>680</td>\n",
       "      <td>1157</td>\n",
       "      <td>65</td>\n",
       "      <td>5313</td>\n",
       "      <td>21178</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2013-11-03 02:35:32</td>\n",
       "      <td>Canada</td>\n",
       "      <td>no</td>\n",
       "      <td>https://t.co/OigaIuriN4|https://t.co/0mebmz62i0</td>\n",
       "      <td>SpaceX</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Photo of James Webb before he transformed into...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Photo of James Webb before he transformed into...</td>\n",
       "      <td>photo of james webb before he transformed into...</td>\n",
       "      <td>Photo of James Webb before he transformed into...</td>\n",
       "      <td>[Photo, of, James, Webb, before, he, transform...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-30 17:36:05</td>\n",
       "      <td>1476608040698884106</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[[]]</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Diego A Pava O</td>\n",
       "      <td>dapavao2706</td>\n",
       "      <td>364</td>\n",
       "      <td>714</td>\n",
       "      <td>1</td>\n",
       "      <td>413925</td>\n",
       "      <td>37967</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014-10-11 22:53:00</td>\n",
       "      <td>None</td>\n",
       "      <td>no</td>\n",
       "      <td>https://t.co/LAUwDoGfP3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Your internet connection over fiber optic cabl...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Your internet connection over fiber optic cabl...</td>\n",
       "      <td>your internet connection over fiber optic cabl...</td>\n",
       "      <td>Your internet connection over fiber optic cabl...</td>\n",
       "      <td>[Your, internet, connection, over, fiber, opti...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           created_at                   id   geo coordinates place  \\\n",
       "0 2021-12-30 17:35:58  1476608009669201922  None        None  None   \n",
       "1 2021-12-30 17:36:01  1476608024521453573  None        None  None   \n",
       "2 2021-12-30 17:36:03  1476608030330724357  None        None  None   \n",
       "3 2021-12-30 17:36:04  1476608036873682952  None        None  None   \n",
       "4 2021-12-30 17:36:05  1476608040698884106  None        None  None   \n",
       "\n",
       "  contributors is_quote_status  quote_count  reply_count  retweet_count  \\\n",
       "0         None            True            0            0              0   \n",
       "1         None           False            0            0              0   \n",
       "2         None           False            0            0              0   \n",
       "3         None           False            0            0              0   \n",
       "4         None           False            0            0              0   \n",
       "\n",
       "   favorite_count favorited retweeted  \\\n",
       "0               0     False     False   \n",
       "1               0     False     False   \n",
       "2               0     False     False   \n",
       "3               0     False     False   \n",
       "4               0     False     False   \n",
       "\n",
       "                                              source in_reply_to_user_id  \\\n",
       "0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...                None   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...                None   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...                None   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...                None   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...                None   \n",
       "\n",
       "  in_reply_to_screen_name          source_text place_id place_url  \\\n",
       "0                    None      Twitter Web App                      \n",
       "1                    None   Twitter for iPhone                      \n",
       "2                    None  Twitter for Android                      \n",
       "3                    None   Twitter for iPhone                      \n",
       "4                    None   Twitter for iPhone                      \n",
       "\n",
       "  place_place_type place_name place_full_name place_country_code  \\\n",
       "0                                                                  \n",
       "1                                                                  \n",
       "2                                                                  \n",
       "3                                                                  \n",
       "4                                                                  \n",
       "\n",
       "  place_country place_bounding_box_type place_bounding_box_coordinates  \\\n",
       "0                                                                 [[]]   \n",
       "1                                                                 [[]]   \n",
       "2                                                                 [[]]   \n",
       "3                                                                 [[]]   \n",
       "4                                                                 [[]]   \n",
       "\n",
       "  place_attributes coords_type coords_lon coords_lat geo_type geo_lon geo_lat  \\\n",
       "0               {}                                                              \n",
       "1               {}                                                              \n",
       "2               {}                                                              \n",
       "3               {}                                                              \n",
       "4               {}                                                              \n",
       "\n",
       "               user_name user_screen_name  user_followers  user_friends  \\\n",
       "0      Radio Justice 📻🎙⚖    justiceputnam            2599          1395   \n",
       "1      Fabricio F. Costa         ffalconi            1623          5002   \n",
       "2   Dr. James O'Donoghue         physicsJ          148322          1081   \n",
       "3  Beyond Blue Aerospace   beyondblueaero             680          1157   \n",
       "4         Diego A Pava O      dapavao2706             364           714   \n",
       "\n",
       "   user_listed  user_favourites  user_statuses user_protected user_verified  \\\n",
       "0           84            93424         193934          False         False   \n",
       "1          303               23          30126          False         False   \n",
       "2         1251            22069          21049          False          True   \n",
       "3           65             5313          21178          False         False   \n",
       "4            1           413925          37967          False         False   \n",
       "\n",
       "  user_contributors_enabled         user_joined  \\\n",
       "0                     False 2009-07-14 05:10:36   \n",
       "1                     False 2009-03-06 05:49:14   \n",
       "2                     False 2010-12-22 22:01:32   \n",
       "3                     False 2013-11-03 02:35:32   \n",
       "4                     False 2014-10-11 22:53:00   \n",
       "\n",
       "                   user_location retweeted_tweet  \\\n",
       "0  Rogue River, Oregon #homebase              no   \n",
       "1             San Francisco Area              no   \n",
       "2   Between Mount Fuji and Tokyo              no   \n",
       "3                         Canada              no   \n",
       "4                           None              no   \n",
       "\n",
       "                                     tweet_text_urls  \\\n",
       "0                                                      \n",
       "1                            https://t.co/9NeCWrTrIp   \n",
       "2  https://t.co/pncsLEDASF#WebbFliesAriane|https:...   \n",
       "3    https://t.co/OigaIuriN4|https://t.co/0mebmz62i0   \n",
       "4                            https://t.co/LAUwDoGfP3   \n",
       "\n",
       "               tweet_text_hashtags  \\\n",
       "0                                    \n",
       "1                                    \n",
       "2  Webb|WebbFliesAriane|VA256|JWST   \n",
       "3                           SpaceX   \n",
       "4                                    \n",
       "\n",
       "                                tweet_text_usernames  num_urls_in_tweet_text  \\\n",
       "0                                                                          0   \n",
       "1                                                                          1   \n",
       "2  RealtraSpace|Arianespace|ariane5|ESA_Webb|NASA...                       2   \n",
       "3                                                                          2   \n",
       "4                                                                          1   \n",
       "\n",
       "   num_users_in_tweet_text  num_hashtags_in_tweet_text  \\\n",
       "0                        0                           0   \n",
       "1                        0                           0   \n",
       "2                        6                           4   \n",
       "3                        0                           1   \n",
       "4                        0                           0   \n",
       "\n",
       "                                                text  contains_wanted_text  \\\n",
       "0  NASA: It wasn't a strike, it was just a work s...                 False   \n",
       "1  NASA just dropped an exciting update about the...                 False   \n",
       "2    Great footage from  camera of NASA/ESA/CSA J...                 False   \n",
       "3  Photo of James Webb before he transformed into...                 False   \n",
       "4  Your internet connection over fiber optic cabl...                 False   \n",
       "\n",
       "   contains_wanted_text_case_sensitive  contains_multi_word_wanted_text  \\\n",
       "0                                 True                            False   \n",
       "1                                 True                             True   \n",
       "2                                 True                             True   \n",
       "3                                False                             True   \n",
       "4                                 True                            False   \n",
       "\n",
       "   contains_crypto_terms  contains_religious_terms  \\\n",
       "0                  False                     False   \n",
       "1                  False                     False   \n",
       "2                  False                     False   \n",
       "3                  False                     False   \n",
       "4                  False                     False   \n",
       "\n",
       "   contains_inappropriate_terms  contains_video_games_terms  \\\n",
       "0                         False                       False   \n",
       "1                         False                       False   \n",
       "2                         False                       False   \n",
       "3                         False                       False   \n",
       "4                         False                       False   \n",
       "\n",
       "   contains_misc_unwanted_terms  contains_non_english_terms  \\\n",
       "0                         False                       False   \n",
       "1                         False                       False   \n",
       "2                         False                       False   \n",
       "3                         False                       False   \n",
       "4                         False                       False   \n",
       "\n",
       "                                       text_stripped  \\\n",
       "0  NASA: It wasn't a strike, it was just a work s...   \n",
       "1  NASA just dropped an exciting update about the...   \n",
       "2  Great footage from camera of NASA/ESA/CSA Jame...   \n",
       "3  Photo of James Webb before he transformed into...   \n",
       "4  Your internet connection over fiber optic cabl...   \n",
       "\n",
       "                                      text_processed  \\\n",
       "0  nasa  it wasn t a strike  it was just a work s...   \n",
       "1  nasa just dropped an exciting update about the...   \n",
       "2  great footage from camera of nasa esa csa jame...   \n",
       "3  photo of james webb before he transformed into...   \n",
       "4  your internet connection over fiber optic cabl...   \n",
       "\n",
       "                                        text_trimmed  \\\n",
       "0  NASA: It wasn't a strike, it was just a work s...   \n",
       "1  NASA just dropped an exciting update about the...   \n",
       "2  Great footage from  camera of NASA/ESA/CSA Jam...   \n",
       "3  Photo of James Webb before he transformed into...   \n",
       "4  Your internet connection over fiber optic cabl...   \n",
       "\n",
       "                                               words  num_words  \n",
       "0  [NASA:, It, wasn't, a, strike,, it, was, just,...         11  \n",
       "1  [NASA, just, dropped, an, exciting, update, ab...         11  \n",
       "2  [Great, footage, from, camera, of, NASA/ESA/CS...         24  \n",
       "3  [Photo, of, James, Webb, before, he, transform...         13  \n",
       "4  [Your, internet, connection, over, fiber, opti...         33  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coordinates</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contributors</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_quote_status</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quote_count</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reply_count</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retweet_count</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favorite_count</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favorited</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retweeted</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_text</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_id</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_url</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_place_type</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_name</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_full_name</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_country_code</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_country</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_bounding_box_type</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_bounding_box_coordinates</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_attributes</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coords_type</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coords_lon</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coords_lat</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_type</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_lon</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_lat</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_name</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_screen_name</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_followers</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_friends</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_listed</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_favourites</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_statuses</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_protected</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_verified</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_contributors_enabled</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_joined</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_location</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retweeted_tweet</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_text_urls</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_text_hashtags</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_text_usernames</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_urls_in_tweet_text</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_users_in_tweet_text</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_hashtags_in_tweet_text</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_wanted_text</th>\n",
       "      <td>boolean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_wanted_text_case_sensitive</th>\n",
       "      <td>boolean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_multi_word_wanted_text</th>\n",
       "      <td>boolean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_crypto_terms</th>\n",
       "      <td>boolean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_religious_terms</th>\n",
       "      <td>boolean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_inappropriate_terms</th>\n",
       "      <td>boolean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_video_games_terms</th>\n",
       "      <td>boolean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_misc_unwanted_terms</th>\n",
       "      <td>boolean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_non_english_terms</th>\n",
       "      <td>boolean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_stripped</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_processed</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_trimmed</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_words</th>\n",
       "      <td>Int32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              dtype\n",
       "created_at                           datetime64[ns]\n",
       "id                                           string\n",
       "geo                                          string\n",
       "coordinates                                  string\n",
       "place                                        string\n",
       "contributors                                 string\n",
       "is_quote_status                              string\n",
       "quote_count                                   Int32\n",
       "reply_count                                   Int32\n",
       "retweet_count                                 Int32\n",
       "favorite_count                                Int32\n",
       "favorited                                    string\n",
       "retweeted                                    string\n",
       "source                                       string\n",
       "in_reply_to_user_id                          string\n",
       "in_reply_to_screen_name                      string\n",
       "source_text                                  string\n",
       "place_id                                     string\n",
       "place_url                                    string\n",
       "place_place_type                             string\n",
       "place_name                                   string\n",
       "place_full_name                              string\n",
       "place_country_code                           string\n",
       "place_country                                string\n",
       "place_bounding_box_type                      string\n",
       "place_bounding_box_coordinates               string\n",
       "place_attributes                             string\n",
       "coords_type                                  string\n",
       "coords_lon                                   string\n",
       "coords_lat                                   string\n",
       "geo_type                                     string\n",
       "geo_lon                                      string\n",
       "geo_lat                                      string\n",
       "user_name                                    string\n",
       "user_screen_name                             string\n",
       "user_followers                                Int32\n",
       "user_friends                                  Int32\n",
       "user_listed                                   Int32\n",
       "user_favourites                               Int32\n",
       "user_statuses                                 Int32\n",
       "user_protected                               string\n",
       "user_verified                                string\n",
       "user_contributors_enabled                    string\n",
       "user_joined                          datetime64[ns]\n",
       "user_location                                string\n",
       "retweeted_tweet                              string\n",
       "tweet_text_urls                              string\n",
       "tweet_text_hashtags                          string\n",
       "tweet_text_usernames                         string\n",
       "num_urls_in_tweet_text                        Int32\n",
       "num_users_in_tweet_text                       Int32\n",
       "num_hashtags_in_tweet_text                    Int32\n",
       "text                                         string\n",
       "contains_wanted_text                        boolean\n",
       "contains_wanted_text_case_sensitive         boolean\n",
       "contains_multi_word_wanted_text             boolean\n",
       "contains_crypto_terms                       boolean\n",
       "contains_religious_terms                    boolean\n",
       "contains_inappropriate_terms                boolean\n",
       "contains_video_games_terms                  boolean\n",
       "contains_misc_unwanted_terms                boolean\n",
       "contains_non_english_terms                  boolean\n",
       "text_stripped                                string\n",
       "text_processed                               string\n",
       "text_trimmed                                 string\n",
       "words                                        string\n",
       "num_words                                     Int32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.61 s, sys: 1.21 s, total: 8.82 s\n",
      "Wall time: 5.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ddf = (\n",
    "    dd.read_parquet(proc_files)\n",
    "    .reset_index(drop=True)\n",
    "    .astype(dtypes_dict)\n",
    "    .set_index(\"created_at\")  # sorts DataFrame based on this column\n",
    "    .reset_index()\n",
    "    .repartition(npartitions=len(proc_files_all))\n",
    ")\n",
    "print(\n",
    "    f\"Loaded processed data from *.parquet.gzip files into Dask DataFrame \"\n",
    "    f\"with {ddf.npartitions:,} partitions\"\n",
    ")\n",
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    display(ddf.head())\n",
    "with pd.option_context(\"display.max_colwidth\", None, \"display.max_rows\", None):\n",
    "    display(ddf.dtypes.rename(\"dtype\").to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5fe93-43b3-4190-92f5-cb7ccc6fed53",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b084c3d-22f6-411a-8cc2-4db267430ff5",
   "metadata": {},
   "source": [
    "Show duplicated tweets (identical text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a2d2216-344a-43d8-907e-9700db29640e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_joined</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1478901017387446275</th>\n",
       "      <td>2022-01-06 01:27:33</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DickMackintosh</td>\n",
       "      <td>Kenneth Richard</td>\n",
       "      <td>Kenneth72712993</td>\n",
       "      <td>2019-01-27 06:11:16</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Why don't you discuss it with NASA?\"NASA says climate models must be improved by a factor of 100 to be able to project what CO2 might do to the planetary temperature in the future.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478901328789262341</th>\n",
       "      <td>2022-01-06 01:28:48</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Assoe</td>\n",
       "      <td>FnAssoe</td>\n",
       "      <td>2020-08-13 11:54:06</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Why don't you discuss it with NASA?\"NASA says climate models must be improved by a factor of 100 to be able to project what CO2 might do to the planetary temperature in the future.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478901795686547456</th>\n",
       "      <td>2022-01-06 01:30:39</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Rob Meekel</td>\n",
       "      <td>RobMeekel</td>\n",
       "      <td>2013-12-03 21:59:37</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Why don't you discuss it with NASA?\"NASA says climate models must be improved by a factor of 100 to be able to project what CO2 might do to the planetary temperature in the future.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478909404690931712</th>\n",
       "      <td>2022-01-06 02:00:53</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>King Arthur &amp; Excalibur</td>\n",
       "      <td>OscarsWild1</td>\n",
       "      <td>2019-01-14 20:35:43</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Why don't you discuss it with NASA?\"NASA says climate models must be improved by a factor of 100 to be able to project what CO2 might do to the planetary temperature in the future.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478913452202766339</th>\n",
       "      <td>2022-01-06 02:16:58</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DickMackintosh</td>\n",
       "      <td>Kenneth Richard</td>\n",
       "      <td>Kenneth72712993</td>\n",
       "      <td>2019-01-27 06:11:16</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Yes you are very confused aren't you?\"The ethnicity of the CO2 molecule was a great mystery, yes.Then NASA cleared it up for us.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478921500010897412</th>\n",
       "      <td>2022-01-06 02:48:57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>rmack2x</td>\n",
       "      <td>rmack2x</td>\n",
       "      <td>2009-01-06 13:02:23</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Yes you are very confused aren't you?\"The ethnicity of the CO2 molecule was a great mystery, yes.Then NASA cleared it up for us.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476609108627410949</th>\n",
       "      <td>2021-12-30 17:40:20</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>WarwickisaHunt</td>\n",
       "      <td>Qam Yasharahla</td>\n",
       "      <td>chosenachwath</td>\n",
       "      <td>2021-08-05 15:41:01</td>\n",
       "      <td>False</td>\n",
       "      <td>&amp;amp; this is straight from the NASA website so let’s talk about it. This information has been out there for years. Go look up how old this interview is.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476610692014743552</th>\n",
       "      <td>2021-12-30 17:46:37</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>VVS4</td>\n",
       "      <td>VVS413</td>\n",
       "      <td>2021-08-22 07:19:21</td>\n",
       "      <td>False</td>\n",
       "      <td>&amp;amp; this is straight from the NASA website so let’s talk about it. This information has been out there for years. Go look up how old this interview is.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479545398192656385</th>\n",
       "      <td>2022-01-07 20:08:06</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Resist_dwp</td>\n",
       "      <td>Cockwomblethrombosis</td>\n",
       "      <td>Cockwomblethro1</td>\n",
       "      <td>2021-04-18 22:06:51</td>\n",
       "      <td>False</td>\n",
       "      <td>&amp;amp;I as I said ,NASA is funded by federal money &amp;amp;the federal gvt is controlled by the money power that wants to replace real money with carbon linked CBDCs for complete &amp;amp;total control &amp;amp; monetization of every person&amp;amp; commodity on earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479555351250931716</th>\n",
       "      <td>2022-01-07 20:47:39</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>International Society of Anglo-Celts</td>\n",
       "      <td>OldSport87</td>\n",
       "      <td>2021-07-12 09:21:20</td>\n",
       "      <td>False</td>\n",
       "      <td>&amp;amp;I as I said ,NASA is funded by federal money &amp;amp;the federal gvt is controlled by the money power that wants to replace real money with carbon linked CBDCs for complete &amp;amp;total control &amp;amp; monetization of every person&amp;amp; commodity on earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479096993897234433</th>\n",
       "      <td>2022-01-06 14:26:18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AdamGentry2021</td>\n",
       "      <td>Adam Gentry</td>\n",
       "      <td>AdamGentry2021</td>\n",
       "      <td>2018-09-03 02:03:43</td>\n",
       "      <td>False</td>\n",
       "      <td>... If anyone reads what I write who is not a CPP -- or a doctor sympathetic to CPPs -- I wouldn't know.  I try to put my views out there in every direction, like NASA sends its gold record of human paraphernalia into interstellar space on Voyager 2 for discovery by aliens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479126494098169860</th>\n",
       "      <td>2022-01-06 16:23:31</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Melrogers@chronicpainadvocate</td>\n",
       "      <td>MelindaJaneOwe2</td>\n",
       "      <td>2018-03-29 00:32:29</td>\n",
       "      <td>False</td>\n",
       "      <td>... If anyone reads what I write who is not a CPP -- or a doctor sympathetic to CPPs -- I wouldn't know.  I try to put my views out there in every direction, like NASA sends its gold record of human paraphernalia into interstellar space on Voyager 2 for discovery by aliens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479209022402142208</th>\n",
       "      <td>2022-01-06 21:51:27</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Heather CPP Extraordinaire🤘🤣🕯</td>\n",
       "      <td>HeatherLinda11</td>\n",
       "      <td>2018-12-28 06:59:40</td>\n",
       "      <td>False</td>\n",
       "      <td>... If anyone reads what I write who is not a CPP -- or a doctor sympathetic to CPPs -- I wouldn't know.  I try to put my views out there in every direction, like NASA sends its gold record of human paraphernalia into interstellar space on Voyager 2 for discovery by aliens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479262550080565248</th>\n",
       "      <td>2022-01-07 01:24:09</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>🌷💞Peaceful💞🌷✌️🐝🎸</td>\n",
       "      <td>Rosesdaughter61</td>\n",
       "      <td>2015-10-20 01:12:40</td>\n",
       "      <td>False</td>\n",
       "      <td>... If anyone reads what I write who is not a CPP -- or a doctor sympathetic to CPPs -- I wouldn't know.  I try to put my views out there in every direction, like NASA sends its gold record of human paraphernalia into interstellar space on Voyager 2 for discovery by aliens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479274791395856385</th>\n",
       "      <td>2022-01-07 02:12:48</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>MdewakantonMommy</td>\n",
       "      <td>lea_dahkotah69</td>\n",
       "      <td>2014-12-08 22:45:13</td>\n",
       "      <td>False</td>\n",
       "      <td>... If anyone reads what I write who is not a CPP -- or a doctor sympathetic to CPPs -- I wouldn't know.  I try to put my views out there in every direction, like NASA sends its gold record of human paraphernalia into interstellar space on Voyager 2 for discovery by aliens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477119932676681730</th>\n",
       "      <td>2022-01-01 03:30:10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>keithccurtis</td>\n",
       "      <td>Tin Foil Awards</td>\n",
       "      <td>TinFoilAwards</td>\n",
       "      <td>2017-05-12 03:45:31</td>\n",
       "      <td>False</td>\n",
       "      <td>Alex Jones?  The guy who claims that the Army is trying to turn frogs gay?  They guy who entertained a guest that claimed that NASA was transporting children to Mars to be used as sex slaves by Marines?  The guy who is about to go bankrupt over Sandy Hook?  THAT Alex Jones?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477120323510157312</th>\n",
       "      <td>2022-01-01 03:31:43</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Dr Anne Condon</td>\n",
       "      <td>SkepticalMutant</td>\n",
       "      <td>2018-05-05 01:45:54</td>\n",
       "      <td>False</td>\n",
       "      <td>Alex Jones?  The guy who claims that the Army is trying to turn frogs gay?  They guy who entertained a guest that claimed that NASA was transporting children to Mars to be used as sex slaves by Marines?  The guy who is about to go bankrupt over Sandy Hook?  THAT Alex Jones?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477264087193108486</th>\n",
       "      <td>2022-01-01 13:02:59</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Ellen</td>\n",
       "      <td>ellethejambo</td>\n",
       "      <td>2016-09-22 10:17:49</td>\n",
       "      <td>False</td>\n",
       "      <td>Alex Jones?  The guy who claims that the Army is trying to turn frogs gay?  They guy who entertained a guest that claimed that NASA was transporting children to Mars to be used as sex slaves by Marines?  The guy who is about to go bankrupt over Sandy Hook?  THAT Alex Jones?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477291759398838272</th>\n",
       "      <td>2022-01-01 14:52:56</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Liz \"The Mask Goes Over Your Nose AND Mouth\" Ditz</td>\n",
       "      <td>lizditz</td>\n",
       "      <td>2007-04-17 01:16:54</td>\n",
       "      <td>False</td>\n",
       "      <td>Alex Jones?  The guy who claims that the Army is trying to turn frogs gay?  They guy who entertained a guest that claimed that NASA was transporting children to Mars to be used as sex slaves by Marines?  The guy who is about to go bankrupt over Sandy Hook?  THAT Alex Jones?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477936016468193280</th>\n",
       "      <td>2022-01-03 09:32:59</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>VictorJennpaul</td>\n",
       "      <td>BubbasPossumRanch</td>\n",
       "      <td>BubbasRanch</td>\n",
       "      <td>2017-02-09 14:39:05</td>\n",
       "      <td>False</td>\n",
       "      <td>All four records shown (including the tree ring proxy from Briffa 1998) are in agreement with what NOAA stated in 1974. Yet, NASA GISS massively altered all three thermometer records to show warming which did not happen.This is junk science.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477938404159537157</th>\n",
       "      <td>2022-01-03 09:42:28</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>𝐌𝐚𝐫𝐢𝐨𝐧 𝐌𝐚𝐫𝐭𝐢𝐧</td>\n",
       "      <td>MarionMartin199</td>\n",
       "      <td>2021-11-21 09:35:41</td>\n",
       "      <td>False</td>\n",
       "      <td>All four records shown (including the tree ring proxy from Briffa 1998) are in agreement with what NOAA stated in 1974. Yet, NASA GISS massively altered all three thermometer records to show warming which did not happen.This is junk science.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477938483050291200</th>\n",
       "      <td>2022-01-03 09:42:47</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Sirius</td>\n",
       "      <td>Particle96</td>\n",
       "      <td>2012-05-23 00:10:08</td>\n",
       "      <td>False</td>\n",
       "      <td>All four records shown (including the tree ring proxy from Briffa 1998) are in agreement with what NOAA stated in 1974. Yet, NASA GISS massively altered all three thermometer records to show warming which did not happen.This is junk science.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477140953156702211</th>\n",
       "      <td>2022-01-01 04:53:41</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CoryRove</td>\n",
       "      <td>Qanon is Jim Watkins</td>\n",
       "      <td>BurgerLab12</td>\n",
       "      <td>2012-02-04 07:11:22</td>\n",
       "      <td>False</td>\n",
       "      <td>And “by your logic”, there is no actionable science.But the reality is, this system has built the modern world. Cell phones, space shuttles, GPS- all came from scientific communities working through the data.We get shit done. Period.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477141399300497410</th>\n",
       "      <td>2022-01-01 04:55:28</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Humanist🇮🇹make a coherent argument please</td>\n",
       "      <td>pawley_robert</td>\n",
       "      <td>2021-02-01 21:45:22</td>\n",
       "      <td>False</td>\n",
       "      <td>And “by your logic”, there is no actionable science.But the reality is, this system has built the modern world. Cell phones, space shuttles, GPS- all came from scientific communities working through the data.We get shit done. Period.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478450637914398720</th>\n",
       "      <td>2022-01-04 19:37:55</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BrolyHans</td>\n",
       "      <td>King Arthur &amp; Excalibur</td>\n",
       "      <td>OscarsWild1</td>\n",
       "      <td>2019-01-14 20:35:43</td>\n",
       "      <td>False</td>\n",
       "      <td>Are you still wittering on about the NASA page for kids that does not state what you claim, got a bunch of things wrong, missing a load of mechanisms and drawn from a page that NASA deleted?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             created_at is_quote_status retweeted favorited  \\\n",
       "id                                                                            \n",
       "1478901017387446275 2022-01-06 01:27:33           False     False     False   \n",
       "1478901328789262341 2022-01-06 01:28:48           False     False     False   \n",
       "1478901795686547456 2022-01-06 01:30:39           False     False     False   \n",
       "1478909404690931712 2022-01-06 02:00:53           False     False     False   \n",
       "1478913452202766339 2022-01-06 02:16:58           False     False     False   \n",
       "1478921500010897412 2022-01-06 02:48:57           False     False     False   \n",
       "1476609108627410949 2021-12-30 17:40:20           False     False     False   \n",
       "1476610692014743552 2021-12-30 17:46:37           False     False     False   \n",
       "1479545398192656385 2022-01-07 20:08:06           False     False     False   \n",
       "1479555351250931716 2022-01-07 20:47:39           False     False     False   \n",
       "1479096993897234433 2022-01-06 14:26:18           False     False     False   \n",
       "1479126494098169860 2022-01-06 16:23:31           False     False     False   \n",
       "1479209022402142208 2022-01-06 21:51:27           False     False     False   \n",
       "1479262550080565248 2022-01-07 01:24:09           False     False     False   \n",
       "1479274791395856385 2022-01-07 02:12:48           False     False     False   \n",
       "1477119932676681730 2022-01-01 03:30:10           False     False     False   \n",
       "1477120323510157312 2022-01-01 03:31:43           False     False     False   \n",
       "1477264087193108486 2022-01-01 13:02:59           False     False     False   \n",
       "1477291759398838272 2022-01-01 14:52:56           False     False     False   \n",
       "1477936016468193280 2022-01-03 09:32:59           False     False     False   \n",
       "1477938404159537157 2022-01-03 09:42:28           False     False     False   \n",
       "1477938483050291200 2022-01-03 09:42:47           False     False     False   \n",
       "1477140953156702211 2022-01-01 04:53:41           False     False     False   \n",
       "1477141399300497410 2022-01-01 04:55:28           False     False     False   \n",
       "1478450637914398720 2022-01-04 19:37:55           False     False     False   \n",
       "\n",
       "                     retweet_count  quote_count  favorite_count  \\\n",
       "id                                                                \n",
       "1478901017387446275              0            0               0   \n",
       "1478901328789262341              0            0               0   \n",
       "1478901795686547456              0            0               0   \n",
       "1478909404690931712              0            0               0   \n",
       "1478913452202766339              0            0               0   \n",
       "1478921500010897412              0            0               0   \n",
       "1476609108627410949              0            0               0   \n",
       "1476610692014743552              0            0               0   \n",
       "1479545398192656385              0            0               0   \n",
       "1479555351250931716              0            0               0   \n",
       "1479096993897234433              0            0               0   \n",
       "1479126494098169860              0            0               0   \n",
       "1479209022402142208              0            0               0   \n",
       "1479262550080565248              0            0               0   \n",
       "1479274791395856385              0            0               0   \n",
       "1477119932676681730              0            0               0   \n",
       "1477120323510157312              0            0               0   \n",
       "1477264087193108486              0            0               0   \n",
       "1477291759398838272              0            0               0   \n",
       "1477936016468193280              0            0               0   \n",
       "1477938404159537157              0            0               0   \n",
       "1477938483050291200              0            0               0   \n",
       "1477140953156702211              0            0               0   \n",
       "1477141399300497410              0            0               0   \n",
       "1478450637914398720              0            0               0   \n",
       "\n",
       "                    in_reply_to_screen_name  \\\n",
       "id                                            \n",
       "1478901017387446275          DickMackintosh   \n",
       "1478901328789262341                    None   \n",
       "1478901795686547456                    None   \n",
       "1478909404690931712                    None   \n",
       "1478913452202766339          DickMackintosh   \n",
       "1478921500010897412                    None   \n",
       "1476609108627410949          WarwickisaHunt   \n",
       "1476610692014743552                    None   \n",
       "1479545398192656385              Resist_dwp   \n",
       "1479555351250931716                    None   \n",
       "1479096993897234433          AdamGentry2021   \n",
       "1479126494098169860                    None   \n",
       "1479209022402142208                    None   \n",
       "1479262550080565248                    None   \n",
       "1479274791395856385                    None   \n",
       "1477119932676681730            keithccurtis   \n",
       "1477120323510157312                    None   \n",
       "1477264087193108486                    None   \n",
       "1477291759398838272                    None   \n",
       "1477936016468193280          VictorJennpaul   \n",
       "1477938404159537157                    None   \n",
       "1477938483050291200                    None   \n",
       "1477140953156702211                CoryRove   \n",
       "1477141399300497410                    None   \n",
       "1478450637914398720               BrolyHans   \n",
       "\n",
       "                                                             user_name  \\\n",
       "id                                                                       \n",
       "1478901017387446275                                    Kenneth Richard   \n",
       "1478901328789262341                                              Assoe   \n",
       "1478901795686547456                                         Rob Meekel   \n",
       "1478909404690931712                            King Arthur & Excalibur   \n",
       "1478913452202766339                                    Kenneth Richard   \n",
       "1478921500010897412                                            rmack2x   \n",
       "1476609108627410949                                     Qam Yasharahla   \n",
       "1476610692014743552                                               VVS4   \n",
       "1479545398192656385                               Cockwomblethrombosis   \n",
       "1479555351250931716               International Society of Anglo-Celts   \n",
       "1479096993897234433                                        Adam Gentry   \n",
       "1479126494098169860                      Melrogers@chronicpainadvocate   \n",
       "1479209022402142208                      Heather CPP Extraordinaire🤘🤣🕯   \n",
       "1479262550080565248                                   🌷💞Peaceful💞🌷✌️🐝🎸   \n",
       "1479274791395856385                                   MdewakantonMommy   \n",
       "1477119932676681730                                    Tin Foil Awards   \n",
       "1477120323510157312                                     Dr Anne Condon   \n",
       "1477264087193108486                                              Ellen   \n",
       "1477291759398838272  Liz \"The Mask Goes Over Your Nose AND Mouth\" Ditz   \n",
       "1477936016468193280                                  BubbasPossumRanch   \n",
       "1477938404159537157                                      𝐌𝐚𝐫𝐢𝐨𝐧 𝐌𝐚𝐫𝐭𝐢𝐧   \n",
       "1477938483050291200                                             Sirius   \n",
       "1477140953156702211                               Qanon is Jim Watkins   \n",
       "1477141399300497410          Humanist🇮🇹make a coherent argument please   \n",
       "1478450637914398720                            King Arthur & Excalibur   \n",
       "\n",
       "                    user_screen_name         user_joined user_verified  \\\n",
       "id                                                                       \n",
       "1478901017387446275  Kenneth72712993 2019-01-27 06:11:16         False   \n",
       "1478901328789262341          FnAssoe 2020-08-13 11:54:06         False   \n",
       "1478901795686547456        RobMeekel 2013-12-03 21:59:37         False   \n",
       "1478909404690931712      OscarsWild1 2019-01-14 20:35:43         False   \n",
       "1478913452202766339  Kenneth72712993 2019-01-27 06:11:16         False   \n",
       "1478921500010897412          rmack2x 2009-01-06 13:02:23         False   \n",
       "1476609108627410949    chosenachwath 2021-08-05 15:41:01         False   \n",
       "1476610692014743552           VVS413 2021-08-22 07:19:21         False   \n",
       "1479545398192656385  Cockwomblethro1 2021-04-18 22:06:51         False   \n",
       "1479555351250931716       OldSport87 2021-07-12 09:21:20         False   \n",
       "1479096993897234433   AdamGentry2021 2018-09-03 02:03:43         False   \n",
       "1479126494098169860  MelindaJaneOwe2 2018-03-29 00:32:29         False   \n",
       "1479209022402142208   HeatherLinda11 2018-12-28 06:59:40         False   \n",
       "1479262550080565248  Rosesdaughter61 2015-10-20 01:12:40         False   \n",
       "1479274791395856385   lea_dahkotah69 2014-12-08 22:45:13         False   \n",
       "1477119932676681730    TinFoilAwards 2017-05-12 03:45:31         False   \n",
       "1477120323510157312  SkepticalMutant 2018-05-05 01:45:54         False   \n",
       "1477264087193108486     ellethejambo 2016-09-22 10:17:49         False   \n",
       "1477291759398838272          lizditz 2007-04-17 01:16:54         False   \n",
       "1477936016468193280      BubbasRanch 2017-02-09 14:39:05         False   \n",
       "1477938404159537157  MarionMartin199 2021-11-21 09:35:41         False   \n",
       "1477938483050291200       Particle96 2012-05-23 00:10:08         False   \n",
       "1477140953156702211      BurgerLab12 2012-02-04 07:11:22         False   \n",
       "1477141399300497410    pawley_robert 2021-02-01 21:45:22         False   \n",
       "1478450637914398720      OscarsWild1 2019-01-14 20:35:43         False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                     text  \n",
       "id                                                                                                                                                                                                                                                                                                                                                         \n",
       "1478901017387446275                                                                                                                                                 \"Why don't you discuss it with NASA?\"NASA says climate models must be improved by a factor of 100 to be able to project what CO2 might do to the planetary temperature in the future.  \n",
       "1478901328789262341                                                                                                                                                 \"Why don't you discuss it with NASA?\"NASA says climate models must be improved by a factor of 100 to be able to project what CO2 might do to the planetary temperature in the future.  \n",
       "1478901795686547456                                                                                                                                                 \"Why don't you discuss it with NASA?\"NASA says climate models must be improved by a factor of 100 to be able to project what CO2 might do to the planetary temperature in the future.  \n",
       "1478909404690931712                                                                                                                                                 \"Why don't you discuss it with NASA?\"NASA says climate models must be improved by a factor of 100 to be able to project what CO2 might do to the planetary temperature in the future.  \n",
       "1478913452202766339                                                                                                                                                                                                     \"Yes you are very confused aren't you?\"The ethnicity of the CO2 molecule was a great mystery, yes.Then NASA cleared it up for us.  \n",
       "1478921500010897412                                                                                                                                                                                                     \"Yes you are very confused aren't you?\"The ethnicity of the CO2 molecule was a great mystery, yes.Then NASA cleared it up for us.  \n",
       "1476609108627410949                                                                                                                                                                             &amp; this is straight from the NASA website so let’s talk about it. This information has been out there for years. Go look up how old this interview is.  \n",
       "1476610692014743552                                                                                                                                                                             &amp; this is straight from the NASA website so let’s talk about it. This information has been out there for years. Go look up how old this interview is.  \n",
       "1479545398192656385                                                                          &amp;I as I said ,NASA is funded by federal money &amp;the federal gvt is controlled by the money power that wants to replace real money with carbon linked CBDCs for complete &amp;total control &amp; monetization of every person&amp; commodity on earth  \n",
       "1479555351250931716                                                                          &amp;I as I said ,NASA is funded by federal money &amp;the federal gvt is controlled by the money power that wants to replace real money with carbon linked CBDCs for complete &amp;total control &amp; monetization of every person&amp; commodity on earth  \n",
       "1479096993897234433                                                     ... If anyone reads what I write who is not a CPP -- or a doctor sympathetic to CPPs -- I wouldn't know.  I try to put my views out there in every direction, like NASA sends its gold record of human paraphernalia into interstellar space on Voyager 2 for discovery by aliens  \n",
       "1479126494098169860                                                     ... If anyone reads what I write who is not a CPP -- or a doctor sympathetic to CPPs -- I wouldn't know.  I try to put my views out there in every direction, like NASA sends its gold record of human paraphernalia into interstellar space on Voyager 2 for discovery by aliens  \n",
       "1479209022402142208                                                     ... If anyone reads what I write who is not a CPP -- or a doctor sympathetic to CPPs -- I wouldn't know.  I try to put my views out there in every direction, like NASA sends its gold record of human paraphernalia into interstellar space on Voyager 2 for discovery by aliens  \n",
       "1479262550080565248                                                     ... If anyone reads what I write who is not a CPP -- or a doctor sympathetic to CPPs -- I wouldn't know.  I try to put my views out there in every direction, like NASA sends its gold record of human paraphernalia into interstellar space on Voyager 2 for discovery by aliens  \n",
       "1479274791395856385                                                     ... If anyone reads what I write who is not a CPP -- or a doctor sympathetic to CPPs -- I wouldn't know.  I try to put my views out there in every direction, like NASA sends its gold record of human paraphernalia into interstellar space on Voyager 2 for discovery by aliens  \n",
       "1477119932676681730                                                    Alex Jones?  The guy who claims that the Army is trying to turn frogs gay?  They guy who entertained a guest that claimed that NASA was transporting children to Mars to be used as sex slaves by Marines?  The guy who is about to go bankrupt over Sandy Hook?  THAT Alex Jones?  \n",
       "1477120323510157312                                                    Alex Jones?  The guy who claims that the Army is trying to turn frogs gay?  They guy who entertained a guest that claimed that NASA was transporting children to Mars to be used as sex slaves by Marines?  The guy who is about to go bankrupt over Sandy Hook?  THAT Alex Jones?  \n",
       "1477264087193108486                                                    Alex Jones?  The guy who claims that the Army is trying to turn frogs gay?  They guy who entertained a guest that claimed that NASA was transporting children to Mars to be used as sex slaves by Marines?  The guy who is about to go bankrupt over Sandy Hook?  THAT Alex Jones?  \n",
       "1477291759398838272                                                    Alex Jones?  The guy who claims that the Army is trying to turn frogs gay?  They guy who entertained a guest that claimed that NASA was transporting children to Mars to be used as sex slaves by Marines?  The guy who is about to go bankrupt over Sandy Hook?  THAT Alex Jones?  \n",
       "1477936016468193280                                                                                     All four records shown (including the tree ring proxy from Briffa 1998) are in agreement with what NOAA stated in 1974. Yet, NASA GISS massively altered all three thermometer records to show warming which did not happen.This is junk science.  \n",
       "1477938404159537157                                                                                     All four records shown (including the tree ring proxy from Briffa 1998) are in agreement with what NOAA stated in 1974. Yet, NASA GISS massively altered all three thermometer records to show warming which did not happen.This is junk science.  \n",
       "1477938483050291200                                                                                     All four records shown (including the tree ring proxy from Briffa 1998) are in agreement with what NOAA stated in 1974. Yet, NASA GISS massively altered all three thermometer records to show warming which did not happen.This is junk science.  \n",
       "1477140953156702211                                                                                             And “by your logic”, there is no actionable science.But the reality is, this system has built the modern world. Cell phones, space shuttles, GPS- all came from scientific communities working through the data.We get shit done. Period.  \n",
       "1477141399300497410                                                                                             And “by your logic”, there is no actionable science.But the reality is, this system has built the modern world. Cell phones, space shuttles, GPS- all came from scientific communities working through the data.We get shit done. Period.  \n",
       "1478450637914398720                                                                                                                                        Are you still wittering on about the NASA page for kids that does not state what you claim, got a bunch of things wrong, missing a load of mechanisms and drawn from a page that NASA deleted?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.18 s, sys: 544 ms, total: 5.73 s\n",
      "Wall time: 4.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = ddf[\n",
    "    [\n",
    "        'id',\n",
    "        'created_at',\n",
    "        # 'contributors',\n",
    "        'is_quote_status',\n",
    "        # 'source_text',\n",
    "        'retweeted',\n",
    "        'favorited',\n",
    "        'retweet_count',\n",
    "        'quote_count',\n",
    "        'favorite_count',\n",
    "        # 'in_reply_to_user_id',\n",
    "        'in_reply_to_screen_name',\n",
    "        'user_name',\n",
    "        'user_screen_name',\n",
    "        'user_joined',\n",
    "        'user_verified',\n",
    "        'text',\n",
    "    ]\n",
    "].compute()\n",
    "with pd.option_context('display.max_colwidth', 1000):\n",
    "    display(\n",
    "        df[df.duplicated(subset=['text'], keep=False)]\n",
    "        .sort_values(by=['text', 'created_at'])\n",
    "        .head(25)\n",
    "        .set_index('id')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bd9756-ebb8-494f-a61a-e31f198f4e75",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. Since we are only focusing on a subset of all available metadata, the subset is small enough to fit in memory so we can call `.compute()` in order to hold the subset in an in-memory (`pandas`) DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd99dc-8e7f-4c95-8ff1-d203ffb9d099",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. These do not appear as retweets, even though the text of the tweet is identical.\n",
    "2. For each duplicate, the first of the duplicated occurrences is in response to a known Twitter user (see the `in_reply_to_screen_name` column). However, subsequent occurrences do not list a user in the `in_reply_to_screen_name` column.\n",
    "3. For ML or NLP model training that\n",
    "   - only uses the text of the tweet to engineer features\n",
    "     - duplicates must be dropped\n",
    "   - uses the text of the tweet as well as tweet metadata to engineer features\n",
    "     - duplicates must be be kept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54568dc-6b45-472e-89a8-14ee92d5e20d",
   "metadata": {},
   "source": [
    "## Split Data and Create Sample for Training NLP Labeling (Transformer) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1522cc1-7e73-4a3d-a1d6-51523d696eef",
   "metadata": {},
   "source": [
    "### Create Data Splits for ML Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e7f61-a000-42ed-832f-1c48e3e4adeb",
   "metadata": {},
   "source": [
    "Create non-randomized training, validation, test and train+validation splits from data pre-dating the first inference `datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb05279-1ac1-4ab0-94e6-b668e0e2b6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.94 ms, sys: 67 µs, total: 2 ms\n",
      "Wall time: 1.96 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_all = ddf[ddf[\"created_at\"] < inference_start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bea2073-ad49-4e3c-93bf-490e1bb070d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 ms, sys: 3.83 ms, total: 18.6 ms\n",
      "Wall time: 24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train_val, df_test = train_test_split(\n",
    "    df_all, test_size=test_split_frac, random_state=88, shuffle=False\n",
    ")\n",
    "df_train, df_val = train_test_split(\n",
    "    df_train_val, test_size=val_split_frac, random_state=88, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518fe442-81ba-4a3a-bfaf-8ff5546324a5",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. When the deployed ML model will be evaluated in production, it will make infererence on data that arrives in chronological order. The inference predictions do not have to be made in chronological order, so the newly arrived data will be randomized before making inference with the the trained (and deployed) model. The order in which the predictions are made does not matter and so we are justified in randomizing the newly arrived data before making inference. The data splits should follow the same ordering during model development, meaning that the splits will be created from data that is sorted in chronological order and then randomized. Since the tweets were being streamed, they have already been accumulated in the cloud storage bucket in chronological order. When these files were loaded here into a single `dask.DataFrame` above, the index is set to `created_at` which results in data being sorted in chronological order (setting the index automatically results in data being sorted based on the new index). So, we just need to create splits before shuffling (randomizing) the data. For this reason, the keyword `shuffle=False` was specified in the `train_test_split()` method used to create the training, validation and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e296ee-2d4d-4b2f-8502-b3bbdfbad7c0",
   "metadata": {},
   "source": [
    "Randomize the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0795f0a8-aebc-44be-b714-a91783a586dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test, df_train_val = [\n",
    "    df_train.sample(frac=1.0, random_state=88),\n",
    "    df_val.sample(frac=1.0, random_state=88),\n",
    "    df_test.sample(frac=1.0, random_state=88),\n",
    "    df_train_val.sample(frac=1.0, random_state=88),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e68705-29cd-436a-b6ae-fc44a3e24c8e",
   "metadata": {},
   "source": [
    "Get split lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "533c8a20-5207-4a45-8431-010804ba48a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 s, sys: 2.3 s, total: 20.5 s\n",
      "Wall time: 16.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_len</th>\n",
       "      <th>val_len</th>\n",
       "      <th>test_len</th>\n",
       "      <th>total</th>\n",
       "      <th>desired_val_frac</th>\n",
       "      <th>desired_test_frac</th>\n",
       "      <th>val_frac</th>\n",
       "      <th>test_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126772</td>\n",
       "      <td>21044</td>\n",
       "      <td>21114</td>\n",
       "      <td>168930</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.124572</td>\n",
       "      <td>0.124987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_len  val_len  test_len   total  desired_val_frac  desired_test_frac  \\\n",
       "0     126772    21044     21114  168930          0.142857              0.125   \n",
       "\n",
       "   val_frac  test_frac  \n",
       "0  0.124572   0.124987  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_rand_dates = (\n",
    "    pd.DataFrame([len(df_train)], columns=['train_len'])\n",
    "    .assign(val_len=len(df_val))\n",
    "    .assign(test_len=len(df_test))\n",
    "    .assign(\n",
    "        total=lambda df: df[\"train_len\"]\n",
    "        + df[\"val_len\"]\n",
    "        + df[\"test_len\"]\n",
    "    )\n",
    "    .assign(desired_val_frac=val_split_frac)\n",
    "    .assign(desired_test_frac=test_split_frac)\n",
    "    .assign(val_frac=lambda df: df[\"val_len\"] / df[\"total\"])\n",
    "    .assign(test_frac=lambda df: df[\"test_len\"] / df[\"total\"])\n",
    ")\n",
    "df_rand_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d9822-cbdb-488e-846a-fd6d4e29b7bc",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. The columns ending in `_frac` are the fraction of rows in the\n",
    "   - validation split\n",
    "   - testing split\n",
    "\n",
    "   relative to the total number of rows across the combination of\n",
    "   - training split\n",
    "   - validation split\n",
    "   - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae021ca0-48c8-44c7-b4e3-5d5dded36b28",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. By design the validation and testing splits are nearly equivalent in size. This is a direct consequence of specifying the `test_split_frac` and `val_split_frac` when creating the splits with `train_test_split()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc5cee6-d5c8-409f-a853-07fe7fab9656",
   "metadata": {},
   "source": [
    "### Create Data Splits from Training Split, for NLP Model Development (to assign sentiment labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34197510-a308-4055-9b85-668ac7e2d80a",
   "metadata": {},
   "source": [
    "We will now draw a random sample from the training split to use in NLP (transformer) model fine-tuning in order to label the tweets with a sentiment (i.e. in order to extract the sentiment of the text in the tweets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a80d7a-215b-4675-9447-e11212fb375b",
   "metadata": {},
   "source": [
    "Next, we'll extract a sample of the training data corresponding to the earlier specified fraction of the training split to be used in NLP model fine-tuning, while dropping duplicated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a2d5fdd-b6d0-45ff-9230-52c984becb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.64 s, sys: 924 ms, total: 7.56 s\n",
      "Wall time: 6.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train_sample = (\n",
    "    df_train.drop_duplicates(subset=[\"text\"], keep=\"first\").sample(frac=nlp_sample_size, random_state=88)\n",
    "    .compute()\n",
    "    .sort_values(by=[\"created_at\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a6d9b-7d9d-4c18-a02c-afac6981fe4b",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. The sample is small enough that it fits in memory and so we don't need to use a big data framework to hold its contents. So, we call `.compute()` to bring this sample into memory and we can use in-memory tools (below) for creating data splits from this sample.\n",
    "2. Before creating the data splits for ML model development, the data was sorted by the `datetime` when the tweet was posted (i.e. sorted by the `created_at` column). In order to create the splits for NLP model development in a way that is consistent with how the data splits were created for ML model development, after drawing the random sample, we sort the sampled data by the same `created_at` column before non-random splits will be created and then randomized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0ad8a-f131-45d9-98c1-1f8f67f57827",
   "metadata": {},
   "source": [
    "Tweets might be created at the same timestamp and so duplicated values are possible in this column, meaning that unique values in this column will be less than the total number of tweets in the data (including in the sampled data). Tweet IDs are unique for each tweet so the number of unique values in the `id` column will match the number of tweets in the data. These are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0ba7fc7-75d2-44a1-90de-c5d2688e8673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique IDs in sampled data = 9,969\n",
      "Number of unique creation datetimes in sampled data = 9,890\n",
      "Number of rows in sampled data = 9,969\n",
      "CPU times: user 5.59 ms, sys: 205 µs, total: 5.8 ms\n",
      "Wall time: 4.46 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\n",
    "    f\"Number of unique IDs in sampled data = {df_train_sample['id'].nunique():,}\\n\"\n",
    "    f\"Number of unique creation datetimes in sampled data = {df_train_sample['created_at'].nunique():,}\\n\"\n",
    "    f\"Number of rows in sampled data = {len(df_train_sample):,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827db55-a575-440f-9d34-848611bc1c34",
   "metadata": {},
   "source": [
    "We'll now create non-randomized training, validation, testing and train+validation splits from the sampled data, which will be used during NLP model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "427f4896-5bb1-458d-9342-646eeb2d0d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_train_val, df_nlp_test = sk_train_test_split(\n",
    "    df_train_sample, test_size=test_split_frac, shuffle=False, random_state=88\n",
    ")\n",
    "df_nlp_train, df_nlp_val = sk_train_test_split(\n",
    "    df_nlp_train_val, test_size=test_split_frac, shuffle=False, random_state=88\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b9833-9c29-4e7d-963b-8cf82c569db9",
   "metadata": {},
   "source": [
    "Randomize the NLP splits and select the necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b01af5f4-cb21-4ca7-aa4c-abfd03886fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.9 ms, sys: 0 ns, total: 16.9 ms\n",
      "Wall time: 17.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_nlp_train, df_nlp_val, df_nlp_test, df_nlp_train_val = [\n",
    "    df_nlp_train[nlp_cols].sample(frac=1.0, random_state=88),\n",
    "    df_nlp_val[nlp_cols].sample(frac=1.0, random_state=88),\n",
    "    df_nlp_test[nlp_cols].sample(frac=1.0, random_state=88),\n",
    "    df_nlp_train_val[nlp_cols].sample(frac=1.0, random_state=88),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2cb60-4c38-4005-a675-68a4c5cb57ff",
   "metadata": {},
   "source": [
    "Get NLP split lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b65929e4-98bf-4a77-893c-011688b8ef18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 ms, sys: 42 µs, total: 4.38 ms\n",
      "Wall time: 3.67 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_len</th>\n",
       "      <th>val_len</th>\n",
       "      <th>test_len</th>\n",
       "      <th>total</th>\n",
       "      <th>val_frac</th>\n",
       "      <th>test_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7631</td>\n",
       "      <td>1091</td>\n",
       "      <td>1247</td>\n",
       "      <td>9969</td>\n",
       "      <td>0.109439</td>\n",
       "      <td>0.125088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_len  val_len  test_len  total  val_frac  test_frac\n",
       "0       7631     1091      1247   9969  0.109439   0.125088"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_nlp_rand_dates = (\n",
    "    pd.DataFrame([len(df_nlp_train)], columns=['train_len'])\n",
    "    .assign(val_len=len(df_nlp_val))\n",
    "    .assign(test_len=len(df_nlp_test))\n",
    "    .assign(\n",
    "        total=lambda df: df[\"train_len\"]\n",
    "        + df[\"val_len\"]\n",
    "        + df[\"test_len\"]\n",
    "    )\n",
    "    .assign(val_frac=lambda df: df[\"val_len\"] / df[\"total\"])\n",
    "    .assign(test_frac=lambda df: df[\"test_len\"] / df[\"total\"])\n",
    ")\n",
    "df_nlp_rand_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32c249-cb3b-45e9-9c97-68daa4c76472",
   "metadata": {},
   "source": [
    "## Export All Data Splits to S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb22c0-f85a-4f9c-919b-49a9d4c0e770",
   "metadata": {},
   "source": [
    "Get the start date for making inference with the trained ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dc6a8c0-7181-45b8-b97d-9aa485355fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220110_000000\n"
     ]
    }
   ],
   "source": [
    "inference_start_date_str = (\n",
    "    inference_start_date.replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"_\")\n",
    ")\n",
    "print(inference_start_date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d0177-f62c-4794-8705-4105220c041b",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. The inference start date is used in file naming as a crude way to version data used in each round of ML model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c76b0a-1225-4dde-8caa-1097c8c86974",
   "metadata": {},
   "source": [
    "All data spits for ML model development will now be saved to a separate `.parquet` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c187cdb-ac43-46ba-a71c-41432b1acbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 126,772 rows to datasets/twitter/kinesis-demo/processed/splits/train__inference_starts_20220110_000000.parquet.gzip\n",
      "Exported 21,044 rows to datasets/twitter/kinesis-demo/processed/splits/val__inference_starts_20220110_000000.parquet.gzip\n",
      "Exported 21,114 rows to datasets/twitter/kinesis-demo/processed/splits/test__inference_starts_20220110_000000.parquet.gzip\n",
      "CPU times: user 45.4 s, sys: 4.78 s, total: 50.2 s\n",
      "Wall time: 38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for split_name, df_split_to_export in zip(\n",
    "    [\"train\", \"val\", \"test\"], [df_train, df_val, df_test]\n",
    "):\n",
    "    fname = f\"{split_name}__inference_starts_{inference_start_date_str}.parquet.gzip\"\n",
    "    if upload_to_s3:\n",
    "        storage_options = {\n",
    "            \"key\": session.get_credentials().access_key,\n",
    "            \"secret\": session.get_credentials().secret_key,\n",
    "        }\n",
    "        prefix = f\"{path_to_folder[1:]}processed/splits/{fname}\"\n",
    "        split_filepath = f\"s3://{s3_bucket_name}/{prefix}\"\n",
    "    else:\n",
    "        storage_options = None\n",
    "        prefix = f\"{processed_data_dir}/{fname}\"\n",
    "        split_filepath = prefix\n",
    "    df_split_to_export.to_parquet(\n",
    "        split_filepath,\n",
    "        write_index=False,\n",
    "        compression='gzip',\n",
    "        storage_options=storage_options,\n",
    "    )\n",
    "    print(f\"Exported {len(df_split_to_export):,} rows to {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eea592-4113-4af1-beb6-a59b5bfaaa5c",
   "metadata": {},
   "source": [
    "All sampled data spits for NLP model development will now be saved to a separate\n",
    "- `.CSV` file on S3\n",
    "- local `.XLSX` file (for use in the next notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7f65ad1-e3be-4553-a0b6-604efe453eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 7,631 rows to datasets/twitter/kinesis-demo/processed/nlp_splits/train_nlp__inference_starts_20220110_000000.csv.zip\n",
      "Exported 1,091 rows to datasets/twitter/kinesis-demo/processed/nlp_splits/val_nlp__inference_starts_20220110_000000.csv.zip\n",
      "Exported 1,247 rows to datasets/twitter/kinesis-demo/processed/nlp_splits/test_nlp__inference_starts_20220110_000000.csv.zip\n",
      "CPU times: user 1.72 s, sys: 2.39 ms, total: 1.72 s\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if create_nlp_splits:\n",
    "    for split_name, df_split_to_export in zip(\n",
    "        [\"train_nlp\", \"val_nlp\", \"test_nlp\"],\n",
    "        [df_nlp_train, df_nlp_val, df_nlp_test],\n",
    "    ):\n",
    "        fname = f\"{split_name}__inference_starts_{inference_start_date_str}.csv.zip\"\n",
    "        if upload_to_s3:\n",
    "            storage_options={\n",
    "                \"key\": session.get_credentials().access_key,\n",
    "                \"secret\": session.get_credentials().secret_key,\n",
    "            }\n",
    "            prefix = f\"{path_to_folder[1:]}processed/nlp_splits/{fname}\"\n",
    "            split_filepath = f\"s3://{s3_bucket_name}/{prefix}\"\n",
    "        else:\n",
    "            storage_options = None\n",
    "            prefix = f\"{processed_data_dir}/{fname}\"\n",
    "            split_filepath = prefix\n",
    "        df_split_to_export.to_csv(\n",
    "            split_filepath,\n",
    "            index=False,\n",
    "            storage_options=storage_options,\n",
    "        )\n",
    "        df_split_to_export.to_excel(\n",
    "            f\"{processed_data_dir}/{fname.replace('.csv.zip', '.xlsx')}\",\n",
    "            index=False,\n",
    "        )\n",
    "        print(f\"Exported {len(df_split_to_export):,} rows to {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66758d51-929c-48b2-b866-1b96f9ac07be",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d474e-070f-48f2-85a4-69d5186e2569",
   "metadata": {},
   "source": [
    "We'll now\n",
    "- delete the local `.zip` file (containing the individual `.parquet` files of prepared data) that was downloaded from S3\n",
    "- delete the local `.parquet` folder with prepared data that was processed in the previous notebook (`5_*.ipynb`) and extracted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "840597e5-e175-4bf5-a6a5-c3b922714fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted local .parquet.gzip files with filtered data.\n",
      "Deleted local .zip file created from all filtered data files.\n",
      "CPU times: user 0 ns, sys: 10.5 ms, total: 10.5 ms\n",
      "Wall time: 9.18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if cleanup_local_files:\n",
    "    # delete locally exported (by PySpark) parquet files\n",
    "    shutil.rmtree(proc_files[0])\n",
    "    print(\"Deleted local .parquet.gzip files with filtered data.\")\n",
    "\n",
    "    # delete local zip file\n",
    "    os.remove(os.path.join(processed_data_dir, proc_text_zip_fname))\n",
    "    print(\"Deleted local .zip file created from all filtered data files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3f200-cd55-429e-9212-9e7b0c911ead",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b63005-1934-433f-91d3-b649dab40b44",
   "metadata": {},
   "source": [
    "<span style=\"float:left;\">\n",
    "    <a href=\"./5_process_data.ipynb\"><< 5 - Big Data Processing</a>\n",
    "</span>\n",
    "\n",
    "<span style=\"float:right;\">\n",
    "    <a href=\"./7_nlp_labeling.ipynb\">7 - NLP-based Labeling >></a>\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark:Python",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
