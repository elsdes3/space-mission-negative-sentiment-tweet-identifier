{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee08390-c516-4a05-8e72-a4d9e67b940d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e4919-9d5d-45f9-8ca0-5bec116c6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !pip3 freeze | grep -E 'boto3|s3fs|black==|jupyter-server|pandas|openpyxl|ipywidgets|IProgress|tqdm|torch|transformers'\n",
    "# !conda list -n spark | grep -E 'ipykernel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155c378-f1ff-40da-bac6-1758599c19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53957578-9301-4a2b-b93e-4e38f54aa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1153dc-6ba9-4499-8230-216e78d3221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a78a4-609d-4cfd-837a-367b52142f52",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a349b0-62c9-4067-a33a-51bccbe80f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folder = \"/datasets/twitter/kinesis-demo/\"\n",
    "\n",
    "# processed data\n",
    "processed_data_dir = \"data/processed/nlp_splits\"\n",
    "\n",
    "label_mapper = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "checkpoint_pretrained = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "\n",
    "model_output_dir = (\n",
    "    f\"{checkpoint_pretrained.split('/')[1].split('-')[0].lower()}-finetuned\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad16026c-5468-4dc2-bdbc-8e46ae10bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = os.getenv(\"AWS_S3_BUCKET_NAME\", \"\")\n",
    "session = boto3.Session(profile_name=\"default\")\n",
    "s3_client = session.client(\"s3\")\n",
    "\n",
    "dtypes_dict = {\n",
    "    \"id\": pd.StringDtype(),\n",
    "    \"text\": pd.StringDtype(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de725875-2335-4aac-b461-7d0a1c3d24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, mytokenizer):\n",
    "    \"\"\"Tokenize text.\"\"\"\n",
    "    return mytokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "def get_metrics(\n",
    "    y_true, y_pred, average=\"binary\", zero_division=\"warn\", use_sample_weights=False\n",
    "):\n",
    "    \"\"\"Use transformers library to calculate sklearn metrics.\"\"\"\n",
    "    if use_sample_weights:\n",
    "        y_true_list = list(y_true)\n",
    "        mapper = dict(Counter(y_true_list))\n",
    "        sample_weights = [mapper[q] for q in y_true_list]\n",
    "    else:\n",
    "        sample_weights = None\n",
    "    metrics_dict = dict(\n",
    "        accuracy=skm.accuracy_score(y_true, y_pred),\n",
    "        balanced_accuracy=skm.balanced_accuracy_score(y_true, y_pred),\n",
    "        precision=skm.precision_score(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=average,\n",
    "            sample_weight=sample_weights,\n",
    "            zero_division=zero_division,\n",
    "        ),\n",
    "        recall=skm.recall_score(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=average,\n",
    "            sample_weight=sample_weights,\n",
    "            zero_division=zero_division,\n",
    "        ),\n",
    "        f1=skm.f1_score(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=average,\n",
    "            sample_weight=sample_weights,\n",
    "            zero_division=zero_division,\n",
    "        ),\n",
    "        f05=skm.fbeta_score(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            beta=0.5,\n",
    "            average=average,\n",
    "            sample_weight=sample_weights,\n",
    "            zero_division=zero_division,\n",
    "        ),\n",
    "        f2=skm.fbeta_score(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            beta=2.0,\n",
    "            average=average,\n",
    "            sample_weight=sample_weights,\n",
    "            zero_division=zero_division,\n",
    "        ),\n",
    "    )\n",
    "    return [metrics_dict, sample_weights]\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate metrics as part of Trainer class.\"\"\"\n",
    "    labels = eval_pred.label_ids\n",
    "    predictions = eval_pred.predictions.argmax(-1)\n",
    "    metrics, _ = get_metrics(labels, predictions, \"weighted\", 0, False)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d1fbc-5a75-4b4b-8e62-622e82094809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files_from_s3(\n",
    "    s3_bucket_name: str,\n",
    "    path_to_folder: str,\n",
    "    data_dir: str,\n",
    "    aws_region: str,\n",
    "    prefix: str,\n",
    ") -> None:\n",
    "    \"\"\"Download files from S3.\"\"\"\n",
    "    s3_filepath_contents = s3_client.list_objects_v2(\n",
    "        Bucket=s3_bucket_name,\n",
    "        Delimiter=\"/\",\n",
    "        Prefix=prefix,\n",
    "    )[\"Contents\"]\n",
    "    s3_filepath_keys = [fc[\"Key\"] for fc in s3_filepath_contents]\n",
    "\n",
    "    for s3_filepath_key in s3_filepath_keys:\n",
    "        dest_filepath = os.path.join(\n",
    "            processed_data_dir, os.path.basename(s3_filepath_key)\n",
    "        )\n",
    "        if not os.path.exists(dest_filepath):\n",
    "            start = datetime.now()\n",
    "            print(\n",
    "                f\"Started downloading processed data zip file from {s3_filepath_key} to \"\n",
    "                f\"{dest_filepath} at {start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}...\"\n",
    "            )\n",
    "            s3 = boto3.resource(\"s3\", region_name=aws_region)\n",
    "            s3.meta.client.download_file(\n",
    "                s3_bucket_name,\n",
    "                s3_filepath_key,\n",
    "                dest_filepath,\n",
    "            )\n",
    "            duration = (datetime.now() - start).total_seconds()\n",
    "            print(f\"Done downloading in {duration:.3f} seconds.\")\n",
    "        else:\n",
    "            print(f\"File found at {dest_filepath}. Did nothing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106a68c-4cd7-41b9-8ab5-ec944c0bb93b",
   "metadata": {},
   "source": [
    "## Get Annotated Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3e267-5620-4caa-8c0e-902fe5bfcdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "download_files_from_s3(\n",
    "    s3_bucket_name,\n",
    "    path_to_folder,\n",
    "    processed_data_dir,\n",
    "    session.region_name,\n",
    "    f\"{path_to_folder[1:]}processed/nlp_splits/\",\n",
    ")\n",
    "proc_files = glob(f\"{processed_data_dir}/*_annotated.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e067d-eabd-4ef8-81bc-86ef44735dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test, df_train, df_val = [pd.read_excel(f) for f in [proc_files]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7663e-f036-4ea2-ab85-b855dc4b7e14",
   "metadata": {},
   "source": [
    "## Get Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31603ce-a456-4bc3-9c13-4d68f038e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = [\n",
    "    df_train[\"text\"],\n",
    "    df_val[\"text\"],\n",
    "    df_test[\"text\"],\n",
    "    df_train[\"labels\"],\n",
    "    df_val[\"labels\"],\n",
    "    df_test[\"labels\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fd586-3877-4cd6-a0d1-c83f53126d55",
   "metadata": {},
   "source": [
    "## Create `huggingface` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0029767-5d55-458b-83dc-958b01088c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {\n",
    "    \"train\": {\"label\": y_train.tolist(), \"text\": X_train.tolist()},\n",
    "    \"val\": {\"label\": y_val.tolist(), \"text\": X_val.tolist()},\n",
    "    \"test\": {\"label\": y_test.tolist(), \"text\": X_test.tolist()},\n",
    "}\n",
    "dataset = DatasetDict()\n",
    "for k, v in mydict.items():\n",
    "    dataset[k] = Dataset.from_dict(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2f665-edfc-469d-8aa9-e0ce4358b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3118d2c-59ef-4e68-bccc-156dd55e8e55",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014822f-25cb-4697-93a8-817a3c7f6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_train))\n",
    "display(\n",
    "    y_train.value_counts(normalize=True)\n",
    "    .rename(\"freq\")\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"label\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586f7ce-4ae4-4172-91de-93741a18a1fd",
   "metadata": {},
   "source": [
    "## Instantiate Pre-Trained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf1c32-3efa-4511-9d03-40caeec4984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {v: k for k, v in label_mapper.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664d97bf-090f-49aa-b0b9-4977fe5f2e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_pretrained)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint_pretrained,\n",
    "    num_labels=y_train.nunique(),\n",
    "    id2label=id2label,\n",
    "    label2id=label_mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8bd00-e3f9-4a35-adcd-6b40a4728829",
   "metadata": {},
   "source": [
    "## Perform Dynamic Batching During Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f31c2f4-c4bf-4901-89d2-0002a4f4b025",
   "metadata": {},
   "source": [
    "Tokenize all the data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d4162-0d3d-421e-a0d1-c7ebca839567",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, fn_kwargs=dict(mytokenizer=tokenizer), batched=True\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b777467-551c-402e-9c50-e44888d9500e",
   "metadata": {},
   "source": [
    "## Dealing With Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007aed5c-389c-4ea9-83d7-441e4fed181b",
   "metadata": {},
   "source": [
    "Create class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd40b84-e990-41be-9e3c-92e0ed4a5fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = (1 - (y_train.value_counts().sort_index() / len(y_train))).values\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1909f8f-4f08-40e1-85f1-1e5ce7603c21",
   "metadata": {},
   "source": [
    "Convert class weights to `pytorch` tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d69c8-4699-4208-92bf-372c28afb4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.from_numpy(class_weights).float()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da745e-b26c-488e-8e34-0f85af261259",
   "metadata": {},
   "source": [
    "Define an instance of the `Trainer` class, that implements a custom `.CrossEntropyLoss()` which uses the above class weights based on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4391695-3120-4aab-9ddc-20940eb82b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract true labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass - feed inputs to model and extract logits\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Define loss function with class weights\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        # Compute loss\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
