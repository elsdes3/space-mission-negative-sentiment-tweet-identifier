{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2926a6-475c-4ba6-bcec-3440e102ca94",
   "metadata": {},
   "source": [
    "# Create Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e755a0-971c-4b0f-993e-bd4f794cf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip3 freeze | grep -E 'boto3|s3fs|scikit-learn|distributed|dask==|dask-m|black==|jupyter-server|pandas'\n",
    "!conda list -n spark | grep -E 'ipykernel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9c137-7951-4fe6-8ac4-88ae864841f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16bcb2f-1b91-4753-8b1b-95f76c4e857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "\n",
    "import boto3\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split as sk_train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ebd833-6c90-4faf-be84-547df67f7cce",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2745891-6f8a-43e9-be97-64f54140764f",
   "metadata": {},
   "source": [
    "### Objective\n",
    "This notebook will split the processed data into training, validation and test splits that can be used to train a machine learning model for twitter sentiment classification.\n",
    "\n",
    "### ML Model Development\n",
    "A random sample of the training split will be further divided into three smaller splits in order to support training a NLP (transformers) model to predict sentiment. This NLP model will be used to label the processed tweets data with sentiment. The NLP model will be used to label the data (i.e. to extract the sentiment) used during ML model development. The ML model will be trained using this labeled data and then deployed.\n",
    "\n",
    "### ML Model Usage in Production\n",
    "In production, the deployed ML model will be used to predict sentiment for 12 hours of incoming tweets. These predictions will be served to customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7966c415-6e0a-4ae2-89a1-2f138bc71fae",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f140590-860a-4423-8cdd-4aad9c25e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folder = \"/datasets/twitter/kinesis-demo/\"\n",
    "\n",
    "# processed data\n",
    "processed_data_dir = \"data/processed\"\n",
    "processed_file_name = \"processed_text\"\n",
    "# cols_to_load = [\n",
    "#     \"id\",\n",
    "#     \"source_text\",\n",
    "#     \"created_at\",\n",
    "#     \"user_joined\",\n",
    "#     \"retweeted_tweet\",\n",
    "#     \"contributors\",\n",
    "#     \"text\",\n",
    "# ]\n",
    "cols_to_load = None\n",
    "\n",
    "# train-test split\n",
    "test_split_frac = 0.125\n",
    "val_split_frac = test_split_frac / (1 - test_split_frac)\n",
    "\n",
    "# sampling data\n",
    "num_sampled_tweets = 10_000\n",
    "sampled_fname = \"sampled_data.csv.zip\"\n",
    "\n",
    "# inference schedule records\n",
    "# - this notebook only supports one record at a time\n",
    "inference_datetime_ranges = [\n",
    "    {\"k\": 0, \"infer_starts_range\": [\"2022-01-07 12:00:00\", \"2022-01-09 12:00:00\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee631341-6575-4593-9960-eba37642312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = os.getenv(\"AWS_S3_BUCKET_NAME\")\n",
    "session = boto3.Session(profile_name=\"default\")\n",
    "s3_client = session.client(\"s3\")\n",
    "\n",
    "dtypes_dict = {\n",
    "    \"id\": pd.StringDtype(),\n",
    "    \"geo\": pd.StringDtype(),\n",
    "    \"coordinates\": pd.StringDtype(),\n",
    "    \"place\": pd.StringDtype(),\n",
    "    \"contributors\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"is_quote_status\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"quote_count\": pd.Int32Dtype(),\n",
    "    \"reply_count\": pd.Int32Dtype(),\n",
    "    \"retweet_count\": pd.Int32Dtype(),\n",
    "    \"favorite_count\": pd.Int32Dtype(),\n",
    "    \"favorited\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"retweeted\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"source\": pd.StringDtype(),\n",
    "    \"in_reply_to_user_id\": pd.StringDtype(),\n",
    "    \"in_reply_to_screen_name\": pd.StringDtype(),\n",
    "    \"source_text\": pd.StringDtype(),\n",
    "    \"place_id\": pd.StringDtype(),\n",
    "    \"place_url\": pd.StringDtype(),\n",
    "    \"place_place_type\": pd.StringDtype(),\n",
    "    \"place_name\": pd.StringDtype(),\n",
    "    \"place_full_name\": pd.StringDtype(),\n",
    "    \"place_country_code\": pd.StringDtype(),\n",
    "    \"place_country\": pd.StringDtype(),\n",
    "    \"place_bounding_box_type\": pd.StringDtype(),\n",
    "    \"place_bounding_box_coordinates\": pd.StringDtype(),\n",
    "    \"place_attributes\": pd.StringDtype(),\n",
    "    \"coords_type\": pd.StringDtype(),\n",
    "    \"coords_lon\": pd.StringDtype(),\n",
    "    \"coords_lat\": pd.StringDtype(),\n",
    "    \"geo_type\": pd.StringDtype(),\n",
    "    \"geo_lon\": pd.StringDtype(),\n",
    "    \"geo_lat\": pd.StringDtype(),\n",
    "    \"user_name\": pd.StringDtype(),\n",
    "    \"user_screen_name\": pd.StringDtype(),\n",
    "    \"user_followers\": pd.Int32Dtype(),\n",
    "    \"user_friends\": pd.Int32Dtype(),\n",
    "    \"user_listed\": pd.Int32Dtype(),\n",
    "    \"user_favourites\": pd.Int32Dtype(),\n",
    "    \"user_statuses\": pd.Int32Dtype(),\n",
    "    \"user_protected\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"user_verified\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"user_contributors_enabled\": pd.StringDtype(),\n",
    "    \"user_location\": pd.StringDtype(),\n",
    "    \"retweeted_tweet\": pd.StringDtype(),\n",
    "    \"tweet_text_urls\": pd.StringDtype(),\n",
    "    \"tweet_text_hashtags\": pd.StringDtype(),\n",
    "    \"tweet_text_usernames\": pd.StringDtype(),\n",
    "    \"num_urls_in_tweet_text\": pd.Int32Dtype(),\n",
    "    \"num_users_in_tweet_text\": pd.Int32Dtype(),\n",
    "    \"num_hashtags_in_tweet_text\": pd.Int32Dtype(),\n",
    "    \"text\": pd.StringDtype(),\n",
    "    \"contains_wanted_text\": pd.BooleanDtype(),\n",
    "    \"contains_wanted_text_case_sensitive\": pd.BooleanDtype(),\n",
    "    \"contains_multi_word_wanted_text\": pd.BooleanDtype(),\n",
    "    \"contains_crypto_terms\": pd.BooleanDtype(),\n",
    "    \"contains_religious_terms\": pd.BooleanDtype(),\n",
    "    \"contains_inappropriate_terms\": pd.BooleanDtype(),\n",
    "    \"contains_video_games_terms\": pd.BooleanDtype(),\n",
    "    \"contains_misc_unwanted_terms\": pd.BooleanDtype(),\n",
    "    \"contains_non_english_terms\": pd.BooleanDtype(),\n",
    "}\n",
    "\n",
    "proc_text_zip_fname = f\"{processed_file_name}.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf4997-cf86-40da-af24-d5a079eedfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_cols(df_cols, cols_to_use):\n",
    "    \"\"\"Highlight a list of columns in a DataFrame.\"\"\"\n",
    "    # copy df to new - original data is not changed\n",
    "    df = df_cols[cols_to_use].copy()\n",
    "    # select all values to yellow color\n",
    "    df.loc[:, :] = \"background-color: yellow\"\n",
    "    # return color df\n",
    "    return df\n",
    "\n",
    "\n",
    "def download_file_from_s3(\n",
    "    s3_bucket_name: str,\n",
    "    path_to_folder: str,\n",
    "    data_dir: str,\n",
    "    fname: str,\n",
    "    aws_region: str,\n",
    "    prefix: str,\n",
    ") -> None:\n",
    "    \"\"\"Download file from .\"\"\"\n",
    "    dest_filepath = os.path.join(data_dir, fname)\n",
    "    s3_filepath_key = s3_client.list_objects_v2(\n",
    "        Bucket=s3_bucket_name,\n",
    "        Delimiter=\"/\",\n",
    "        Prefix=prefix,\n",
    "    )[\"Contents\"][0][\"Key\"]\n",
    "    start = datetime.now()\n",
    "    print(\n",
    "        f\"Started downloading processed data zip file from {s3_filepath_key} to \"\n",
    "        f\"{dest_filepath} at {start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}...\"\n",
    "    )\n",
    "    s3 = boto3.resource(\"s3\", region_name=aws_region)\n",
    "    s3.meta.client.download_file(\n",
    "        s3_bucket_name,\n",
    "        s3_filepath_key,\n",
    "        dest_filepath,\n",
    "    )\n",
    "    duration = (datetime.now() - start).total_seconds()\n",
    "    print(f\"Done downloading in {duration:.3f} seconds.\")\n",
    "\n",
    "\n",
    "def extract_zip_file(dest_filepath: str, data_dir: str) -> None:\n",
    "    \"\"\".\"\"\"\n",
    "    start = datetime.now()\n",
    "    print(\n",
    "        \"Started extracting filtered data parquet files from \"\n",
    "        f\"processed data zip file to {data_dir} at \"\n",
    "        f\"{start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}...\"\n",
    "    )\n",
    "    zip_ref = zipfile.ZipFile(dest_filepath)\n",
    "    zip_ref.extractall(data_dir)\n",
    "    zip_ref.close()\n",
    "    duration = (datetime.now() - start).total_seconds()\n",
    "    print(f\"Done extracting in {duration:.3f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e97e0a-e20e-408c-a77e-400b80f3a051",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9471c1-fa45-4955-8063-5cab8458d75d",
   "metadata": {},
   "source": [
    "We will start by downloaded the processed and filtered `.zip` file from S3 and extracting all the contained `.parquet` files into a `.parquet.gzip` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2969e-1326-481f-86a0-99cb49d18f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not os.path.exists(os.path.join(processed_data_dir, proc_text_zip_fname)):\n",
    "    download_file_from_s3(\n",
    "        s3_bucket_name,\n",
    "        path_to_folder,\n",
    "        processed_data_dir,\n",
    "        proc_text_zip_fname,\n",
    "        session.region_name,\n",
    "        f\"{path_to_folder[1:]}processed/{os.path.splitext(proc_text_zip_fname)[0]}\",\n",
    "    )\n",
    "    extract_zip_file(\n",
    "        os.path.join(processed_data_dir, proc_text_zip_fname),\n",
    "        f\"{processed_data_dir}/{os.path.splitext(proc_text_zip_fname)[0]}.parquet.gzip\",\n",
    "    )\n",
    "proc_files = glob(f\"{processed_data_dir}/*.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14642cfd-3926-437f-8f5e-43d503199104",
   "metadata": {},
   "source": [
    "Use Dask to load the `.parquet.gzip` file (consisting of multiple `.parquet` files) into a single Dask DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4099bd-bae8-40e6-b8c8-d5004c9d9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = dd.read_parquet(proc_files, columns=cols_to_load).astype(dtypes_dict)\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(ddf.head())\n",
    "display(ddf.dtypes.rename(\"dtype\").to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0763a85e-670b-4b5f-830f-6d574fd19823",
   "metadata": {},
   "source": [
    "### Create Data Splits for ML Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c16195-1e7b-4a0d-8a65-0ab4c62d9f71",
   "metadata": {},
   "source": [
    "Get starting (inclusive) and ending (exclusive) `datetime`s for training, validation, testing and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebaceb8-51e4-4004-a84b-c835757f7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_length(split_start, split_end):\n",
    "    ddf_split = ddf[(ddf['created_at'] >= split_start) & (ddf['created_at'] < split_end)]\n",
    "    return len(ddf_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b3f20-6d08-426d-aa2f-e6013bccc649",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cols_order = [\n",
    "    'model_training_num',\n",
    "    'train_starts',\n",
    "    'train_ends',\n",
    "    'val_starts',\n",
    "    'val_ends',\n",
    "    'test_starts',\n",
    "    'test_ends',\n",
    "    'infer_num',\n",
    "    'infer_starts',\n",
    "    'infer_ends',\n",
    "    'inference_time',\n",
    "    'train_len',\n",
    "    'val_len',\n",
    "    'test_len',\n",
    "    'infer_len',\n",
    "    'total_len',\n",
    "    'train_frac',\n",
    "    'val_frac',\n",
    "    'test_frac',\n",
    "    'infer_frac',\n",
    "]\n",
    "dfs_rand_dates = []\n",
    "for k, infer_record in enumerate(inference_datetime_ranges):\n",
    "    v = infer_record['infer_starts_range']\n",
    "    df_rand_dates = (\n",
    "        pd.Series(\n",
    "            pd.date_range(\n",
    "                v[0],  # \"2022-01-07 04:00:00\" or \"2022-01-08 00:00:00\"\n",
    "                v[1],\n",
    "                freq=\"12H\",  # \"4H\" or \"12H\"\n",
    "                name=\"infer_starts\",\n",
    "            )\n",
    "        ).to_frame()\n",
    "        .assign(infer_ends=lambda df: df['infer_starts']+pd.Timedelta(12, unit=\"H\"))\n",
    "        .assign(test_ends=lambda df: df['infer_starts'].min())\n",
    "        .assign(test_starts=lambda df: df['test_ends']-pd.Timedelta(12, unit=\"H\"))\n",
    "        .assign(val_ends=lambda df: df['test_starts'])\n",
    "        .assign(val_starts=lambda df: df['val_ends']-pd.Timedelta(12, unit=\"H\"))\n",
    "        .assign(train_ends=lambda df: df['val_starts'])\n",
    "        .assign(train_starts=datetime(2021, 12, 30, 17, 0, 0))\n",
    "        .assign(inference_time=lambda df: df['infer_ends'])\n",
    "        .assign(infer_num=lambda df: range(len(df)))\n",
    "        .assign(model_training_num=k)\n",
    "        .assign(train_len=lambda df: get_split_length(df.iloc[0][\"train_starts\"], df.iloc[0][\"train_ends\"]))\n",
    "        .assign(val_len=lambda df: get_split_length(df.iloc[0][\"val_starts\"], df.iloc[0][\"val_ends\"]))\n",
    "        .assign(test_len=lambda df: get_split_length(df.iloc[0][\"test_starts\"], df.iloc[0][\"test_ends\"]))\n",
    "        .assign(\n",
    "            infer_len=lambda df: pd.Series(\n",
    "                np.vectorize(get_split_length)(df[\"infer_starts\"], df[\"infer_ends\"]),\n",
    "                index=df.index,\n",
    "                name='infer_length',\n",
    "            )\n",
    "        )\n",
    "        .assign(total_len=len(ddf))\n",
    "        .assign(train_frac=lambda df: df['train_len']/df['total_len'])\n",
    "        .assign(val_frac=lambda df: df['val_len']/df['total_len'])\n",
    "        .assign(test_frac=lambda df: df['test_len']/df['total_len'])\n",
    "        .assign(infer_frac=lambda df: df['infer_len']/df['total_len'])\n",
    "        [cols_order]\n",
    "    )\n",
    "    dfs_rand_dates.append(df_rand_dates)\n",
    "df_rand_dates = pd.concat(dfs_rand_dates)\n",
    "df_rand_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e712cc-0f9b-4c8b-a98a-fbffa3256fe5",
   "metadata": {},
   "source": [
    "## Split Data and Create Sample for Training NLP Labeling (Transformer) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6bed5-c1bb-49f0-bd8f-c304a7d97f51",
   "metadata": {},
   "source": [
    "### Create Data Splits for ML Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b34ca5-abf5-4e09-9a69-2b5779794f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_starts = df_rand_dates.iloc[0][\"train_starts\"]\n",
    "train_ends = df_rand_dates.iloc[0][\"train_ends\"]\n",
    "val_starts = df_rand_dates.iloc[0][\"val_starts\"]\n",
    "val_ends = df_rand_dates.iloc[0][\"val_ends\"]\n",
    "test_starts = df_rand_dates.iloc[0][\"test_starts\"]\n",
    "test_ends = df_rand_dates.iloc[0][\"test_ends\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614b481-2574-43c2-a2fc-ce4f34d84e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = [\n",
    "    ddf[(ddf['created_at'] >= train_starts) & (ddf['created_at'] < train_ends)].sample(frac=1.0),\n",
    "    ddf[(ddf['created_at'] >= val_starts) & (ddf['created_at'] < val_ends)].sample(frac=1.0),\n",
    "    ddf[(ddf['created_at'] >= test_starts) & (ddf['created_at'] < test_ends)].sample(frac=1.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631a7458-87c6-4bc8-adcb-b63cb9926566",
   "metadata": {},
   "source": [
    "Get lengths of inference period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b43da-8d52-4011-8f76-f22b775bc99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_inferences = {}\n",
    "for k, row in df_rand_dates.iterrows():\n",
    "    df_infer = ddf[\n",
    "        (ddf[\"created_at\"] >= row[\"infer_starts\"])\n",
    "        & (ddf[\"created_at\"] < row[\"infer_ends\"])\n",
    "    ]\n",
    "    df_inferences[k] = df_infer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f75255-970f-470a-b701-32032a7aa23f",
   "metadata": {},
   "source": [
    "### Create Data Splits from Training Split, for NLP Model Development (to assign sentiment labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edbb01-60b2-438e-9eb2-13d062d10519",
   "metadata": {},
   "source": [
    "We will now draw a random sample from the training split to use in NLP (transformer) model fine-tuning in order to label the tweets with a sentiment (i.e. in order to extract the sentiment of the text in the tweets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae4f71d-e50e-44fe-9920-f9d06b488dfe",
   "metadata": {},
   "source": [
    "First, we'll define the fraction of the training split to be used in NLP model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29e158-3eaf-423c-b7ec-d4b6c9fb7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nlp_sample_size = num_sampled_tweets / len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a9f2b1-0f1d-4508-9366-192d82275f58",
   "metadata": {},
   "source": [
    "Next, we'll extract a sample of the training data corresponding to this fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d32ed-912c-473f-ba48-d31ff6da89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train_sample = (\n",
    "    df_train.sample(frac=nlp_sample_size, random_state=88)\n",
    "    .compute()\n",
    "    .sort_values(by=[\"created_at\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af036391-a7e3-4ecb-8233-579bc505751e",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. The sample is small enough that it fits in memory and so we don't need to use a big data framework to hold its contents. So, we call `.compute()` to bring this sample into memory and we can use in-memory tools (below) for creating data splits from this sample.\n",
    "2. Before creating the data splits for ML model development, the data was sorted by `datetime` when the tweet was posted (i.e. sorted by the `created_at` column). In order to create the splits for NLP model development in a way that is consistent with how the data splits were created for ML model development, after drawing the random sample, we sort the sampled data by the same `created_at` column before random splits will be created next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eabe1c-f06b-47da-952e-3b4bfe36ef7e",
   "metadata": {},
   "source": [
    "Tweets might be created at the same timestamp and so duplicated values are possible in this column, meaning that unique values in this column will be less than the total number of tweets in the data (including in the mpled data). Tweet IDs are unique for each tweet so the number of unique values in the `id` column will match the number of tweets in the data. These are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80eec0-1471-4c98-a4c0-9fbfc53d60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\n",
    "    f\"Number of unique IDs in sampled data = {df_train_sample['id'].nunique():,}\\n\"\n",
    "    f\"Number of unique creation datetimes in sampled data = {df_train_sample['created_at'].nunique():,}\\n\"\n",
    "    f\"Number of rows in sampled data = {len(df_train_sample):,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49ed88-742b-4ea9-b35f-a71b48a9b2cb",
   "metadata": {},
   "source": [
    "We'll now create random training, validation and testing splits from the sampled data, which will be used during NLP model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3b4ae-6f2f-45bf-bdbf-3413f4f09e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_train_val, df_nlp_test = sk_train_test_split(\n",
    "    df_train_sample, test_size=test_split_frac, random_state=88\n",
    ")\n",
    "df_nlp_train, df_nlp_val = sk_train_test_split(\n",
    "    df_nlp_train_val, test_size=test_split_frac, random_state=88\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d8904a-fb78-4812-8974-7450f67d21e5",
   "metadata": {},
   "source": [
    "## Export All Data Splits to S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108ea13-5cea-4304-86f6-e2f541a6996b",
   "metadata": {},
   "source": [
    "Get the counter for the two-day inference period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598c05f-95a4-40a9-80ce-f9f9a3429b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_idx = inference_datetime_ranges[0][\"k\"]\n",
    "infer_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450730c-b085-4666-a175-59f9d7041bb1",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. The counter indicates which occurrence of two-day inference has been made by a deployed ML model.\n",
    "2. The counter is used in file naming as a crude way to version data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190ab6ff-165a-42a1-b37a-7f098596a5b4",
   "metadata": {},
   "source": [
    "All data spits for ML model development will now be saved to a separate `.parquet` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f7ad3-959e-4ddb-872a-ba7a1ca13d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_end = df_rand_dates.loc[0, \"test_ends\"].strftime(\"%Y%m%d_%H%M%S\")\n",
    "for split_name, df_split_to_export in zip(\n",
    "    [\"train\", \"val\", \"test\"], [df_train, df_val, df_test]\n",
    "):\n",
    "    fname = f\"{split_name}__inference2d_{infer_idx}__{test_end}.parquet.gzip\"\n",
    "    split_filepath = f\"s3://{s3_bucket_name}{path_to_folder}processed/splits/{fname}\"\n",
    "    # df_split_to_export.to_csv(\n",
    "    #     split_filepath,\n",
    "    #     index=False,\n",
    "    #     storage_options={\n",
    "    #         \"key\": session.get_credentials().access_key,\n",
    "    #         \"secret\": session.get_credentials().secret_key,\n",
    "    #     },\n",
    "    # )\n",
    "    dest_path_str = f\"{path_to_folder[1:]}processed/splits/{fname}\"\n",
    "    print(f\"Exported {len(df_split_to_export):,} to {dest_path_str} to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89c40b-f962-4219-a736-8664bd6bf1c1",
   "metadata": {},
   "source": [
    "All sampled data spits for NLP model development will now be saved to a separate `.CSV` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f4a0c-f74a-4fc1-a82f-c0d64f602684",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_end = df_train[\"created_at\"].max().compute().strftime(\"%Y%m%d_%H%M%S\")\n",
    "for split_name, df_split_to_export in zip(\n",
    "    [\"train_nlp\", \"val_nlp\", \"test_nlp\"],\n",
    "    [df_nlp_train, df_nlp_val, df_nlp_test],\n",
    "):\n",
    "    fname = f\"{split_name}__inference2d_{infer_idx}__{train_end}.csv.zip\"\n",
    "    split_filepath = f\"s3://{s3_bucket_name}{path_to_folder}processed/nlp_splits/{fname}\"\n",
    "    # df_split_to_export.to_csv(\n",
    "    #     split_filepath,\n",
    "    #     index=False,\n",
    "    #     storage_options={\n",
    "    #         \"key\": session.get_credentials().access_key,\n",
    "    #         \"secret\": session.get_credentials().secret_key,\n",
    "    #     },\n",
    "    # )\n",
    "    dest_fpath = f\"{path_to_folder[1:]}processed/nlp_splits/{fname}\"\n",
    "    print(f\"Exported {len(df_split_to_export):,} to {dest_fpath} on S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15459708-b479-40b6-979b-bfffef4e75ac",
   "metadata": {},
   "source": [
    "All the data for inference during ML model development, will now be saved to a separate `.parquet` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83b177-877d-40e4-b541-36a1283eeaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for infer_idx, df_infer in df_inferences.items():\n",
    "    infer_start, infer_end = [\n",
    "        df_rand_dates.loc[infer_idx, \"infer_starts\"].strftime(\"%Y%m%d_%H%M%S\"),\n",
    "        df_rand_dates.loc[infer_idx, \"infer_ends\"].strftime(\"%Y%m%d_%H%M%S\")\n",
    "    ]\n",
    "    fname = f\"inference_{infer_idx}__{infer_start}__{infer_end}.parquet.gzip\"\n",
    "    infer_filepath = f\"s3://{s3_bucket_name}{path_to_folder}processed/inference/{fname}\"\n",
    "    # df_infer.to_parquet(\n",
    "    #     infer_filepath,\n",
    "    #     index=False,\n",
    "    #     compression='gzip',\n",
    "    #     storage_options={\n",
    "    #         \"key\": session.get_credentials().access_key,\n",
    "    #         \"secret\": session.get_credentials().secret_key,\n",
    "    #     },\n",
    "    # )\n",
    "    dest_path_str = f\"{path_to_folder[1:]}processed/inference/{fname}\"\n",
    "    print(f\"Exported {len(df_infer):,} rows to {dest_path_str} on S3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
