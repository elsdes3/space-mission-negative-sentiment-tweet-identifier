{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deaace75-6698-474b-98a5-3f471855736e",
   "metadata": {},
   "source": [
    "# Data Preparation - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc1605a-247a-40e4-8c3e-0c6697c10156",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349d30fb-3eb7-4d14-bbd5-55ca30982e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from dotenv import find_dotenv, load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40361fd-4a61-4a17-b525-e60bb2847e95",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242299c1-e8a9-4034-90be-84f620fbb003",
   "metadata": {},
   "source": [
    "Updated data processing in preparation for `v2` analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511ce523-216a-418e-82d2-e940d62bfbed",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2853ff27-5b0f-48d8-9fe5-0c417f6ec7c4",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "process_raw_files = False\n",
    "remove_short_tweets = False\n",
    "\n",
    "path_to_folder = \"/datasets/twitter/kinesis-demo/\"\n",
    "years_wanted = [2021] + [2022]\n",
    "\n",
    "# List of headers for all streamed twitter attributes\n",
    "headers = [\n",
    "    \"id\",\n",
    "    \"geo\",\n",
    "    \"coordinates\",\n",
    "    \"place\",\n",
    "    \"contributors\",\n",
    "    \"is_quote_status\",\n",
    "    \"quote_count\",\n",
    "    \"reply_count\",\n",
    "    \"retweet_count\",\n",
    "    \"favorite_count\",\n",
    "    \"favorited\",\n",
    "    \"retweeted\",\n",
    "    \"created_at\",\n",
    "    \"source\",\n",
    "    \"in_reply_to_user_id\",\n",
    "    \"in_reply_to_screen_name\",\n",
    "    \"source_text\",\n",
    "    \"place_id\",\n",
    "    \"place_url\",\n",
    "    \"place_place_type\",\n",
    "    \"place_name\",\n",
    "    \"place_full_name\",\n",
    "    \"place_country_code\",\n",
    "    \"place_country\",\n",
    "    \"place_bounding_box_type\",\n",
    "    \"place_bounding_box_coordinates\",\n",
    "    \"place_attributes\",\n",
    "    \"coords_type\",\n",
    "    \"coords_lon\",\n",
    "    \"coords_lat\",\n",
    "    \"geo_type\",\n",
    "    \"geo_lon\",\n",
    "    \"geo_lat\",\n",
    "    \"user_name\",\n",
    "    \"user_screen_name\",\n",
    "    \"user_followers\",\n",
    "    \"user_friends\",\n",
    "    \"user_listed\",\n",
    "    \"user_favourites\",\n",
    "    \"user_statuses\",\n",
    "    \"user_protected\",\n",
    "    \"user_verified\",\n",
    "    \"user_contributors_enabled\",\n",
    "    \"user_joined\",\n",
    "    \"user_location\",\n",
    "    \"retweeted_tweet\",\n",
    "    \"tweet_text_urls\",\n",
    "    \"tweet_text_hashtags\",\n",
    "    \"tweet_text_usernames\",\n",
    "    \"num_urls_in_tweet_text\",\n",
    "    \"num_users_in_tweet_text\",\n",
    "    \"num_hashtags_in_tweet_text\",\n",
    "    \"text\",\n",
    "]\n",
    "string_cols = [\n",
    "    \"favorited\",\n",
    "    \"retweeted\",\n",
    "    # 'is_quote_status',  # contains True, False, 0, 1, 2, 3\n",
    "    \"user_protected\",\n",
    "    \"user_verified\",\n",
    "    \"user_contributors_enabled\",\n",
    "]\n",
    "text_cols = [\n",
    "    \"user_name\",\n",
    "    \"user_screen_name\",\n",
    "    \"user_location\",\n",
    "    \"place_name\",\n",
    "    \"place_full_name\",\n",
    "    \"place_country_code\",\n",
    "    \"place_country\",\n",
    "    \"tweet_text_urls\",\n",
    "    \"source_text\",\n",
    "    \"text\",\n",
    "]\n",
    "int_cols = [\n",
    "    \"quote_count\",\n",
    "    \"reply_count\",\n",
    "    # 'retweet_count',  # monirity of rows are text\n",
    "    \"favorite_count\",\n",
    "    \"user_followers\",\n",
    "    \"user_friends\",\n",
    "    \"user_listed\",\n",
    "    \"user_favourites\",\n",
    "    \"user_statuses\",\n",
    "    \"num_urls_in_tweet_text\",\n",
    "    \"num_users_in_tweet_text\",\n",
    "    \"num_hashtags_in_tweet_text\",\n",
    "]\n",
    "datetime_cols = [\"created_at\", \"user_joined\"]\n",
    "\n",
    "# List of partial strings to use to filter out unwanted tweets\n",
    "# - tweets containing sensitive tweet texts that were found retrospectively\n",
    "#   and should be excluded from the CSV files\n",
    "unwanted_partial_strings_list = [\n",
    "    # specific to crypto mining\n",
    "    \"crypto\",\n",
    "    \"token\",\n",
    "    \"koistarter\",\n",
    "    \"daostarter\",\n",
    "    \"decentralized\",\n",
    "    \"services\",\n",
    "    \"pancakeswap\",\n",
    "    \"eraxnft\",\n",
    "    \"browsing\",\n",
    "    \"kommunitas\",\n",
    "    \"hosting\",\n",
    "    \"internet\",\n",
    "    \"exipofficial\",\n",
    "    \"servers\",\n",
    "    \"wallet\",\n",
    "    \"liquidity\",\n",
    "    \"rewards\",\n",
    "    \"floki\",\n",
    "    \"10000000000000linkstelegram\",\n",
    "    \"dogecoin\",\n",
    "    \"czbinance\",\n",
    "    \"watch\",\n",
    "    \"binance\",\n",
    "    \"dogelonmars\",\n",
    "    \"cryptocurrency\",\n",
    "    \"hbomax\",\n",
    "    \"money\",\n",
    "    \"danheld\",\n",
    "    \"cybersecurity\",\n",
    "    # others\n",
    "    \"prostitution\",\n",
    "    \"nairobi\",\n",
    "    \"musembe\",\n",
    "    \"volcano detected\",\n",
    "    \"block-2\",\n",
    "    \"mo-greene\",\n",
    "    \"running scared2012\",\n",
    "    \"running scared 2012\",\n",
    "    \"massacres\",\n",
    "    \"eric ephriam chavez\",\n",
    "    \"drugs\",\n",
    "    \"tanzanite\",\n",
    "    \"vvsorigin\",\n",
    "    \"gemstonecarat\",\n",
    "    \"bin laden\",\n",
    "    \"saddam\",\n",
    "    \"webuye\",\n",
    "    \"bungoma\",\n",
    "    \"perished\",\n",
    "    \"popescu\",\n",
    "    \"whore\",\n",
    "    \"nasty\",\n",
    "    \"ethereum\",\n",
    "    \"pay someone\",\n",
    "    \"gamejoin\",\n",
    "    \"nft\",\n",
    "    \"breeding\",\n",
    "    \"seungkwan\",\n",
    "    \"woozi\",\n",
    "    \"hoshi\",\n",
    "    \"bitcrush\",\n",
    "    \"arcade\",\n",
    "    \"homeworkpay\",\n",
    "    \"homework\",\n",
    "    \"photocards\",\n",
    "    \"deta\",\n",
    "    \"marketing\",\n",
    "    \"dreamcast\",\n",
    "    \"sega\",\n",
    "    \"xbox\",\n",
    "    \"wii\",\n",
    "    \"ps4\",\n",
    "    \"kasama\",\n",
    "    \"nung\",\n",
    "    \"lahat\",\n",
    "    \"jinsoul\",\n",
    "    \"brunisoul\",\n",
    "    \"loona\",\n",
    "    \"taas\",\n",
    "    \"nung\",\n",
    "    \"essay\",\n",
    "    # religious\n",
    "    \"scriptures\",\n",
    "    \"methusealah\",\n",
    "    \"testament\",\n",
    "    \"yahweh\",\n",
    "    \"god\",\n",
    "    \"mullah\",\n",
    "    \"allah\",\n",
    "    \"clergy\",\n",
    "    \"mercy\",\n",
    "    \"morality\",\n",
    "    \"muslims,\",\n",
    "    \"hindus\",\n",
    "    \"buddhist\",\n",
    "    \"catholics\",\n",
    "    \"christians\",\n",
    "    \"atheist\",\n",
    "    # inappropriate\n",
    "    \"nazist\",\n",
    "    \"antifa\",\n",
    "    \"proud boys\",\n",
    "]\n",
    "\n",
    "raw_data_filepath = \"data/raw/raw_data.parquet.gzip\"\n",
    "proc_data_filepath = \"data/processed/processed_data\"\n",
    "\n",
    "blanks_threshold = 0.6\n",
    "nones_threshold = 0.8\n",
    "object_dtype_mapper = {\n",
    "    \"id\": pd.StringDtype(),\n",
    "    \"source\": pd.StringDtype(),\n",
    "    \"is_quote_status\": pd.BooleanDtype(),\n",
    "}\n",
    "empty_array_cols = [\"place_bounding_box_coordinates\"]\n",
    "profanities_word_list_url = (\n",
    "    \"https://raw.githubusercontent.com/ben174/profanity/\"\n",
    "    \"master/profanity/data/wordlist.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b52c94-31b2-4969-913e-e0915a9509bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "aws_region = os.getenv(\"AWS_REGION\", default=\"us-east-2\")\n",
    "s3_bucket_name = os.getenv(\"AWS_S3_BUCKET_NAME\")\n",
    "profanities = pd.read_csv(profanities_word_list_url).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73201b-ba00-4548-b318-1b1722e35f9f",
   "metadata": {},
   "source": [
    "## Get AWS Python SDK Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e994e616-83c1-40b4-af3d-a441c18165f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\", region_name=aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2df80c-7529-466b-a540-91dccd21a2d6",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf7f241-3f1c-4ac4-8cb0-10448e122370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hourly_folders_per_day(\n",
    "    s3_bucket_name: str, path_to_folder: str, years_wanted: List[int]\n",
    ") -> List[str]:\n",
    "    \"\"\"Get list of S3 hourly data folders, per day of streamed data.\"\"\"\n",
    "    list_of_hourly_dirs = []\n",
    "    for year in years_wanted:\n",
    "        monthly_prefixes = s3_client.list_objects_v2(\n",
    "            Bucket=s3_bucket_name,\n",
    "            Prefix=f\"{path_to_folder[1:]}{year}/\",\n",
    "            Delimiter=\"/\",\n",
    "        )[\"CommonPrefixes\"]\n",
    "        # print(monthly_prefixes)\n",
    "\n",
    "        for monthly_prefix in monthly_prefixes:\n",
    "            daily_prefixes = s3_client.list_objects_v2(\n",
    "                Bucket=s3_bucket_name,\n",
    "                Prefix=monthly_prefix[\"Prefix\"],\n",
    "                Delimiter=\"/\",\n",
    "            )[\"CommonPrefixes\"]\n",
    "            # print(monthly_prefix, daily_prefixes)\n",
    "\n",
    "            for daily_prefix in daily_prefixes:\n",
    "                hourly_prefixes = s3_client.list_objects_v2(\n",
    "                    Bucket=s3_bucket_name,\n",
    "                    Prefix=daily_prefix[\"Prefix\"],\n",
    "                    Delimiter=\"/\",\n",
    "                )[\"CommonPrefixes\"]\n",
    "                # print(\n",
    "                #     monthly_prefix,\n",
    "                #     # daily_prefixes,\n",
    "                #     hourly_prefixes,\n",
    "                # )\n",
    "                list_of_hourly_dirs.append(hourly_prefixes)\n",
    "    list_of_hourly_dirs_flat = [sl[\"Prefix\"] for l in list_of_hourly_dirs for sl in l]\n",
    "    print(f\"Found {len(list_of_hourly_dirs_flat):,} hourly folders\")\n",
    "    for hourly_dirs in list_of_hourly_dirs_flat:\n",
    "        print(hourly_dirs)\n",
    "    return list_of_hourly_dirs_flat\n",
    "\n",
    "\n",
    "def read_files_per_hour(\n",
    "    s3_bucket_name: str, flat_list_of_hourly_dirs: List[str]\n",
    ") -> List[str]:\n",
    "    \"\"\"Read individual files in each hourly folder in the S3 bucket.\"\"\"\n",
    "    file_contents_all = []\n",
    "    for q, list_of_hourly_dirs in enumerate(flat_list_of_hourly_dirs, 1):\n",
    "        objects_hourly_all = s3_client.list_objects_v2(\n",
    "            Bucket=s3_bucket_name, Prefix=list_of_hourly_dirs\n",
    "        )\n",
    "        file_contents_list = []\n",
    "        for k, file_obj_dict in enumerate(objects_hourly_all[\"Contents\"], 1):\n",
    "            file_body = s3_client.get_object(\n",
    "                Bucket=s3_bucket_name, Key=file_obj_dict.get(\"Key\")\n",
    "            )[\"Body\"].read()\n",
    "            print(\n",
    "                f\"Dir {q}/{len(flat_list_of_hourly_dirs)} - \"\n",
    "                f\"{list_of_hourly_dirs}, reading object \"\n",
    "                f\"{k}/{len(objects_hourly_all['Contents'])}\"\n",
    "            )\n",
    "            file_contents_list.append(file_body)\n",
    "        print(\n",
    "            f\"Dir {q}/{len(flat_list_of_hourly_dirs)} - {list_of_hourly_dirs} contains \"\n",
    "            f\"{len(file_contents_list):,} file objects\"\n",
    "        )\n",
    "        file_contents_all.append(file_contents_list)\n",
    "    file_contents_all_flat = [\n",
    "        file_contents\n",
    "        for file_contents_list in file_contents_all\n",
    "        for file_contents in file_contents_list\n",
    "    ]\n",
    "    return file_contents_all_flat\n",
    "\n",
    "\n",
    "def convert_file_contents_to_df(file_contents_all_flat: List) -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    nested_list_of_records = []\n",
    "    for file_body in file_contents_all_flat:\n",
    "        list_of_records = file_body.decode(\"utf-8\").split(\"\\n\")[:-1]\n",
    "        nested_list_of_records.append(list_of_records)\n",
    "    df = pd.DataFrame(\n",
    "        [record.split(\"\\t\")[:-1] for sl in nested_list_of_records for record in sl]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_column_headers(df: pd.DataFrame, headers: List[str]) -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    num_rows = len(df)\n",
    "    df_mismatched = df[~df.iloc[:, -2:].isna().all(axis=1)]\n",
    "    mismatched_rows = len(df_mismatched)\n",
    "    # with pd.option_context(\"display.max_columns\", None):\n",
    "    #     display(df_mismatched.style.set_caption(f\"{mismatched_rows:,} Mismatched rows\"))\n",
    "    df = df.loc[df.iloc[:, -2:].isna().all(axis=1)].drop(columns=[53, 54])\n",
    "    assert df.shape[1] == len(headers)\n",
    "    df.columns = headers\n",
    "    num_new_rows = len(df)\n",
    "    print(f\"Dropped {(num_rows - num_new_rows):,} mismatched rows out of {num_rows:,}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def str_to_bool(str_value: str) -> bool:\n",
    "    \"\"\"\n",
    "    Convert a string to a boolean.\n",
    "\n",
    "    Source\n",
    "    ------\n",
    "    https://python.tutorialink.com/how-to-convert-multiple-pandas-columns-from-string-boolean-to-boolean/\n",
    "    \"\"\"\n",
    "    if str_value == \"True\":\n",
    "        return True\n",
    "    elif str_value == \"False\":\n",
    "        return False\n",
    "    else:  # str_value == 'None':\n",
    "        return None\n",
    "\n",
    "\n",
    "def convert_string_cols_to_bool(df, string_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Converting string columns to booleans...\")\n",
    "    for c in string_cols:\n",
    "        df[c] = df[c].apply(str_to_bool).astype(pd.BooleanDtype())\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_text_cols_to_str(df: pd.DataFrame, text_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Converting text columns to strings...\")\n",
    "    for c in text_cols:\n",
    "        df[c] = df[c].astype(pd.StringDtype())\n",
    "        df.loc[df[c].isin([\"None\"]), c] = None  # done for user_location\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_text_cols_to_int(df: pd.DataFrame, int_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Converting string columns to integers...\")\n",
    "    for c in int_cols:\n",
    "        df[c] = df[c].astype(pd.Int16Dtype())\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_datetime_cols(df: pd.DataFrame, datetime_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Converting datetime columns...\")\n",
    "    for c in datetime_cols:\n",
    "        df[c] = pd.to_datetime(df[c])\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_nans(\n",
    "    df: pd.DataFrame, columns_to_drop_nans: List[str] = [\"text\"]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Dropping columns with missing values...\")\n",
    "    df = df.dropna(how=\"all\")\n",
    "    df = df.dropna(subset=columns_to_drop_nans)\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_unwanted_partial_tweets(\n",
    "    df: pd.DataFrame,\n",
    "    columns_to_drop_tweets: List[str],\n",
    "    unwanted_partial_strings_list: List[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Removing texts with unwanted substrings...\")\n",
    "    unwanted_partial_strings = \"|\".join(unwanted_partial_strings_list)\n",
    "    for c in columns_to_drop_tweets:\n",
    "        df = df[~df[c].str.lower().str.contains(unwanted_partial_strings)]\n",
    "    print(\"Done.\")\n",
    "    return df.copy()\n",
    "\n",
    "\n",
    "def drop_retweets(\n",
    "    df: pd.DataFrame,\n",
    "    retweet_col: str = \"retweeted_tweet\",\n",
    "    retweet_value: str = \"yes\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Removing re-tweets...\")\n",
    "    df = df[df[retweet_col] != retweet_value]\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_empty_tweets(df: pd.DataFrame, tweet_col: str = \"text\") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Removing empty (blank) tweets...\")\n",
    "    df = df[df[tweet_col] != \"\"]\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_to_parquet(df: pd.DataFrame, filepath: str) -> None:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Saving data to a parquet file...\")\n",
    "    df.to_parquet(filepath, index=False)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "def upload_file_to_s3(\n",
    "    local_fpath: str,\n",
    "    s3_fpath: str,\n",
    "    s3_bucket_name: str,\n",
    "    aws_region: str = \"us-east-2\",\n",
    ") -> None:\n",
    "    \"\"\"Upload single file to S3.\"\"\"\n",
    "    print(f\"Uploading {local_fpath} to {s3_bucket_name}/{s3_fpath}...\")\n",
    "    s3_client = boto3.client(\"s3\", region_name=aws_region)\n",
    "    s3_client.upload_file(local_fpath, s3_bucket_name, s3_fpath)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "def remove_html_ampersand(\n",
    "    df: pd.DataFrame,\n",
    "    text_col: str = \"text\",\n",
    "    ampersand_html_substring: str = \"&amp;\",\n",
    "    replacement_text: str = \" & \",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(f\"Removing {ampersand_html_substring} from text column...\")\n",
    "    df[text_col] = df[text_col].str.replace(\n",
    "        ampersand_html_substring, replacement_text, regex=False\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_multiple_whitespaces(\n",
    "    df: pd.DataFrame, text_col: str = \"text\", proc_text_col: str = \"processed_text\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Removing multiple whitespaces from text column...\")\n",
    "    df[\"processed_text\"] = df[text_col].str.replace(\"[^\\w\\s]\", \"\", regex=True)\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_leading_trailing_spaces(\n",
    "    df: pd.DataFrame, text_col: str = \"text\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Removing leading and trailing whitespaces from text column...\")\n",
    "    df[text_col] = df[text_col].str.strip()\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def change_text_to_lowercase(df: pd.DataFrame, text_col: str = \"text\") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Changing text to lowercase...\")\n",
    "    df[text_col] = df[text_col].str.lower()\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_special_characters(df: pd.DataFrame, text_col: str = \"text\") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Removing special characters from text column...\")\n",
    "    df[text_col] = df[text_col].str.replace(r\"[^a-zA-z]\", \" \", regex=True)\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_punctuation(df: pd.DataFrame, text_col: str = \"text\") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Removing punctuation from text column...\")\n",
    "    df[text_col] = df[text_col].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_words_from_text(\n",
    "    df: pd.DataFrame, text_col: str = \"text\", words_col: str = \"words\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Getting words from text...\")\n",
    "    df[words_col] = df[text_col].str.split(\"\\s+\")\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_tweets_by_num_words(\n",
    "    df: pd.DataFrame, words_col: str = \"words\", min_num_words: int = 15\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Filtering tweets by number of words...\")\n",
    "    df = df[df[words_col].str.len() > min_num_words]\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_non_useful_cols(\n",
    "    df: pd.DataFrame, nones_threshold: float, blanks_threshold: float\n",
    ") -> List[List[str]]:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Getting lists of non-useful columns...\")\n",
    "    includes_nan_cols = []\n",
    "    includes_blanks_cols = []\n",
    "    single_value_cols = []\n",
    "    for c in df.dtypes.to_frame().loc[df.dtypes == \"object\"].index.tolist():\n",
    "        if c not in [\"words\"]:\n",
    "            # print(c)\n",
    "            includes_nans = set(df[c].unique().tolist()) > set([\"None\"])\n",
    "            includes_blanks = set(df[c].unique().tolist()) > set([\"\"])\n",
    "            single_value = df[c].nunique() == 1\n",
    "            if includes_nans:\n",
    "                includes_nan_cols.append(c)\n",
    "            if includes_blanks:\n",
    "                includes_blanks_cols.append(c)\n",
    "            if single_value:\n",
    "                single_value_cols.append(c)\n",
    "\n",
    "    mostly_nan_or_blanks = []\n",
    "    for c in includes_nan_cols + includes_blanks_cols:\n",
    "        nan_frac = len(df[df[c] == \"None\"]) / len(df)\n",
    "        blank_frac = len(df[df[c] == \"\"]) / len(df)\n",
    "        print(\n",
    "            f\"Column = {c}, Missing fraction: {nan_frac:.2f}, \"\n",
    "            f\"Blank fraction: {blank_frac:.2f}\"\n",
    "        )\n",
    "        if nan_frac > nones_threshold or blank_frac > blanks_threshold:\n",
    "            mostly_nan_or_blanks.append(c)\n",
    "    print(\"Done.\")\n",
    "    return [single_value_cols, mostly_nan_or_blanks]\n",
    "\n",
    "\n",
    "def drop_columns_from_list(\n",
    "    df: pd.DataFrame, cols_to_drop: List[str], cols_type: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(f\"Dropping {cols_type} columns from list...\")\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def change_object_cols_dtype(\n",
    "    df: pd.DataFrame, object_cols_to_bool_dict: Dict, mapper_dict: Dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\".\"\"\"\n",
    "    print(\"Changing datatype of object columns...\")\n",
    "    df = df.astype(object_cols_to_bool_dict).astype(mapper_dict)\n",
    "    print(\"Done.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b7831-d2cb-42c5-a865-7623a14579d6",
   "metadata": {},
   "source": [
    "## Get Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988048c-f1c5-4463-8578-c225fbf61c71",
   "metadata": {},
   "source": [
    "### Get List of Hourly File Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011b0e1-b301-468e-8856-b364b5b9183c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if process_raw_files:\n",
    "    list_of_hourly_dirs_flat = get_hourly_folders_per_day(\n",
    "        s3_bucket_name, path_to_folder, years_wanted\n",
    "    )\n",
    "    file_contents_all_flat = read_files_per_hour(s3_bucket_name, list_of_hourly_dirs_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b9ed8-fe8c-44ed-b9d4-d5c98a8249c0",
   "metadata": {},
   "source": [
    "### Convert List to `DataFrame` and Save to Local Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac82fd6-6246-4512-a0fb-925dbdd04ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if process_raw_files:\n",
    "    df = (\n",
    "        convert_file_contents_to_df(file_contents_all_flat)\n",
    "        .pipe(add_column_headers, headers=headers)\n",
    "        .pipe(save_to_parquet, filepath=raw_data_filepath)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c256609-83e5-457a-8e72-717a0b2858de",
   "metadata": {},
   "source": [
    "## (Optional) Load Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d2c0b-5924-4238-a972-31da874d8f84",
   "metadata": {},
   "source": [
    "### Read Data from Local Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a0a970-80e3-4221-90ef-8929c4988e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not process_raw_files:\n",
    "    df = pd.read_parquet(raw_data_filepath).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc4e3c-1f73-4b8e-9b59-fd6b08cbedbc",
   "metadata": {},
   "source": [
    "### Inspect Column Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616fa9a-9c25-4687-824a-bd8cfcfabe57",
   "metadata": {},
   "source": [
    "Show values for columns of text to be converted to `boolean`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564cc5d-1f69-457e-956d-2cb9ae6bde7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in string_cols:\n",
    "    display(df[c].value_counts(dropna=False).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cbc39a-855f-4869-b1e0-91a6a8bff227",
   "metadata": {},
   "source": [
    "Show values for columns of text to be converted to `string`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ae8ac-81e5-46a3-990a-c9172e7a2fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in text_cols:\n",
    "    display(df[c].value_counts(dropna=False).to_frame())\n",
    "    print(len(df), len(df[df[c].isin([\"None\"])]), df[c].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5ee1c-9e20-4ef8-bd27-6ecc6cb6bf0d",
   "metadata": {},
   "source": [
    "Show values for columns of text to be converted to `integer`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc502e2e-a9c8-4348-8de2-63afbc7d4b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in int_cols:\n",
    "    display(df[c].value_counts(dropna=False).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216414f2-5997-438e-962c-44199a40ac4e",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c1881-e5d8-43bf-b553-e8da2e94fc6d",
   "metadata": {},
   "source": [
    "### Set Data Types for `DataFrame` Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac9d05e-b9fc-4b02-b145-142128b2f850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = (\n",
    "    df\n",
    "    .pipe(convert_string_cols_to_bool, string_cols=string_cols)\n",
    "    .pipe(convert_text_cols_to_str, text_cols=text_cols)\n",
    "    .pipe(convert_text_cols_to_int, int_cols=int_cols)\n",
    "    # .pipe(convert_datetime_cols, datetime_cols=datetime_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b39a4-ed72-4c05-8d1d-15197f761901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Number of rows after loading data = {len(df):,}\")\n",
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    display(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24f8a35-5720-4bc0-bbfc-d15433d33840",
   "metadata": {},
   "source": [
    "Show values for columns of text converted to `boolean`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe35e9f-272a-4d8b-96f3-f9f2d44c938a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in string_cols:\n",
    "    display(df[c].value_counts(dropna=False).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e5451-8013-45ce-baad-c6b2958a1c1f",
   "metadata": {},
   "source": [
    "Show values for columns of text converted to `string`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf554fb-0974-44c4-9ed1-cb1a0e4da981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in text_cols:\n",
    "    display(df[c].value_counts(dropna=False).to_frame())\n",
    "    print(len(df), len(df[df[c].isin([\"None\"])]), df[c].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5efdd4-564d-4bbd-808c-b2334c515eab",
   "metadata": {},
   "source": [
    "Show values for columns of text converted to `integer`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb8902-b89d-4bb1-8506-87ab4570efc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in int_cols:\n",
    "    display(df[c].value_counts(dropna=False).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c1694-569d-48a2-9ee7-444cd14f391a",
   "metadata": {},
   "source": [
    "## Process Text Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ecfe1-9c3d-45fc-9a70-70fd14455bee",
   "metadata": {},
   "source": [
    "### Cleaning Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806ba83-9325-44e2-9e79-2a7d50363cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = (\n",
    "    df\n",
    "    .pipe(\n",
    "        remove_html_ampersand,\n",
    "        text_col=\"text\",\n",
    "        ampersand_html_substring='&amp;',\n",
    "        replacement_text=' and ',\n",
    "    )\n",
    "    .pipe(change_text_to_lowercase, text_col='text')\n",
    "    .pipe(\n",
    "        drop_unwanted_partial_tweets,\n",
    "        columns_to_drop_tweets=['text'],\n",
    "        unwanted_partial_strings_list=list(set(unwanted_partial_strings_list + profanities)),\n",
    "    )\n",
    "    .pipe(remove_multiple_whitespaces, text_col='text', proc_text_col='processed_text')\n",
    "    .pipe(remove_leading_trailing_spaces, text_col='processed_text')\n",
    "    .pipe(remove_special_characters, text_col='processed_text')\n",
    "    .pipe(remove_punctuation, text_col='processed_text')\n",
    "    .pipe(get_words_from_text, text_col='processed_text', words_col='words')\n",
    ")\n",
    "print(f\"Number of rows in processed data = {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2478196-dd0a-4ab2-9f6f-71b5810b11f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "assert df[\"text\"].str.contains(\"&amp;\").sum() == 0\n",
    "assert df[\"text\"].str.contains(\n",
    "    \"|\".join(list(set(unwanted_partial_strings_list + profanities)))\n",
    ").sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e0ac3-ee73-46a9-9a79-5ff51b6df6e8",
   "metadata": {},
   "source": [
    "### Filtering Data Based on Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8e90c-2d0d-4720-b1fd-2de4380671c5",
   "metadata": {},
   "source": [
    "Drop rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b3d121-d005-4014-a850-49d2b2b22891",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df.pipe(drop_nans, columns_to_drop_nans=[\"processed_text\"])\n",
    "print(\n",
    "    f\"Number of rows after dropping missing values in the tweet text column = {len(df):,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687d0c6-0e39-4cdc-9268-4e9b9ab775f8",
   "metadata": {},
   "source": [
    "Drop retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4f7f6-e9ae-452c-925f-6782459b41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df.pipe(drop_retweets, retweet_col=\"retweeted_tweet\", retweet_value=\"yes\")\n",
    "print(f\"Number of rows after removing re-tweets = {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353fc680-fdd9-4f79-8a0b-dae0d996f8a8",
   "metadata": {},
   "source": [
    "Drop blank tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0972f8-6872-404e-a989-dc2036aed5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df.pipe(drop_empty_tweets, tweet_col=\"processed_text\")\n",
    "print(f\"Number of rows after removing blank tweets = {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf5f4bc-905d-4619-ad3b-17ece5c0e740",
   "metadata": {},
   "source": [
    "(Optional) Remove short tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d9bb6-8edb-46f5-955c-6cf76f696895",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if remove_short_tweets:\n",
    "    df = df.pipe(filter_tweets_by_num_words, words_col='words', min_num_words=15)\n",
    "    print(\n",
    "        \"Number of rows in processed data, after filtering out tweets based on \"\n",
    "        f\"length of text = {len(df):,}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1fd34-ce82-48f1-a5c2-81c433aa86e6",
   "metadata": {},
   "source": [
    "## Process Non-Text Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a0811-0303-4165-ab65-1b68e4df47f6",
   "metadata": {},
   "source": [
    "### Get Columns Without Useful Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6f253-08b0-48aa-b8b5-17fc3218809e",
   "metadata": {},
   "source": [
    "We will get the following types of columns\n",
    "- column only contains the text `None` or a blank string as the unique value\n",
    "- column contains the text `None` or a blank string as one of the unique values\n",
    "- column contains a single unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed14f6-af60-4ece-9858-6e254a83661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "only_nan_cols = []\n",
    "includes_nan_cols = []\n",
    "only_blanks_cols = []\n",
    "includes_blanks_cols = []\n",
    "single_value_cols = []\n",
    "for c in df.dtypes.to_frame().loc[df.dtypes == \"object\"].index.tolist():\n",
    "    if c not in [\"words\"]:\n",
    "        # print(c)\n",
    "        only_nans = df[c].unique().tolist() == [\"None\"]\n",
    "        only_blanks = df[c].unique().tolist() == [\"\"]\n",
    "        includes_nans = set(df[c].unique().tolist()) > set([\"None\"])\n",
    "        includes_blanks = set(df[c].unique().tolist()) > set([\"\"])\n",
    "        single_value = df[c].nunique() == 1\n",
    "        if only_nans:\n",
    "            # print(c, includes_nans, only_nans)\n",
    "            only_nan_cols.append(c)\n",
    "        if includes_nans:\n",
    "            includes_nan_cols.append(c)\n",
    "        if only_blanks:\n",
    "            only_blanks_cols.append(c)\n",
    "        if includes_blanks:\n",
    "            includes_blanks_cols.append(c)\n",
    "        if single_value:\n",
    "            single_value_cols.append(c)\n",
    "print(only_nan_cols)\n",
    "print(includes_nan_cols)\n",
    "print(only_blanks_cols)\n",
    "print(includes_blanks_cols)\n",
    "print(single_value_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf7449-679b-4f4d-9105-0f2fbbbec677",
   "metadata": {},
   "source": [
    "Drop columns containing a single unique value (by definition, this will drop columns containing only `None` or blank strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbcb895-4dff-47c8-8299-f0e83a6103e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df.drop(columns=single_value_cols)\n",
    "# # not necessary\n",
    "# df = df.drop(columns=only_nan_cols).drop(columns=only_blanks_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409fab32-91ed-4ce0-a6cf-32f65f99ce6f",
   "metadata": {},
   "source": [
    "Show columns that include `None` or a blank string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653f35f-fc8c-42d6-8952-330ccc8cc760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# blanks_threshold = 0.6\n",
    "# nones_threshold = 0.8\n",
    "mostly_nan_or_blanks = []\n",
    "for c in includes_nan_cols + includes_blanks_cols:\n",
    "    print(c, len(df[df[c] == 'None']) / len(df), len(df[df[c] == '']) / len(df))\n",
    "    if (\n",
    "        (len(df[df[c] == 'None']) / len(df)) > nones_threshold\n",
    "        or (len(df[df[c] == '']) / len(df)) > blanks_threshold\n",
    "    ):\n",
    "        mostly_nan_or_blanks.append(c)\n",
    "print(mostly_nan_or_blanks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb14d3fd-bd27-4ed9-b357-a9588fcc20e4",
   "metadata": {},
   "source": [
    "Drop columns that include `None` or a blank string above the acceptable threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc091b6-d422-4fad-bf66-ad4a8ebf6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df.drop(columns=mostly_nan_or_blanks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52231031-5059-45fc-94ee-0e010d93d944",
   "metadata": {},
   "source": [
    "Show the datatype and unique values in the remaining `object` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1de33-50a8-4e42-9d31-433c0d32663f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for c in [\n",
    "    c\n",
    "    for c in df.dtypes.to_frame().loc[df.dtypes == \"object\"].index.tolist()\n",
    "    if c != \"words\"\n",
    "]:\n",
    "    print(c, df[c].dtype)\n",
    "    display(df[c].value_counts(normalize=True, dropna=False).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114aa67-63e9-4be2-8d18-cfd88a9eac53",
   "metadata": {},
   "source": [
    "Drop columns if they mostly contain an empty `list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7141eb4-9ace-44fa-ae9b-897435b43946",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# empty_array_cols = ['place_bounding_box_coordinates']\n",
    "df = df.drop(columns=empty_array_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5c74e-7666-4689-ab2a-1714d5985aa8",
   "metadata": {},
   "source": [
    "Change the datatype of the remaining `object` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc70df2e-48a1-42c8-95ad-e5c6e859f3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "object_dtype_mapper = {\n",
    "#     \"id\": pd.StringDtype(),\n",
    "#     'source': pd.StringDtype(),\n",
    "#     'is_quote_status': pd.BooleanDtype(),\n",
    "# }\n",
    "df = df.astype({\"is_quote_status\": bool}).astype(object_dtype_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15fad1-0942-4206-a025-24c662accb58",
   "metadata": {},
   "source": [
    "### Show Missing Values per Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8321f6f-3da4-4977-99a7-a85b268f5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "(\n",
    "    df.isna().sum().rename(\"missing\").to_frame().merge(\n",
    "        df.dtypes.rename(\"dtype\").to_frame(),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    .assign(num_rows=len(df))\n",
    "    .assign(frac_missing=lambda df: (df['missing'] / df['num_rows'])*100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b234eb-448a-4e84-962e-6c9845d06047",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0061cfce-561e-49e6-92d6-2416e6abb297",
   "metadata": {},
   "source": [
    "Show tweets with a url, hashtag or username in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06e9b6-62f3-4a30-b4b6-be7ccaf8d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_uhu = df.loc[\n",
    "    (df[\"tweet_text_urls\"] != \"\")\n",
    "    | (df[\"tweet_text_hashtags\"] != \"\")\n",
    "    | (df[\"tweet_text_usernames\"] != \"\"),\n",
    "    [\n",
    "        \"text\",\n",
    "        \"tweet_text_urls\",\n",
    "        \"tweet_text_hashtags\",\n",
    "        \"tweet_text_usernames\",\n",
    "    ],\n",
    "]\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(df_uhu.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd4938-41e1-4e3e-93c0-cbe586f8d8ab",
   "metadata": {},
   "source": [
    "## Complete Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a39cc4ee-8e75-415f-9eca-7c7cd5e5b09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting string columns to booleans...\n",
      "Done.\n",
      "Converting text columns to strings...\n",
      "Done.\n",
      "Converting string columns to integers...\n",
      "Done.\n",
      "Converting datetime columns...\n",
      "Done.\n",
      "Removing &amp; from text column...\n",
      "Done.\n",
      "Changing text to lowercase...\n",
      "Done.\n",
      "Removing texts with unwanted substrings...\n",
      "Done.\n",
      "Removing multiple whitespaces from text column...\n",
      "Done.\n",
      "Removing leading and trailing whitespaces from text column...\n",
      "Done.\n",
      "Removing special characters from text column...\n",
      "Done.\n",
      "Removing punctuation from text column...\n",
      "Done.\n",
      "Getting words from text...\n",
      "Done.\n",
      "Dropping columns with missing values...\n",
      "Done.\n",
      "Removing re-tweets...\n",
      "Done.\n",
      "Removing empty (blank) tweets...\n",
      "Done.\n",
      "Performing sanity checks on modified text column...\n",
      "Done\n",
      "Getting lists of non-useful columns...\n",
      "Column = place, Missing fraction: 0.99, Blank fraction: 0.00\n",
      "Column = in_reply_to_user_id, Missing fraction: 0.86, Blank fraction: 0.00\n",
      "Column = in_reply_to_screen_name, Missing fraction: 0.86, Blank fraction: 0.00\n",
      "Column = place_id, Missing fraction: 0.00, Blank fraction: 0.99\n",
      "Column = place_url, Missing fraction: 0.00, Blank fraction: 0.99\n",
      "Column = place_place_type, Missing fraction: 0.00, Blank fraction: 0.99\n",
      "Column = place_bounding_box_type, Missing fraction: 0.00, Blank fraction: 0.99\n",
      "Column = tweet_text_hashtags, Missing fraction: 0.00, Blank fraction: 0.70\n",
      "Column = tweet_text_usernames, Missing fraction: 0.00, Blank fraction: 0.63\n",
      "Done.\n",
      "Dropping single-value columns from list...\n",
      "Done.\n",
      "Dropping mostly None or mostly blank columns from list...\n",
      "Done.\n",
      "Dropping empty array columns from list...\n",
      "Done.\n",
      "Changing datatype of object columns...\n",
      "Done.\n",
      "Saving data to a parquet file...\n",
      "Done.\n",
      "Uploading data/processed/processed_data__20220701_140042.parquet.gzip to sagemakertestwillz3s/processed_data/processed_data__20220701_140042.parquet.gzip...\n",
      "Done.\n",
      "CPU times: user 4min 24s, sys: 8.2 s, total: 4min 32s\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if process_raw_files:\n",
    "    list_of_hourly_dirs_flat = get_hourly_folders_per_day(\n",
    "        s3_bucket_name, path_to_folder, years_wanted\n",
    "    )\n",
    "    file_contents_all_flat = read_files_per_hour(s3_bucket_name, list_of_hourly_dirs_flat)\n",
    "    df = (\n",
    "        convert_file_contents_to_df(file_contents_all_flat)\n",
    "        .pipe(add_column_headers, headers=headers)\n",
    "        .pipe(save_to_parquet, filepath=raw_data_filepath)\n",
    "    )\n",
    "else:\n",
    "    df = pd.read_parquet(raw_data_filepath).copy()\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .pipe(convert_string_cols_to_bool, string_cols=string_cols)\n",
    "    .pipe(convert_text_cols_to_str, text_cols=text_cols)\n",
    "    .pipe(convert_text_cols_to_int, int_cols=int_cols)\n",
    "    .pipe(convert_datetime_cols, datetime_cols=datetime_cols)\n",
    "    .pipe(\n",
    "        remove_html_ampersand,\n",
    "        text_col=\"text\",\n",
    "        ampersand_html_substring='&amp;',\n",
    "        replacement_text=' and ',\n",
    "    )\n",
    "    .pipe(change_text_to_lowercase, text_col='text')\n",
    "    .pipe(\n",
    "        drop_unwanted_partial_tweets,\n",
    "        columns_to_drop_tweets=['text'],\n",
    "        unwanted_partial_strings_list=list(set(unwanted_partial_strings_list + profanities)),\n",
    "    )\n",
    "    .pipe(remove_multiple_whitespaces, text_col='text', proc_text_col='processed_text')\n",
    "    .pipe(remove_leading_trailing_spaces, text_col='processed_text')\n",
    "    .pipe(remove_special_characters, text_col='processed_text')\n",
    "    .pipe(remove_punctuation, text_col='processed_text')\n",
    "    .pipe(get_words_from_text, text_col='processed_text', words_col='words')\n",
    "    .pipe(drop_nans, columns_to_drop_nans=[\"processed_text\"])\n",
    "    .pipe(drop_retweets, retweet_col=\"retweeted_tweet\", retweet_value=\"yes\")\n",
    "    .pipe(drop_empty_tweets, tweet_col=\"processed_text\")\n",
    ")\n",
    "\n",
    "print(\"Performing sanity checks on modified text column...\")\n",
    "assert df[\"text\"].str.contains(\"&amp;\").sum() == 0\n",
    "assert df[\"text\"].str.contains(\n",
    "    \"|\".join(list(set(unwanted_partial_strings_list + profanities)))\n",
    ").sum() == 0\n",
    "print(\"Done\")\n",
    "\n",
    "single_value_cols, mostly_nan_or_blanks = get_non_useful_cols(\n",
    "    df, nones_threshold, blanks_threshold\n",
    ")\n",
    "df = (\n",
    "    df\n",
    "    .pipe(\n",
    "        drop_columns_from_list,\n",
    "        cols_to_drop=single_value_cols,\n",
    "        cols_type=\"single-value\",\n",
    "    )\n",
    "    .pipe(\n",
    "        drop_columns_from_list,\n",
    "        cols_to_drop=mostly_nan_or_blanks,\n",
    "        cols_type=\"mostly None or mostly blank\"\n",
    "    )\n",
    "    .pipe(\n",
    "        drop_columns_from_list,\n",
    "        cols_to_drop=empty_array_cols,\n",
    "        cols_type=\"empty array\",\n",
    "    )\n",
    "    .pipe(\n",
    "        change_object_cols_dtype,\n",
    "        object_cols_to_bool_dict={\"is_quote_status\": bool},\n",
    "        mapper_dict=object_dtype_mapper,\n",
    "    )\n",
    ")\n",
    "\n",
    "if remove_short_tweets:\n",
    "    df = df.pipe(filter_tweets_by_num_words, words_col='words', min_num_words=15)\n",
    "\n",
    "now_dtime = datetime.now().replace(microsecond=0)\n",
    "filepath_with_ext = (\n",
    "    f\"{proc_data_filepath}__{now_dtime.strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \".parquet.gzip\"\n",
    ")\n",
    "s3_filepath_with_ext = (\n",
    "    f\"{os.path.basename(proc_data_filepath)}/processed_data__\"\n",
    "    f\"{now_dtime.strftime('%Y%m%d_%H%M%S')}.parquet.gzip\"\n",
    ")\n",
    "df.pipe(save_to_parquet, filepath=filepath_with_ext)\n",
    "upload_file_to_s3(\n",
    "    local_fpath=filepath_with_ext,\n",
    "    s3_fpath=s3_filepath_with_ext,\n",
    "    s3_bucket_name=s3_bucket_name,\n",
    "    aws_region=aws_region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c41858c-8fc1-481b-84c0-7c6d898c6c62",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf819384-8983-4351-b626-befc0c2d97a1",
   "metadata": {},
   "source": [
    "1. [Meaning of `&amp;` in the text of a tweet](https://www.hammockforums.net/forum/showthread.php/105012-What-does-amp-amp-mean-see-it-a-lot-and-its-not-in-the-acronym-page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
