{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee08390-c516-4a05-8e72-a4d9e67b940d",
   "metadata": {},
   "source": [
    "# NLP Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c155c378-f1ff-40da-bac6-1758599c19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53957578-9301-4a2b-b93e-4e38f54aa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from calendar import day_name\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb9695f-45de-4b46-918a-0586169ae1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT = os.path.join(os.pardir)\n",
    "src_dir = os.path.join(PROJ_ROOT, \"src\")\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ec60e5-6996-4786-8d9b-7166fe902f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport metrics_utils\n",
    "from metrics_utils import calculate_metrics\n",
    "\n",
    "%aimport model_utils\n",
    "from model_utils import (\n",
    "    compute_metrics,\n",
    "    get_metrics,\n",
    "    make_predictions,\n",
    "    tokenize_function,\n",
    ")\n",
    "\n",
    "%aimport pandas_utils\n",
    "from pandas_utils import save_to_parquet\n",
    "\n",
    "%aimport s3_utils\n",
    "from s3_utils import download_files_from_s3, extract_zip_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1153dc-6ba9-4499-8230-216e78d3221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8aa40a-30ad-457d-b0a7-9981603ef604",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632e2aa-161e-4f16-b172-b2e6639f49d2",
   "metadata": {},
   "source": [
    "### Objective\n",
    "This notebook [fine-tunes a pre-trained](https://huggingface.co/docs/transformers/training#train-in-native-pytorch) transformers model ([1](https://huggingface.co/microsoft/MiniLM-L12-H384-uncasedhttps://huggingface.co/microsoft/MiniLM-L12-H384-uncased), [2](https://arxiv.org/pdf/2002.10957.pdf#page=5https://arxiv.org/pdf/2002.10957.pdf#page=5)) using the [PyTorch](https://pytorch.org/https://pytorch.org/) deep learning framework.\n",
    "\n",
    "### Data\n",
    "The data used for fine-tuning consists of the three data splits\n",
    "- (training) `train_nlp_inference_starts_*.xlsx` (approximately 2,900 tweets, for initial model training)\n",
    "- (validation, for model scoring per [epoch](https://deepai.org/machine-learning-glossary-and-terms/epochhttps://deepai.org/machine-learning-glossary-and-terms/epoch)) `val_nlp_inference_starts_*.xlsx` (600 tweets)\n",
    "- (testing) `test_nlp_inference_starts_*.xlsx` (600 tweets)\n",
    "\n",
    "that were\n",
    "- created in `6-split-data/notebooks/6_split_data.ipynb`\n",
    "- manually labeled by reading the tweets to identify the sentiment\n",
    "  - 0 - negative\n",
    "  - 1 - neutral, or\n",
    "  - 2 - positive\n",
    "\n",
    "  of each tweet\n",
    "\n",
    "As a reminder of the context outlined in the project scope, tweets labeled with a\n",
    "- negative or neutral sentiment\n",
    "  - need support from (must be reviewed by) the mission support team\n",
    "- positive sentiment\n",
    "  - do not need support from the mission support team\n",
    "\n",
    "Model\n",
    "- fine-tuning is performed using the training and validation splits\n",
    "- evaluation is performed using the training and testing splits using ML and business metrics\n",
    "  - for assessing the business metrics, a comparison is made between the metrics calculated using\n",
    "    - the fine-tuned ML model\n",
    "    - a naive (non-ML) model\n",
    "\n",
    "  and the fine-tuned model must have\n",
    "  - F2-score greather than 0.8\n",
    "  - superior (higher) evaluation metrics than those using a naive (non-ML) approach\n",
    "\n",
    "### Outputs\n",
    "1. `test_nlp__inference_starts_xxxx__batch_n__with_preds.parquet.gzip`\n",
    "   - predictions of the sentiment in the *testing* split will be appended to the test split and then exported to a `.parquet` file\n",
    "   - this file will be used to monitor the relationship between metadata and both the fine-tuned ML and naive (non-ML) predictions for one or multiple testing splits\n",
    "2. `metrics__inference_starts_xxxx__batch_n`\n",
    "   - summary of ML evaluation and business metrics for the *testing* split\n",
    "   - this file is used to validate the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a78a4-609d-4cfd-837a-367b52142f52",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a349b0-62c9-4067-a33a-51bccbe80f86",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "path_to_folder = \"/datasets/twitter/kinesis-demo/\"\n",
    "\n",
    "# processed data\n",
    "processed_data_dir = \"../data/processed\"\n",
    "\n",
    "label_mapper = {\"does_not_need_support\": 0, \"needs_support\": 1}\n",
    "\n",
    "needs_support_labels = [0, 1]\n",
    "\n",
    "checkpoint_pretrained = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "\n",
    "model_output_dir = \"../model-fine-tuned\"\n",
    "\n",
    "# Metadata - feature engineering\n",
    "b = [0, 4, 8, 12, 16, 20, 24]\n",
    "l = [\"Late Night\", \"Early Morning\", \"Morning\", \"Afternoon\", \"Evening\", \"Night\"]\n",
    "num_words_bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "num_words_labels = [\n",
    "    \"0-5\",\n",
    "    \"6-10\",\n",
    "    \"11-15\",\n",
    "    \"16-20\",\n",
    "    \"20-25\",\n",
    "    \"26-30\",\n",
    "    \"31-35\",\n",
    "    \"36-40\",\n",
    "    \"41-45\",\n",
    "    \"46-50\",\n",
    "    \"51-55\",\n",
    "    \"56-60\",\n",
    "]\n",
    "\n",
    "# Model evaluation\n",
    "wanted_pred_proba_stats = [\n",
    "    \"count\",\n",
    "    \"min\",\n",
    "    \"mean\",\n",
    "    \"coeff_of_var\",\n",
    "    \"std_error\",\n",
    "    \"num_samples\",\n",
    "]\n",
    "avg_reading_speed_wpm = 135\n",
    "avg_typing_speed_wpm = 40\n",
    "frac_support_tweets_needing_response = 1.0\n",
    "\n",
    "upload_to_s3 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82a913d6-1abf-40e7-85d9-47534f890e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved AWS credentials from .env file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'does_not_need_support', 1: 'needs_support'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_bucket_name = os.getenv(\"AWS_S3_BUCKET_NAME\", \"\")\n",
    "\n",
    "try:\n",
    "    session = boto3.Session(profile_name=\"default\")\n",
    "    s3_client = session.client(\"s3\")\n",
    "    aws_region = session.region_name\n",
    "    print(\"Retrieved AWS credentials from ~/.ssh/aws file\")\n",
    "except Exception as e:\n",
    "    if str(e) == \"The config profile (default) could not be found\":\n",
    "        aws_region = os.getenv(\"AWS_REGION\")\n",
    "        s3_client = boto3.client(\"s3\", region_name=aws_region)\n",
    "        print(\"Retrieved AWS credentials from .env file\")\n",
    "\n",
    "dtypes_dict = {\n",
    "    \"id\": pd.StringDtype(),\n",
    "    \"contributors\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"source_text\": pd.StringDtype(),\n",
    "    \"place_country\": pd.StringDtype(),\n",
    "    \"user_location\": pd.StringDtype(),\n",
    "    \"user_followers\": pd.Int32Dtype(),\n",
    "    \"user_friends\": pd.Int32Dtype(),\n",
    "    \"user_listed\": pd.Int32Dtype(),\n",
    "    \"user_favourites\": pd.Int32Dtype(),\n",
    "    \"user_statuses\": pd.Int32Dtype(),\n",
    "    \"user_protected\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"user_verified\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"is_quote_status\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"retweeted\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"retweeted_tweet\": pd.StringDtype(),\n",
    "    \"in_reply_to_screen_name\": pd.StringDtype(),\n",
    "    \"user_screen_name\": pd.StringDtype(),\n",
    "    \"num_urls_in_tweet_text\": pd.Int32Dtype(),\n",
    "    \"num_words\": pd.Int32Dtype(),\n",
    "    \"text\": pd.StringDtype(),\n",
    "    \"sentiment\": pd.Int32Dtype(),\n",
    "    \"order\": pd.Int32Dtype(),\n",
    "    \"hour\": pd.Int32Dtype(),\n",
    "    \"day\": pd.Int32Dtype(),\n",
    "    \"weekday\": pd.StringDtype(),\n",
    "    \"time_of_day\": pd.StringDtype(),\n",
    "    \"batch_num\": pd.Int32Dtype(),\n",
    "}\n",
    "test_feats_dtypes_test_dict = {\n",
    "    \"pred\": pd.Int32Dtype(),\n",
    "    \"created_at_hour\": pd.Int32Dtype(),\n",
    "    \"created_at_day\": pd.StringDtype(),\n",
    "    \"user_joined_hour\": pd.Int32Dtype(),\n",
    "    \"user_joined_day\": pd.StringDtype(),\n",
    "    \"split\": pd.StringDtype(),\n",
    "    \"created_at_time_of_day\": pd.StringDtype(),\n",
    "    \"reading_time\": pd.Float32Dtype(),\n",
    "    \"replying_time\": pd.Float32Dtype(),\n",
    "    \"response_time\": pd.Float32Dtype(),\n",
    "}\n",
    "metrics_dtypes_dict = dict(\n",
    "    batch_num=pd.Int32Dtype(),\n",
    "    num_tweets_missed=pd.Int32Dtype(),\n",
    "    num_tweets_unnecessarily_read=pd.Int32Dtype(),\n",
    "    total_number_tweets=pd.Int32Dtype(),\n",
    "    num_needs_support=pd.Int32Dtype(),\n",
    ")\n",
    "\n",
    "# Metadata - Feature Engineering\n",
    "mdict = dict(\n",
    "    USA=[\n",
    "        \" AL\",\n",
    "        \" AK\",\n",
    "        \" AZ\",\n",
    "        \" AR\",\n",
    "        \" CA\",\n",
    "        \" CO\",\n",
    "        \" CT\",\n",
    "        \" DC\",\n",
    "        \" DE\",\n",
    "        \" FL\",\n",
    "        \" GA\",\n",
    "        \" HI\",\n",
    "        \" ID\",\n",
    "        \" IL\",\n",
    "        \" IN\",\n",
    "        \" IA\",\n",
    "        \" KS\",\n",
    "        \" KY\",\n",
    "        \" LA\",\n",
    "        \" ME\",\n",
    "        \" MD\",\n",
    "        \" MA\",\n",
    "        \" MI\",\n",
    "        \" MN\",\n",
    "        \" MS\",\n",
    "        \" MO\",\n",
    "        \" MT\",\n",
    "        \" NE\",\n",
    "        \" NV\",\n",
    "        \" NH\",\n",
    "        \" NJ\",\n",
    "        \" NM\",\n",
    "        \" NY\",\n",
    "        \" NC\",\n",
    "        \" ND\",\n",
    "        \" OH\",\n",
    "        \" OK\",\n",
    "        \" OR\",\n",
    "        \" PA\",\n",
    "        \" RI\",\n",
    "        \" SC\",\n",
    "        \" SD\",\n",
    "        \" TN\",\n",
    "        \" TX\",\n",
    "        \" UT\",\n",
    "        \" VT\",\n",
    "        \" VA\",\n",
    "        \" WA\",\n",
    "        \" WV\",\n",
    "        \" WI\",\n",
    "        \" WY\",\n",
    "        \"USA\",\n",
    "        \" USA\",\n",
    "        \"United States\",\n",
    "        \"Los Angeles\",\n",
    "        \"Los Angeles \",\n",
    "        \"Texas\",\n",
    "    ],\n",
    "    UK=[\"United Kingdom\", \" United Kingdom\", \" England\", \"London\", \"UK\"],\n",
    "    India=[\" India\", \"India\"],\n",
    "    Australia=[\"Australia\"],\n",
    "    Canada=[\" Ontario\", \"Canada\"],\n",
    "    Philippines=[\"Republic of the Philippines\", \"Philippines\"],\n",
    "    Indonesia=[\"Indonesia\"],\n",
    "    France=[\"France\", \" France\"],\n",
    "    Germany=[\"Germany\", \"Deutschland\"],\n",
    "    Kenya=[\" Kenya\"],\n",
    ")\n",
    "\n",
    "id2label = {v: k for k, v in label_mapper.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106a68c-4cd7-41b9-8ab5-ec944c0bb93b",
   "metadata": {},
   "source": [
    "## Get Annotated Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7031a8c-b106-4252-9a69-a97785dc2732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found at ../data/processed/test_nlp__inference_starts_20220110_000000.xlsx. Did nothing.\n",
      "File found at ../data/processed/train_nlp__inference_starts_20220110_000000.xlsx. Did nothing.\n",
      "File found at ../data/processed/val_nlp__inference_starts_20220110_000000.xlsx. Did nothing.\n",
      "../data/processed/test_nlp__inference_starts_20220110_000000.xlsx\n",
      "CPU times: user 16 ms, sys: 199 µs, total: 16.2 ms\n",
      "Wall time: 182 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/processed/test_nlp__inference_starts_20220110_000000.xlsx',\n",
       " '../data/processed/train_nlp__inference_starts_20220110_000000.xlsx',\n",
       " '../data/processed/val_nlp__inference_starts_20220110_000000.xlsx']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "download_files_from_s3(\n",
    "    s3_client,\n",
    "    s3_bucket_name,\n",
    "    processed_data_dir,\n",
    "    aws_region,\n",
    "    f\"{path_to_folder[1:]}processed/nlp_splits/\",\n",
    "    \".xlsx\",\n",
    ")\n",
    "proc_files = sorted(glob(f\"{processed_data_dir}/*_nlp_*.xlsx\"))\n",
    "proc_file_test = [f for f in proc_files if \"test_\" in f][0]\n",
    "print(proc_file_test)\n",
    "proc_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b922b3ab-18ee-4c0a-a613-0c6c7c17b014",
   "metadata": {},
   "source": [
    "## Load and Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc3baa-98ef-4f45-adfa-36e32eb64fdb",
   "metadata": {},
   "source": [
    "Perform the following\n",
    "- rename the class labels column from `sentiment` to `labels`\n",
    "- remove retweets (tweets starting with *RT*)\n",
    "- map the `labels` column (sentiment) to indicate a tweet\n",
    "  - needing support (neutral or negative sentiment)\n",
    "  - not needing support (positive sentiment)\n",
    "- text processing to\n",
    "  - remove leading and trailing spaces\n",
    "  - replace HTML by `>`, `<` or `&`, as appropriate\n",
    "- add a column with a binned version of the number of words in the tweet\n",
    "  - bin width was chosen as 5 words (eg. 0-5, 6-10, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4623dd24-f82f-40a5-8396-9d22ec32c7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.53 s, sys: 13.3 ms, total: 2.54 s\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test, df_train, df_val = [\n",
    "    (\n",
    "        pd.read_excel(\n",
    "            f,\n",
    "            dtype=dtypes_dict,\n",
    "            usecols=list(dtypes_dict)+['created_at', 'user_joined']\n",
    "        ).rename(columns={\"sentiment\": \"labels\"})\n",
    "        # .sort_values(by=['created_at'])\n",
    "        .query(\"~text.str.startswith('RT')\")\n",
    "        .assign(split=st)\n",
    "        .assign(labels=lambda df: df['labels'].isin(needs_support_labels).astype(pd.Int32Dtype()))\n",
    "        .assign(\n",
    "            text=lambda df: (\n",
    "                df[\"text\"]\n",
    "                .str.lstrip()\n",
    "                .str.rstrip()\n",
    "                .str.replace(\"&gt;\", \">\")\n",
    "                .str.replace(\"&lt;\", \"<\")\n",
    "                .str.replace(\"&amp;\", \"&\")\n",
    "            )\n",
    "        )\n",
    "        .assign(\n",
    "            bin_name=lambda df: pd.cut(df[\"num_words\"], bins=num_words_bins, labels=num_words_labels).astype(pd.StringDtype())\n",
    "        )\n",
    "    )\n",
    "    for f, st in zip(proc_files, ['test', 'train', 'val'])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb81b6-3be2-4eea-b6bb-7f5784a5e567",
   "metadata": {},
   "source": [
    "Drop any tweets which were not manually labeled with a sentiment. Since re-training and manual labeling are only performed after every five batches of new data arrives, the test split will contain data that is missing labels which won't be used in both of\n",
    "- re-training\n",
    "- model evaluation\n",
    "\n",
    "so these rows must be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b28d33-1824-4ea5-a882-0b88509c2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.dropna(subset=[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db1699-591b-4cc0-9f74-203817d22df2",
   "metadata": {},
   "source": [
    "Get split sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88cfd739-4e34-4356-8e8f-b7188c1a26a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sizes = [{\"train\": len(df_train), \"val\": len(df_val), \"test\": len(df_test)}]\n",
    "df_split_sizes = pd.DataFrame.from_records(split_sizes).assign(type=\"raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25639787-b41b-41ab-9209-8e9b056960ec",
   "metadata": {},
   "source": [
    "Get the start and end date of the raw data in each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef5c4dc0-8e35-4827-bc4e-8a101658dbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>2021-12-30 17:39:11</td>\n",
       "      <td>2022-01-08 15:14:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val</td>\n",
       "      <td>2022-01-08 15:15:45</td>\n",
       "      <td>2022-01-09 01:17:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>2022-01-09 01:18:13</td>\n",
       "      <td>2022-01-10 01:29:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split                start                  end\n",
       "0  train  2021-12-30 17:39:11  2022-01-08 15:14:33\n",
       "1    val  2022-01-08 15:15:45  2022-01-09 01:17:04\n",
       "2   test  2022-01-09 01:18:13  2022-01-10 01:29:01"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split_dates = pd.DataFrame.from_records(\n",
    "    [\n",
    "        {\n",
    "            \"split\": split_type,\n",
    "            \"start\": df_nlp_spit[\"created_at\"].min().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"end\": df_nlp_spit[\"created_at\"].max().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "        for df_nlp_spit, split_type in zip(\n",
    "            [df_train, df_val, df_test], [\"train\", \"val\", \"test\"]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "df_split_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ac5a1-174e-4777-913a-96f669996c2e",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The training split preceeds the validation split which, in-turn, preceeds the testing split. This is expected, since the splits were separated based on `datetime` in order to replicate the arrival of new (unseen) data during inference. See `README.md` for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a46097-5a3f-4453-9ae0-9808793a502b",
   "metadata": {},
   "source": [
    "Perform sanity checks to verify the expected time-ordering of the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bdadf29-7455-496c-a33b-f988e36af231",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = df_train[\"created_at\"].max()\n",
    "val_start = df_val[\"created_at\"].min()\n",
    "val_end = df_val[\"created_at\"].max()\n",
    "test_start = df_test[\"created_at\"].min()\n",
    "assert train_end < val_start\n",
    "assert val_start < val_end\n",
    "assert val_end < test_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc27983-94a7-4cc4-afef-8fd4150053c7",
   "metadata": {},
   "source": [
    "(If not initial training run) Get most current test split and\n",
    "- append validation split to training split\n",
    "- use the most recent non-current test split as the validation split\n",
    "- use the most recent test split as the current test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7818bcc3-f7fc-4f15-b3f6-c876b7d22e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_nums = df_test[\"batch_num\"].unique().tolist()\n",
    "batch_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c820326-24ed-46a3-a923-f96528490b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_current_batch_num = df_test[\"batch_num\"].max()\n",
    "# if df_test[\"batch_num\"].nunique() > 1:\n",
    "#     df_train = pd.concat(\n",
    "#         [df_train, df_val, df_test.query(f\"batch_num < {test_current_batch_num-1}\")]\n",
    "#     )\n",
    "#     df_val = df_test.query(f\"batch_num == {test_current_batch_num-1}\")\n",
    "#     df_test = df_test.query(f\"batch_num == {test_current_batch_num}\")\n",
    "\n",
    "if len(batch_nums) > 1:\n",
    "    # get all but second last and last batch numbers from test split (to use in training split)\n",
    "    training_batch_nums = batch_nums[:-2]\n",
    "    # get second last batch number from test split (to use in validation split)\n",
    "    val_batch_num = batch_nums[-2]\n",
    "    # get last batch number from test split (to use as current test split)\n",
    "    test_current_batch_num = batch_nums[-1]\n",
    "\n",
    "    # Slice raw data splits based on batch numbers defined above\n",
    "    df_train = pd.concat(\n",
    "        [df_train, df_val, df_test.query(f\"batch_num.isin(@training_batch_nums)\")]\n",
    "    )\n",
    "    df_val = df_test.query(f\"batch_num == {val_batch_num}\")\n",
    "    df_test = df_test.query(f\"batch_num == {test_current_batch_num}\")\n",
    "else:\n",
    "    test_current_batch_num = df_test[\"batch_num\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a1dda-4dac-450e-88ec-6ca575dffb76",
   "metadata": {},
   "source": [
    "Drop duplicates in the\n",
    "- training split\n",
    "  - this will improve (reduce) fine-tuning time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4cc87fb-cfff-4486-ac18-63579e103022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop_duplicates(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9906d-8850-488b-851b-82fe28d66cfe",
   "metadata": {},
   "source": [
    "Get the new split sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c7cdd0e-3a75-4437-8d39-63e08d5199a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sizes_no_dups = [\n",
    "    {\"train\": len(df_train), \"val\": len(df_val), \"test\": len(df_test)}\n",
    "]\n",
    "df_split_sizes_no_dups = pd.DataFrame.from_records(split_sizes_no_dups).assign(\n",
    "    type=\"without-duplicates\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fcf5d5-601a-48cd-b340-85b0228729d9",
   "metadata": {},
   "source": [
    "Show split sizes before and after dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5d23c1c-74c3-4e46-9503-8fc3ba57a059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "      <th>test</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2931</td>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2775</td>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "      <td>without-duplicates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train  val  test                type\n",
       "0   2931  600   600                 raw\n",
       "1   2775  600   600  without-duplicates"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split_sizes_comp = pd.concat(\n",
    "    [df_split_sizes, df_split_sizes_no_dups], ignore_index=True\n",
    ")\n",
    "df_split_sizes_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d888f8d-e177-49a4-b939-c1354033abea",
   "metadata": {},
   "source": [
    "Get the start and end date of each split after combining training, validation and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fd8f63b-4461-477b-a3e3-6283ec27ffc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>2021-12-30 17:39:11</td>\n",
       "      <td>2022-01-08 15:14:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val</td>\n",
       "      <td>2022-01-08 15:15:45</td>\n",
       "      <td>2022-01-09 01:17:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>2022-01-09 01:18:13</td>\n",
       "      <td>2022-01-10 01:29:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split                start                  end\n",
       "0  train  2021-12-30 17:39:11  2022-01-08 15:14:33\n",
       "1    val  2022-01-08 15:15:45  2022-01-09 01:17:04\n",
       "2   test  2022-01-09 01:18:13  2022-01-10 01:29:01"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split_dates = pd.DataFrame.from_records(\n",
    "    [\n",
    "        {\n",
    "            \"split\": split_type,\n",
    "            \"start\": df_nlp_spit[\"created_at\"].min().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"end\": df_nlp_spit[\"created_at\"].max().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "        for df_nlp_spit, split_type in zip(\n",
    "            [df_train, df_val, df_test], [\"train\", \"val\", \"test\"]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "df_split_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05562bd4-155f-4c35-8e56-218f64ba5ee0",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. The time-dependence seen in the raw splits should still be preserved after combining splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02415c5-b50d-47f0-9281-438ea08ef38e",
   "metadata": {},
   "source": [
    "Perform sanity checks to verify the expected time-ordering of the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "675b8a09-ae72-452e-a089-dd2da83d43b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = df_train[\"created_at\"].max()\n",
    "val_start = df_val[\"created_at\"].min()\n",
    "val_end = df_val[\"created_at\"].max()\n",
    "test_start = df_test[\"created_at\"].min()\n",
    "assert train_end < val_start\n",
    "assert val_start < val_end\n",
    "assert val_end < test_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b7873-cefd-41a8-9a40-0f30703571ed",
   "metadata": {},
   "source": [
    "The feature `bin_name`, containing a binned version of the number of words in each tweet, was added here. The bin boundaries were defined based on statistics for the training data, which are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "220ef319-7d29-439f-8d92-c3500d94d15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2775.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>22.950631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.881696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nunique_num_words</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   num_words\n",
       "count                 2775.0\n",
       "mean               22.950631\n",
       "std                12.881696\n",
       "min                      3.0\n",
       "25%                     12.0\n",
       "50%                     20.0\n",
       "75%                     33.0\n",
       "max                     57.0\n",
       "nunique_num_words         54"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"num_words\"].describe().to_frame().T.assign(\n",
    "    nunique_num_words=df_train[\"num_words\"].nunique()\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7663e-f036-4ea2-ab85-b855dc4b7e14",
   "metadata": {},
   "source": [
    "## Get Features and Labels For Each Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a31603ce-a456-4bc3-9c13-4d68f038e11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 201 µs, sys: 0 ns, total: 201 µs\n",
      "Wall time: 205 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = [\n",
    "    df_train[\"text\"],\n",
    "    df_val[\"text\"],\n",
    "    df_test[\"text\"],\n",
    "    df_train[\"labels\"],\n",
    "    df_val[\"labels\"],\n",
    "    df_test[\"labels\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fd586-3877-4cd6-a0d1-c83f53126d55",
   "metadata": {},
   "source": [
    "## Create `huggingface` Dataset (Including All Splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0029767-5d55-458b-83dc-958b01088c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {\n",
    "    \"train\": {\"label\": y_train.tolist(), \"text\": X_train.tolist()},\n",
    "    \"val\": {\"label\": y_val.tolist(), \"text\": X_val.tolist()},\n",
    "    \"test\": {\"label\": y_test.tolist(), \"text\": X_test.tolist()},\n",
    "}\n",
    "dataset = DatasetDict()\n",
    "for k, v in mydict.items():\n",
    "    dataset[k] = Dataset.from_dict(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5d2f665-edfc-469d-8aa9-e0ce4358b2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'James Webb telescope could fundermentaley change are understanding of the universe. This right now is one of the most exciting times to ever be alive, If your interested in space and future tech then this should be at the top of your least xxx'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3118d2c-59ef-4e68-bccc-156dd55e8e55",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9014822f-25cb-4697-93a8-817a3c7f6045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_bab47\">\n",
       "  <caption>Class Balance of Train Split</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bab47_level0_col0\" class=\"col_heading level0 col0\" >label</th>\n",
       "      <th id=\"T_bab47_level0_col1\" class=\"col_heading level0 col1\" >freq</th>\n",
       "      <th id=\"T_bab47_level0_col2\" class=\"col_heading level0 col2\" >rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bab47_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bab47_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_bab47_row0_col1\" class=\"data row0 col1\" >0.630631</td>\n",
       "      <td id=\"T_bab47_row0_col2\" class=\"data row0 col2\" >1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bab47_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_bab47_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_bab47_row1_col1\" class=\"data row1 col1\" >0.369369</td>\n",
       "      <td id=\"T_bab47_row1_col2\" class=\"data row1 col2\" >1025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5c1af533a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.unique(y_train))\n",
    "display(\n",
    "    y_train.value_counts(normalize=True)\n",
    "    .rename(\"freq\")\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"label\"})\n",
    "    .merge(\n",
    "        y_train.value_counts()\n",
    "        .rename(\"rows\")\n",
    "        .sort_index()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"label\"})\n",
    "    )\n",
    "    .style.set_caption(\"Class Balance of Train Split\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586f7ce-4ae4-4172-91de-93741a18a1fd",
   "metadata": {},
   "source": [
    "## Instantiate Pre-Trained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "664d97bf-090f-49aa-b0b9-4977fe5f2e41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9589a795c64b3685f2601073dd5439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8170a576c5144168b9a147ae383cfe11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b744e2ca4abd4fe594949e02ddcc5ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5b093470b64389820e8d09b28dc6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269be7a2e89f46ce8cd27e78178b0f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.83 s, sys: 298 ms, total: 2.13 s\n",
      "Wall time: 3.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_pretrained)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint_pretrained,\n",
    "    num_labels=y_train.nunique(),\n",
    "    id2label=id2label,\n",
    "    label2id=label_mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8bd00-e3f9-4a35-adcd-6b40a4728829",
   "metadata": {},
   "source": [
    "## Perform Dynamic Batching During Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f31c2f4-c4bf-4901-89d2-0002a4f4b025",
   "metadata": {},
   "source": [
    "Tokenize all the data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d44d4162-0d3d-421e-a0d1-c7ebca839567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe6813a196740f995e91f6e398d935d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44eba6a3ca584e2f8ae3cf6a1dbc9a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fec66a635348a6bbc4509240036522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 660 ms, sys: 17.8 ms, total: 678 ms\n",
      "Wall time: 248 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, fn_kwargs=dict(mytokenizer=tokenizer), batched=True\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b777467-551c-402e-9c50-e44888d9500e",
   "metadata": {},
   "source": [
    "## Dealing With Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007aed5c-389c-4ea9-83d7-441e4fed181b",
   "metadata": {},
   "source": [
    "Create class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dd40b84-e990-41be-9e3c-92e0ed4a5fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36936936, 0.6306306 ], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = (1 - (y_train.value_counts().sort_index() / len(y_train))).to_numpy(\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1909f8f-4f08-40e1-85f1-1e5ce7603c21",
   "metadata": {},
   "source": [
    "Convert class weights to `pytorch` tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b84d69c8-4699-4208-92bf-372c28afb4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3694, 0.6306])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = torch.from_numpy(class_weights).float()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da745e-b26c-488e-8e34-0f85af261259",
   "metadata": {},
   "source": [
    "Define an instance of the `Trainer` class, that implements a custom `.CrossEntropyLoss()` which uses the above class weights based on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4391695-3120-4aab-9ddc-20940eb82b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract true labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass - feed inputs to model and extract logits\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Define loss function with class weights\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        # Compute loss\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ba83e-3eaa-4740-acba-50c5ac6abb9f",
   "metadata": {},
   "source": [
    "## Re-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32025a-77d8-44f0-b7f8-cd32adcf87d0",
   "metadata": {},
   "source": [
    "### Set Up `Trainer` Object (using `huggingface`'s `Trainer` API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d4ee53a-3fa4-4f23-bcd2-7ca44c72a5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.66 ms, sys: 230 µs, total: 4.89 ms\n",
      "Wall time: 14.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 64\n",
    "logging_steps = len(df_train) // batch_size\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy = \"epoch\",  # new\n",
    "    logging_steps=logging_steps,\n",
    "    load_best_model_at_end=True,  # new\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"all\",  # default='all'\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "trainer = CustomTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e41ea-fdaa-4e3c-9f9c-42c2b409e9f7",
   "metadata": {},
   "source": [
    "### Train (Performs Fine-Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2314927e-db11-4cca-ae82-060b3c979061",
   "metadata": {},
   "source": [
    "Perform fine-tuning of the pre-trained transformers model using the\n",
    "- training\n",
    "- validation\n",
    "\n",
    "splits from the manually labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17ccef78-b293-4e85-8937-2297e5c085b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2775\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 220\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting time = 2022-11-16 21:39:59.745...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [220/220 08:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F05</th>\n",
       "      <th>F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.673600</td>\n",
       "      <td>0.526155</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.835979</td>\n",
       "      <td>0.842713</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.827904</td>\n",
       "      <td>0.835401</td>\n",
       "      <td>0.824671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>0.398444</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.859503</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.423700</td>\n",
       "      <td>0.388109</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.856237</td>\n",
       "      <td>0.864669</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.863841</td>\n",
       "      <td>0.864300</td>\n",
       "      <td>0.863498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>0.368850</td>\n",
       "      <td>0.868333</td>\n",
       "      <td>0.862133</td>\n",
       "      <td>0.869833</td>\n",
       "      <td>0.868333</td>\n",
       "      <td>0.868880</td>\n",
       "      <td>0.869404</td>\n",
       "      <td>0.868503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>0.374702</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.856561</td>\n",
       "      <td>0.865767</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.865321</td>\n",
       "      <td>0.865573</td>\n",
       "      <td>0.865113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../model-fine-tuned/checkpoint-44\n",
      "Configuration saved in ../model-fine-tuned/checkpoint-44/config.json\n",
      "Model weights saved in ../model-fine-tuned/checkpoint-44/pytorch_model.bin\n",
      "tokenizer config file saved in ../model-fine-tuned/checkpoint-44/tokenizer_config.json\n",
      "Special tokens file saved in ../model-fine-tuned/checkpoint-44/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../model-fine-tuned/checkpoint-88\n",
      "Configuration saved in ../model-fine-tuned/checkpoint-88/config.json\n",
      "Model weights saved in ../model-fine-tuned/checkpoint-88/pytorch_model.bin\n",
      "tokenizer config file saved in ../model-fine-tuned/checkpoint-88/tokenizer_config.json\n",
      "Special tokens file saved in ../model-fine-tuned/checkpoint-88/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../model-fine-tuned/checkpoint-132\n",
      "Configuration saved in ../model-fine-tuned/checkpoint-132/config.json\n",
      "Model weights saved in ../model-fine-tuned/checkpoint-132/pytorch_model.bin\n",
      "tokenizer config file saved in ../model-fine-tuned/checkpoint-132/tokenizer_config.json\n",
      "Special tokens file saved in ../model-fine-tuned/checkpoint-132/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../model-fine-tuned/checkpoint-176\n",
      "Configuration saved in ../model-fine-tuned/checkpoint-176/config.json\n",
      "Model weights saved in ../model-fine-tuned/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in ../model-fine-tuned/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in ../model-fine-tuned/checkpoint-176/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../model-fine-tuned/checkpoint-220\n",
      "Configuration saved in ../model-fine-tuned/checkpoint-220/config.json\n",
      "Model weights saved in ../model-fine-tuned/checkpoint-220/pytorch_model.bin\n",
      "tokenizer config file saved in ../model-fine-tuned/checkpoint-220/tokenizer_config.json\n",
      "Special tokens file saved in ../model-fine-tuned/checkpoint-220/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../model-fine-tuned/checkpoint-176 (score: 0.36885035037994385).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done at 2022-11-16 21:48:30.303.\n",
      "CPU times: user 50min 23s, sys: 9.95 s, total: 50min 33s\n",
      "Wall time: 8min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f\"Starting time = {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}...\")\n",
    "trainer.train()\n",
    "print(f\"done at {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72917e20-0643-4afb-8d0e-daa44ead03e1",
   "metadata": {},
   "source": [
    "Train the non-ML (naive) model to make random guesses at whether the tweet needs support (non-positive sentiment) or not (positive sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90d8e47e-251d-4e83-b152-5200e9011770",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"clf\", DummyClassifier(strategy=\"uniform\", random_state=88))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "094aa55e-d724-43ea-9e39-6ce6660f5f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 842 µs, sys: 3 µs, total: 845 µs\n",
      "Wall time: 828 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696b7dd-d4f0-4b80-9257-d1ac37f8846c",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70abbba-1077-444c-804d-ed68e81fc2d1",
   "metadata": {},
   "source": [
    "Use the model trained above to make predictions on the manually labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f1a62-3119-4bbe-84c0-1b5df565b608",
   "metadata": {},
   "source": [
    "### Make Predictions with Re-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c5fb6c-3dd1-4b8d-8367-be92c62d2c27",
   "metadata": {},
   "source": [
    "Make predictions of the test split using the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc449114-929b-4836-a4e2-d6d3ceafc579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 600\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.4 s, sys: 168 ms, total: 39.6 s\n",
      "Wall time: 6.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_pred, y_test_proba = make_predictions(tokenized_datasets['test'], trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f260b-4fb6-4c52-9d81-70004210f1ea",
   "metadata": {},
   "source": [
    "Make predictions of the test split using the naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "383dfb57-9c46-43bb-898a-83b172a1ef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 728 µs, sys: 3 µs, total: 731 µs\n",
      "Wall time: 699 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_pred_naive = (\n",
    "    pd.Series(pipe.predict(X_test), name='label', index=y_test.index)\n",
    "    .astype(pd.Int32Dtype())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca27e50-c3b3-4025-ae76-a2845410d47b",
   "metadata": {},
   "source": [
    "Make predictions of the train split using the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b80bec17-b7cd-49b7-befc-d51d10986dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2775\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 1s, sys: 864 ms, total: 3min 1s\n",
      "Wall time: 30.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train_pred, y_train_proba = make_predictions(tokenized_datasets['train'], trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501ad45-8611-44a0-af5e-bc573340533f",
   "metadata": {},
   "source": [
    "The fine-tuned model's predictions are now evaluated using the following\n",
    "- evaluation metrics\n",
    "  - accuracy\n",
    "  - precision\n",
    "  - recall\n",
    "  - [F1-, F-0.5 and F2-score](https://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/scorers.html#f05-f1-and-f2)\n",
    "  - confusion matrix\n",
    "  - classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b8ed1-c14f-4be0-b3a7-2ac97489feb5",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07802fd0-8775-4b11-bcd2-a6f786f585ac",
   "metadata": {},
   "source": [
    "Model evaluation is performed on the predictions of the test split using the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15b153a5-a4a1-4b68-b890-1789cb9e7c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f05</th>\n",
       "      <th>f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.805</td>\n",
       "      <td>0.80725</td>\n",
       "      <td>0.812648</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.806673</td>\n",
       "      <td>0.80975</td>\n",
       "      <td>0.805141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  balanced_accuracy  precision  recall        f1      f05        f2\n",
       "0     0.805            0.80725   0.812648   0.805  0.806673  0.80975  0.805141"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>does_not_need_support</th>\n",
       "      <th>needs_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>does_not_need_support</td>\n",
       "      <td>290</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>needs_support</td>\n",
       "      <td>43</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Actual  does_not_need_support  needs_support\n",
       "0  does_not_need_support                    290             74\n",
       "1          needs_support                     43            193"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>does_not_need_support</th>\n",
       "      <td>0.870871</td>\n",
       "      <td>0.796703</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>364</td>\n",
       "      <td>0.606667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>needs_support</th>\n",
       "      <td>0.722846</td>\n",
       "      <td>0.817797</td>\n",
       "      <td>0.767396</td>\n",
       "      <td>236</td>\n",
       "      <td>0.393333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       precision    recall  f1-score  support      freq\n",
       "does_not_need_support   0.870871  0.796703  0.832138      364  0.606667\n",
       "needs_support           0.722846  0.817797  0.767396      236  0.393333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 ms, sys: 16 µs, total: 27.5 ms\n",
      "Wall time: 30.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_numpy = tokenized_datasets[\"test\"].data.to_pandas()[\"label\"].to_numpy()\n",
    "metrics_dict, df_cm, df_cr = calculate_metrics(\n",
    "    y_test_numpy,\n",
    "    y_test_pred,\n",
    "    list(label_mapper.values()),\n",
    "    list(label_mapper.keys()),\n",
    "    \"weighted\",\n",
    "    0,\n",
    "    use_sample_weights=False,\n",
    ")\n",
    "df_metrics = pd.DataFrame.from_dict(metrics_dict, orient=\"index\").T\n",
    "df_cr = df_cr.merge(\n",
    "    y_test.value_counts(normalize=True).rename(\"freq\").reset_index().assign(\n",
    "        index=lambda df: df[\"index\"].map(id2label)\n",
    "    ).set_index(\"index\"),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "display(df_metrics)\n",
    "display(df_cm)\n",
    "display(df_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e98716-9e64-4d97-8d31-b118f2ebbdda",
   "metadata": {},
   "source": [
    "Model evaluation is now performed on the predictions of the train split using the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec08fafe-c580-43a0-90e9-9ce978795983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f05</th>\n",
       "      <th>f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.861982</td>\n",
       "      <td>0.874404</td>\n",
       "      <td>0.877282</td>\n",
       "      <td>0.861982</td>\n",
       "      <td>0.864056</td>\n",
       "      <td>0.870689</td>\n",
       "      <td>0.861419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  balanced_accuracy  precision    recall        f1       f05  \\\n",
       "0  0.861982           0.874404   0.877282  0.861982  0.864056  0.870689   \n",
       "\n",
       "         f2  \n",
       "0  0.861419  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>does_not_need_support</th>\n",
       "      <th>needs_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>does_not_need_support</td>\n",
       "      <td>1447</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>needs_support</td>\n",
       "      <td>80</td>\n",
       "      <td>945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Actual  does_not_need_support  needs_support\n",
       "0  does_not_need_support                   1447            303\n",
       "1          needs_support                     80            945"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>does_not_need_support</th>\n",
       "      <td>0.947610</td>\n",
       "      <td>0.826857</td>\n",
       "      <td>0.883125</td>\n",
       "      <td>1750</td>\n",
       "      <td>0.630631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>needs_support</th>\n",
       "      <td>0.757212</td>\n",
       "      <td>0.921951</td>\n",
       "      <td>0.831500</td>\n",
       "      <td>1025</td>\n",
       "      <td>0.369369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       precision    recall  f1-score  support      freq\n",
       "does_not_need_support   0.947610  0.826857  0.883125     1750  0.630631\n",
       "needs_support           0.757212  0.921951  0.831500     1025  0.369369"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.5 ms, sys: 10 µs, total: 30.5 ms\n",
      "Wall time: 29.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train_numpy = tokenized_datasets[\"train\"].data.to_pandas()[\"label\"].to_numpy()\n",
    "metrics_dict_train, df_cm_train, df_cr_train = calculate_metrics(\n",
    "    y_train_numpy,\n",
    "    y_train_pred,\n",
    "    list(label_mapper.values()),\n",
    "    list(label_mapper.keys()),\n",
    "    \"weighted\",\n",
    "    0,\n",
    "    use_sample_weights=False,\n",
    ")\n",
    "df_metrics_train = pd.DataFrame.from_dict(metrics_dict_train, orient=\"index\").T\n",
    "df_cr_train = df_cr_train.merge(\n",
    "    y_train.value_counts(normalize=True).rename(\"freq\").reset_index().assign(\n",
    "        index=lambda df: df[\"index\"].map(id2label)\n",
    "    ).set_index(\"index\"),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "display(df_metrics_train)\n",
    "display(df_cm_train)\n",
    "display(df_cr_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3161c35-b727-43d3-bf5c-c5c2f895a1c3",
   "metadata": {},
   "source": [
    "Model evaluation is performed on the predictions of the test split using the naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76809d53-74d7-41e8-bed8-c19876e57d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f05</th>\n",
       "      <th>f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.515</td>\n",
       "      <td>0.516833</td>\n",
       "      <td>0.538858</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.520564</td>\n",
       "      <td>0.529993</td>\n",
       "      <td>0.515681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  balanced_accuracy  precision  recall        f1       f05  \\\n",
       "0     0.515           0.516833   0.538858   0.515  0.520564  0.529993   \n",
       "\n",
       "         f2  \n",
       "0  0.515681  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>does_not_need_support</th>\n",
       "      <th>needs_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>does_not_need_support</td>\n",
       "      <td>185</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>needs_support</td>\n",
       "      <td>112</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Actual  does_not_need_support  needs_support\n",
       "0  does_not_need_support                    185            179\n",
       "1          needs_support                    112            124"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>does_not_need_support</th>\n",
       "      <td>0.622896</td>\n",
       "      <td>0.508242</td>\n",
       "      <td>0.559758</td>\n",
       "      <td>364</td>\n",
       "      <td>0.606667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>needs_support</th>\n",
       "      <td>0.409241</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.460111</td>\n",
       "      <td>236</td>\n",
       "      <td>0.393333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       precision    recall  f1-score  support      freq\n",
       "does_not_need_support   0.622896  0.508242  0.559758      364  0.606667\n",
       "needs_support           0.409241  0.525424  0.460111      236  0.393333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test_pred_naive_numpy = y_test_pred_naive.astype(\"float64\").to_numpy()\n",
    "metrics_dict_naive, df_cm_naive, df_cr_naive = calculate_metrics(\n",
    "    y_test_numpy,\n",
    "    y_test_pred_naive_numpy,\n",
    "    list(label_mapper.values()),\n",
    "    list(label_mapper.keys()),\n",
    "    \"weighted\",\n",
    "    0,\n",
    "    use_sample_weights=False,\n",
    ")\n",
    "df_metrics_naive = pd.DataFrame.from_dict(metrics_dict_naive, orient=\"index\").T\n",
    "df_cr_naive = df_cr_naive.merge(\n",
    "    y_test.value_counts(normalize=True)\n",
    "    .rename(\"freq\")\n",
    "    .reset_index()\n",
    "    .assign(index=lambda df: df[\"index\"].map(id2label))\n",
    "    .set_index(\"index\"),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "display(df_metrics_naive)\n",
    "display(df_cm_naive)\n",
    "display(df_cr_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb3548d-5877-49c2-a7fe-bb7d2fe78bb1",
   "metadata": {},
   "source": [
    "Summarize the model evaluation metrics for both train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3aed820d-6e02-4ab4-be2f-3886f7a6530f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f05</th>\n",
       "      <th>f2</th>\n",
       "      <th>split_type</th>\n",
       "      <th>model_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.861982</td>\n",
       "      <td>0.874404</td>\n",
       "      <td>0.877282</td>\n",
       "      <td>0.861982</td>\n",
       "      <td>0.864056</td>\n",
       "      <td>0.870689</td>\n",
       "      <td>0.861419</td>\n",
       "      <td>train</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.807250</td>\n",
       "      <td>0.812648</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.806673</td>\n",
       "      <td>0.809750</td>\n",
       "      <td>0.805141</td>\n",
       "      <td>test</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.516833</td>\n",
       "      <td>0.538858</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.520564</td>\n",
       "      <td>0.529993</td>\n",
       "      <td>0.515681</td>\n",
       "      <td>test</td>\n",
       "      <td>naive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  balanced_accuracy  precision    recall        f1       f05  \\\n",
       "0  0.861982           0.874404   0.877282  0.861982  0.864056  0.870689   \n",
       "1  0.805000           0.807250   0.812648  0.805000  0.806673  0.809750   \n",
       "2  0.515000           0.516833   0.538858  0.515000  0.520564  0.529993   \n",
       "\n",
       "         f2 split_type model_type  \n",
       "0  0.861419      train         ML  \n",
       "1  0.805141       test         ML  \n",
       "2  0.515681       test      naive  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_combo = (\n",
    "    pd.concat([df_metrics_train, df_metrics, df_metrics_naive], ignore_index=True)\n",
    "    .assign(split_type=[\"train\", \"test\", \"test\"])\n",
    "    .assign(model_type=[\"ML\", \"ML\", \"naive\"])\n",
    ")\n",
    "df_metrics_combo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef17988-8aa8-4839-825a-53065738a3a2",
   "metadata": {},
   "source": [
    "Summarize the classification report for both train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b97324b3-a5ab-4101-a47a-4a756a44d20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>freq</th>\n",
       "      <th>split_type</th>\n",
       "      <th>model_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>does_not_need_support</td>\n",
       "      <td>0.947610</td>\n",
       "      <td>0.826857</td>\n",
       "      <td>0.883125</td>\n",
       "      <td>1750</td>\n",
       "      <td>0.630631</td>\n",
       "      <td>train</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does_not_need_support</td>\n",
       "      <td>0.870871</td>\n",
       "      <td>0.796703</td>\n",
       "      <td>0.832138</td>\n",
       "      <td>364</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>test</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>does_not_need_support</td>\n",
       "      <td>0.622896</td>\n",
       "      <td>0.508242</td>\n",
       "      <td>0.559758</td>\n",
       "      <td>364</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>test</td>\n",
       "      <td>naive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>needs_support</td>\n",
       "      <td>0.757212</td>\n",
       "      <td>0.921951</td>\n",
       "      <td>0.831500</td>\n",
       "      <td>1025</td>\n",
       "      <td>0.369369</td>\n",
       "      <td>train</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>needs_support</td>\n",
       "      <td>0.722846</td>\n",
       "      <td>0.817797</td>\n",
       "      <td>0.767396</td>\n",
       "      <td>236</td>\n",
       "      <td>0.393333</td>\n",
       "      <td>test</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>needs_support</td>\n",
       "      <td>0.409241</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.460111</td>\n",
       "      <td>236</td>\n",
       "      <td>0.393333</td>\n",
       "      <td>test</td>\n",
       "      <td>naive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   label  precision    recall  f1-score  support      freq  \\\n",
       "0  does_not_need_support   0.947610  0.826857  0.883125     1750  0.630631   \n",
       "2  does_not_need_support   0.870871  0.796703  0.832138      364  0.606667   \n",
       "4  does_not_need_support   0.622896  0.508242  0.559758      364  0.606667   \n",
       "1          needs_support   0.757212  0.921951  0.831500     1025  0.369369   \n",
       "3          needs_support   0.722846  0.817797  0.767396      236  0.393333   \n",
       "5          needs_support   0.409241  0.525424  0.460111      236  0.393333   \n",
       "\n",
       "  split_type model_type  \n",
       "0      train         ML  \n",
       "2       test         ML  \n",
       "4       test      naive  \n",
       "1      train         ML  \n",
       "3       test         ML  \n",
       "5       test      naive  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cr_combo = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            df_cr_train.assign(split_type=\"train\").assign(model_type=\"ML\"),\n",
    "            df_cr.assign(split_type=\"test\").assign(model_type=\"ML\"),\n",
    "            df_cr_naive.assign(split_type=\"test\").assign(model_type=\"naive\"),\n",
    "        ]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"label\"})\n",
    "    .sort_values(by=[\"label\"])\n",
    ")\n",
    "df_cr_combo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88384886-0d45-444f-b935-8ff19224a3a1",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. As mentioned in `6-split-data.ipynb`, shorter tweets were retrospectively added to the business metrics evaluation batches. So, the total number of tweets in each split will be different from those shown in `6-split-data.ipynb` - see that notebook for more details. See **Notes** point 2. from **Summary of Sizes of Datasets Used in This Notebook** in `6_split_data.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca64502b-81fb-461b-b90d-d4bc2608c4d9",
   "metadata": {},
   "source": [
    "### Append Predictions and Features to the Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89bc73-8d26-4132-9078-dc30f2154b24",
   "metadata": {},
   "source": [
    "Append test split predictions to the test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "039f250a-e952-41fb-8544-8d9998a96d06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>labels</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_naive</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>1480261790567182336</td>\n",
       "      <td>2022-01-09 19:34:47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>1480033459083087873</td>\n",
       "      <td>2022-01-09 04:27:28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1480292181306871810</td>\n",
       "      <td>2022-01-09 21:35:33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>1480239785574912008</td>\n",
       "      <td>2022-01-09 18:07:21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1480055280264220672</td>\n",
       "      <td>2022-01-09 05:54:11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1480337582387040259</td>\n",
       "      <td>2022-01-10 00:35:57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>1480045552041562114</td>\n",
       "      <td>2022-01-09 05:15:32</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1479999803761307651</td>\n",
       "      <td>2022-01-09 02:13:44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>1480060383368986625</td>\n",
       "      <td>2022-01-09 06:14:28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1480008567189090307</td>\n",
       "      <td>2022-01-09 02:48:34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id          created_at  labels  pred  pred_naive split\n",
       "548  1480261790567182336 2022-01-09 19:34:47       1     1           1  test\n",
       "437  1480033459083087873 2022-01-09 04:27:28       0     0           0  test\n",
       "241  1480292181306871810 2022-01-09 21:35:33       0     1           1  test\n",
       "525  1480239785574912008 2022-01-09 18:07:21       0     0           1  test\n",
       "236  1480055280264220672 2022-01-09 05:54:11       1     1           0  test\n",
       "68   1480337582387040259 2022-01-10 00:35:57       0     0           0  test\n",
       "535  1480045552041562114 2022-01-09 05:15:32       1     1           0  test\n",
       "204  1479999803761307651 2022-01-09 02:13:44       0     1           1  test\n",
       "383  1480060383368986625 2022-01-09 06:14:28       0     0           1  test\n",
       "252  1480008567189090307 2022-01-09 02:48:34       0     0           0  test"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test.assign(pred=pd.Series(y_test_pred, index=df_test.index)).assign(\n",
    "    pred_naive=pd.Series(y_test_pred_naive, index=df_test.index)\n",
    ")\n",
    "df_test[[\"id\", \"created_at\", \"labels\", \"pred\", \"pred_naive\", \"split\"]].sample(\n",
    "    n=10, random_state=88\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673893c-b36c-4261-bde0-540ec311e5d0",
   "metadata": {},
   "source": [
    "Engineer features from metadata for the test split, including appending a *response time* column (approximate time spent reading and responding to tweets) based on the\n",
    "- number of words in each tweet\n",
    "- [average reading speed of 130 words per minute](https://www.omnicalculator.com/everyday-life/words-per-minute)\n",
    "- [average typing speed of 40 wpm](https://www.ratatype.com/learn/average-typing-speed/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02ac1a48-eb1e-4ca4-b649-bddcf0b1ba05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted country for every user_location\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering for metadata\n",
    "df_test = (\n",
    "    df_test.assign(error=lambda df: df[\"labels\"] != df[\"pred\"])\n",
    "    .assign(created_at_hour=lambda df: df[\"created_at\"].dt.hour)\n",
    "    .assign(created_at_day=lambda df: df[\"created_at\"].dt.day_name())\n",
    "    .assign(user_joined_hour=lambda df: df[\"user_joined\"].dt.hour)\n",
    "    .assign(user_joined_day=lambda df: df[\"user_joined\"].dt.day_name())\n",
    "    .assign(\n",
    "        created_at_time_of_day=lambda df: pd.cut(\n",
    "            df[\"created_at_hour\"], bins=b, labels=l, include_lowest=True\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        reading_time=lambda df: df[\"num_words\"] * (1 / avg_reading_speed_wpm) * (60 / 1)\n",
    "    )\n",
    "    .assign(\n",
    "        replying_time=lambda df: df[\"num_words\"]\n",
    "        * frac_support_tweets_needing_response\n",
    "        * (1 / avg_typing_speed_wpm)\n",
    "        * (60 / 1)\n",
    "    )\n",
    "    .assign(response_time=lambda df: df[\"reading_time\"] + df[\"replying_time\"])\n",
    "    .astype(test_feats_dtypes_test_dict)\n",
    ")\n",
    "df_test[\"country\"] = \"Other\"\n",
    "for k, v in mdict.items():\n",
    "    mask = df_test[\"user_location\"].str.contains(\"|\".join(v))\n",
    "    df_test.loc[mask, \"country\"] = k\n",
    "df_test = df_test.astype({\"country\": pd.StringDtype()})\n",
    "# Verify that a country is present for every row of metadata\n",
    "try:\n",
    "    assert df_test[\"country\"].value_counts().sum() == len(df_test)\n",
    "    print(\"Extracted country for every user_location\")\n",
    "except AssertionError as e:\n",
    "    print(f\"{str(e)}: Did not sucessfully extract country for every user_location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddad326-1cbc-469a-9979-94305e3756a3",
   "metadata": {},
   "source": [
    "### Test Split Errors by Source Country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0560f-6170-4d65-8bd6-edcdbbb5874b",
   "metadata": {},
   "source": [
    "Summarize model errors of the test split by the country from which the tweet originated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e865d37-6de4-4264-bcb1-cf034b7cb1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>error</th>\n",
       "      <th>error_freq</th>\n",
       "      <th>counts</th>\n",
       "      <th>freq</th>\n",
       "      <th>freq_rank</th>\n",
       "      <th>error_freq_rank</th>\n",
       "      <th>error_freq_to_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>7</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Canada</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>15</td>\n",
       "      <td>0.025</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Other</td>\n",
       "      <td>85</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>420</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.202381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USA</td>\n",
       "      <td>23</td>\n",
       "      <td>0.038333</td>\n",
       "      <td>116</td>\n",
       "      <td>0.193333</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.198276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UK</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>25</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Australia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>India</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     country  error  error_freq  counts      freq  freq_rank  error_freq_rank  \\\n",
       "4    Germany      2    0.003333       7  0.011667          6                5   \n",
       "2     Canada      4    0.006667      15     0.025          4                3   \n",
       "0      Other     85    0.141667     420       0.7          1                1   \n",
       "1        USA     23    0.038333     116  0.193333          2                2   \n",
       "3         UK      3       0.005      25  0.041667          3                4   \n",
       "5  Australia      0         0.0       4  0.006667          7                6   \n",
       "6      India      0         0.0      12      0.02          5                6   \n",
       "7      Kenya      0         0.0       1  0.001667          8                6   \n",
       "\n",
       "   error_freq_to_freq  \n",
       "4            0.285714  \n",
       "2            0.266667  \n",
       "0            0.202381  \n",
       "1            0.198276  \n",
       "3                0.12  \n",
       "5                 0.0  \n",
       "6                 0.0  \n",
       "7                 0.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_country = (\n",
    "    df_test[\"country\"]\n",
    "    .value_counts()\n",
    "    .rename(\"counts\")\n",
    "    .to_frame()\n",
    "    .merge(\n",
    "        df_test[\"country\"].value_counts(normalize=True).rename(\"freq\").to_frame(),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "df_error_by_country = (\n",
    "    df_test.groupby(\"country\", as_index=False)[\"error\"]\n",
    "    .sum()\n",
    "    .sort_values(by=[\"error\"], ascending=False)\n",
    "    .assign(error_freq=lambda df: df[\"error\"] / len(df_test))\n",
    "    .set_index(\"country\")\n",
    "    .merge(df_by_country, left_index=True, right_index=True, how=\"left\")\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        freq_rank=lambda df: df[\"freq\"]\n",
    "        .rank(ascending=False, method=\"dense\")\n",
    "        .astype(int)\n",
    "        .astype(pd.Int32Dtype())\n",
    "    )\n",
    "    .assign(\n",
    "        error_freq_rank=lambda df: df[\"error_freq\"]\n",
    "        .rank(ascending=False, method=\"dense\")\n",
    "        .astype(int)\n",
    "        .astype(pd.Int32Dtype())\n",
    "    )\n",
    "    .assign(error_freq_to_freq=lambda df: df[\"error\"] / df[\"counts\"])\n",
    "    .sort_values(by=[\"error_freq_to_freq\"], ascending=False)\n",
    ")\n",
    "df_error_by_country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf6265-45ee-49ba-b9c7-31e961bbcfaa",
   "metadata": {},
   "source": [
    "### Test Split Errors by Weekday and Hour of Day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80089305-b7a9-41b3-81ff-ff4961a718b7",
   "metadata": {},
   "source": [
    "Summarize model errors of the test split by\n",
    "- hour the day\n",
    "- day of the week (Monday - Sunday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2edd6dd-5f29-42ca-a1ca-a057b3c62591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f9d51_row0_col0, #T_f9d51_row1_col0, #T_f9d51_row2_col0, #T_f9d51_row3_col0, #T_f9d51_row4_col0, #T_f9d51_row5_col0, #T_f9d51_row5_col1 {\n",
       "  background-color: #ffffcc;\n",
       "  color: #000000;\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_f9d51_row0_col1 {\n",
       "  background-color: #fc5f2f;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_f9d51_row1_col1 {\n",
       "  background-color: #fff8bb;\n",
       "  color: #000000;\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_f9d51_row2_col1 {\n",
       "  background-color: #ef3323;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_f9d51_row3_col1 {\n",
       "  background-color: #800026;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_f9d51_row4_col1 {\n",
       "  background-color: #fed36f;\n",
       "  color: #000000;\n",
       "  font-size: 12px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f9d51\">\n",
       "  <caption>Tweet Frequency by Time of Day</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >created_at_day</th>\n",
       "      <th id=\"T_f9d51_level0_col0\" class=\"col_heading level0 col0\" >Monday</th>\n",
       "      <th id=\"T_f9d51_level0_col1\" class=\"col_heading level0 col1\" >Sunday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >created_at_time_of_day</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f9d51_level0_row0\" class=\"row_heading level0 row0\" >Afternoon</th>\n",
       "      <td id=\"T_f9d51_row0_col0\" class=\"data row0 col0\" >nan</td>\n",
       "      <td id=\"T_f9d51_row0_col1\" class=\"data row0 col1\" >0.181667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9d51_level0_row1\" class=\"row_heading level0 row1\" >Early Morning</th>\n",
       "      <td id=\"T_f9d51_row1_col0\" class=\"data row1 col0\" >nan</td>\n",
       "      <td id=\"T_f9d51_row1_col1\" class=\"data row1 col1\" >0.108333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9d51_level0_row2\" class=\"row_heading level0 row2\" >Evening</th>\n",
       "      <td id=\"T_f9d51_row2_col0\" class=\"data row2 col0\" >nan</td>\n",
       "      <td id=\"T_f9d51_row2_col1\" class=\"data row2 col1\" >0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9d51_level0_row3\" class=\"row_heading level0 row3\" >Late Night</th>\n",
       "      <td id=\"T_f9d51_row3_col0\" class=\"data row3 col0\" >0.038333</td>\n",
       "      <td id=\"T_f9d51_row3_col1\" class=\"data row3 col1\" >0.236667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9d51_level0_row4\" class=\"row_heading level0 row4\" >Morning</th>\n",
       "      <td id=\"T_f9d51_row4_col0\" class=\"data row4 col0\" >nan</td>\n",
       "      <td id=\"T_f9d51_row4_col1\" class=\"data row4 col1\" >0.138333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9d51_level0_row5\" class=\"row_heading level0 row5\" >Night</th>\n",
       "      <td id=\"T_f9d51_row5_col0\" class=\"data row5 col0\" >nan</td>\n",
       "      <td id=\"T_f9d51_row5_col1\" class=\"data row5 col1\" >0.101667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5ae45a3a30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ac230_row0_col0, #T_ac230_row1_col0, #T_ac230_row1_col1, #T_ac230_row2_col0, #T_ac230_row3_col0, #T_ac230_row4_col0, #T_ac230_row5_col0 {\n",
       "  background-color: #ffffcc;\n",
       "  color: #000000;\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_ac230_row0_col1 {\n",
       "  background-color: #fd9c42;\n",
       "  color: #000000;\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_ac230_row2_col1 {\n",
       "  background-color: #e2191c;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_ac230_row3_col1 {\n",
       "  background-color: #800026;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_ac230_row4_col1 {\n",
       "  background-color: #fff1a9;\n",
       "  color: #000000;\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_ac230_row5_col1 {\n",
       "  background-color: #feca66;\n",
       "  color: #000000;\n",
       "  font-size: 12px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ac230\">\n",
       "  <caption>Error Frequency by Time of Day</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >created_at_day</th>\n",
       "      <th id=\"T_ac230_level0_col0\" class=\"col_heading level0 col0\" >Monday</th>\n",
       "      <th id=\"T_ac230_level0_col1\" class=\"col_heading level0 col1\" >Sunday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >created_at_time_of_day</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ac230_level0_row0\" class=\"row_heading level0 row0\" >Afternoon</th>\n",
       "      <td id=\"T_ac230_row0_col0\" class=\"data row0 col0\" >nan</td>\n",
       "      <td id=\"T_ac230_row0_col1\" class=\"data row0 col1\" >0.031667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ac230_level0_row1\" class=\"row_heading level0 row1\" >Early Morning</th>\n",
       "      <td id=\"T_ac230_row1_col0\" class=\"data row1 col0\" >nan</td>\n",
       "      <td id=\"T_ac230_row1_col1\" class=\"data row1 col1\" >0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ac230_level0_row2\" class=\"row_heading level0 row2\" >Evening</th>\n",
       "      <td id=\"T_ac230_row2_col0\" class=\"data row2 col0\" >nan</td>\n",
       "      <td id=\"T_ac230_row2_col1\" class=\"data row2 col1\" >0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ac230_level0_row3\" class=\"row_heading level0 row3\" >Late Night</th>\n",
       "      <td id=\"T_ac230_row3_col0\" class=\"data row3 col0\" >0.008333</td>\n",
       "      <td id=\"T_ac230_row3_col1\" class=\"data row3 col1\" >0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ac230_level0_row4\" class=\"row_heading level0 row4\" >Morning</th>\n",
       "      <td id=\"T_ac230_row4_col0\" class=\"data row4 col0\" >nan</td>\n",
       "      <td id=\"T_ac230_row4_col1\" class=\"data row4 col1\" >0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ac230_level0_row5\" class=\"row_heading level0 row5\" >Night</th>\n",
       "      <td id=\"T_ac230_row5_col0\" class=\"data row5 col0\" >nan</td>\n",
       "      <td id=\"T_ac230_row5_col1\" class=\"data row5 col1\" >0.026667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5ae6e72ec0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_by_time_of_day = (\n",
    "    (\n",
    "        df_test.groupby([\"created_at_time_of_day\", \"created_at_day\"])[\"id\"].count()\n",
    "        / len(df_test)\n",
    "    )\n",
    "    .reset_index()\n",
    "    .astype({\"id\": pd.Float32Dtype()})\n",
    "    .pivot(\n",
    "        index=\"created_at_time_of_day\",\n",
    "        columns=[\"created_at_day\"],\n",
    "        values=\"id\",\n",
    "    )\n",
    ")\n",
    "df_error_by_time_of_day = (\n",
    "    (\n",
    "        df_test.groupby([\"created_at_time_of_day\", \"created_at_day\"])[\"error\"].sum()\n",
    "        / len(df_test)\n",
    "    )\n",
    "    .reset_index()\n",
    "    .astype({\"error\": pd.Float32Dtype()})\n",
    "    .pivot(\n",
    "        index=\"created_at_time_of_day\",\n",
    "        columns=[\"created_at_day\"],\n",
    "        values=\"error\",\n",
    "    )\n",
    ")\n",
    "display(\n",
    "    df_by_time_of_day.astype(\"float64\")\n",
    "    .style.set_caption(\"Tweet Frequency by Time of Day\")\n",
    "    .background_gradient(cmap=\"YlOrRd\")\n",
    "    .set_properties(**{\"font-size\": \"12px\"})\n",
    ")\n",
    "display(\n",
    "    df_error_by_time_of_day.astype(\"float64\")\n",
    "    .style.set_caption(\"Error Frequency by Time of Day\")\n",
    "    .background_gradient(cmap=\"YlOrRd\")\n",
    "    .set_properties(**{\"font-size\": \"12px\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d88b3-e1b0-4066-b325-f121620abd88",
   "metadata": {},
   "source": [
    "### Test Split Errors by Twitter Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef108c-74a6-4036-8e3e-d8730e2f70ea",
   "metadata": {},
   "source": [
    "Summarize model errors of the test split by the Twitter client used to post the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3c69bc8-279e-4c6b-8d41-d4c97296223b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client</th>\n",
       "      <th>error</th>\n",
       "      <th>error_freq</th>\n",
       "      <th>counts</th>\n",
       "      <th>freq</th>\n",
       "      <th>freq_rank</th>\n",
       "      <th>error_freq_rank</th>\n",
       "      <th>error_freq_to_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>53</td>\n",
       "      <td>0.088333</td>\n",
       "      <td>215</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.246512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>31</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>179</td>\n",
       "      <td>0.298333</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.173184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>27</td>\n",
       "      <td>0.045</td>\n",
       "      <td>166</td>\n",
       "      <td>0.276667</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.162651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPad</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>20</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alt-brain news test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>8</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>autonewssite.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hacker__News</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Heropost</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IFTTT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SocialChamp IO</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tweetbot for Mac</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Typefully</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hncynic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 client  error  error_freq  counts      freq  freq_rank  \\\n",
       "0       Twitter Web App     53    0.088333     215  0.358333          1   \n",
       "1    Twitter for iPhone     31    0.051667     179  0.298333          2   \n",
       "2   Twitter for Android     27       0.045     166  0.276667          3   \n",
       "3      Twitter for iPad      3       0.005      20  0.033333          4   \n",
       "4   Alt-brain news test      1    0.001667       1  0.001667          8   \n",
       "5             TweetDeck      1    0.001667       8  0.013333          5   \n",
       "6      autonewssite.com      1    0.001667       1  0.001667          8   \n",
       "7          Hacker__News      0         0.0       1  0.001667          8   \n",
       "8              Heropost      0         0.0       1  0.001667          8   \n",
       "9                 IFTTT      0         0.0       3     0.005          6   \n",
       "10       SocialChamp IO      0         0.0       1  0.001667          8   \n",
       "11     Tweetbot for Mac      0         0.0       1  0.001667          8   \n",
       "12            Typefully      0         0.0       2  0.003333          7   \n",
       "13              hncynic      0         0.0       1  0.001667          8   \n",
       "\n",
       "    error_freq_rank  error_freq_to_freq  \n",
       "0                 1            0.246512  \n",
       "1                 2            0.173184  \n",
       "2                 3            0.162651  \n",
       "3                 4                0.15  \n",
       "4                 5                 1.0  \n",
       "5                 5               0.125  \n",
       "6                 5                 1.0  \n",
       "7                 6                 0.0  \n",
       "8                 6                 0.0  \n",
       "9                 6                 0.0  \n",
       "10                6                 0.0  \n",
       "11                6                 0.0  \n",
       "12                6                 0.0  \n",
       "13                6                 0.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_device = (\n",
    "    df_test[\"source_text\"]\n",
    "    .value_counts()\n",
    "    .rename(\"counts\")\n",
    "    .to_frame()\n",
    "    .merge(\n",
    "        df_test[\"source_text\"].value_counts(normalize=True).rename(\"freq\").to_frame(),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "df_error_by_device = (\n",
    "    df_test.groupby(\"source_text\", as_index=False)[\"error\"]\n",
    "    .sum()\n",
    "    .sort_values(by=[\"error\"], ascending=False)\n",
    "    .assign(error_freq=lambda df: df[\"error\"] / len(df_test))\n",
    "    .set_index(\"source_text\")\n",
    "    .merge(df_by_device, left_index=True, right_index=True, how=\"left\")\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        freq_rank=lambda df: df[\"freq\"]\n",
    "        .rank(ascending=False, method=\"dense\")\n",
    "        .astype(int)\n",
    "        .astype(pd.Int32Dtype())\n",
    "    )\n",
    "    .assign(\n",
    "        error_freq_rank=lambda df: df[\"error_freq\"]\n",
    "        .rank(ascending=False, method=\"dense\")\n",
    "        .astype(int)\n",
    "        .astype(pd.Int32Dtype())\n",
    "    )\n",
    "    .assign(error_freq_to_freq=lambda df: df[\"error\"] / df[\"counts\"])\n",
    "    .sort_values(by=[\"error_freq\"], ascending=False)\n",
    "    .rename(columns={\"source_text\": \"client\"})\n",
    ")\n",
    "df_error_by_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96897c-f43a-43c8-910b-d50aa7f9904f",
   "metadata": {},
   "source": [
    "### Business Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8400b1-c802-47c4-ad4d-1797669bdd5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab9b4690-da94-40b1-a988-3f19098b4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missed = df_test.query(\"(labels == 1) & (pred == 0)\")\n",
    "df_wasted = df_test.query(\"(labels == 0) & (pred == 1)\")\n",
    "df_missed_naive = df_test.query(\"(labels == 1) & (pred_naive == 0)\")\n",
    "df_wasted_naive = df_test.query(\"(labels == 0) & (pred_naive == 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b5d42-804a-44bd-9fa3-5610e5d3260d",
   "metadata": {},
   "source": [
    "Calculate the business metrics for the test split using the\n",
    "- fine-tuned ML\n",
    "- naive\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60bc85ab-e63a-47e1-b2d3-41e975e6e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = (\n",
    "    # business metrics for current test split\n",
    "    pd.concat(\n",
    "        [\n",
    "            pd.DataFrame.from_dict(\n",
    "                dict(\n",
    "                    time_missed_ML=df_missed[\"response_time\"].sum() / 60,\n",
    "                    time_missed_naive=df_missed_naive[\"response_time\"].sum() / 60,\n",
    "                ),\n",
    "                orient=\"index\",\n",
    "            ),\n",
    "            pd.DataFrame.from_dict(\n",
    "                dict(\n",
    "                    time_wasted_ML=df_wasted[\"response_time\"].sum() / 60,\n",
    "                    time_wasted_naive=df_wasted_naive[\"response_time\"].sum() / 60,\n",
    "                ),\n",
    "                orient=\"index\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    .T.assign(num_tweets_missed_ML=len(df_missed))\n",
    "    .assign(num_tweets_missed_naive=len(df_missed_naive))\n",
    "    .assign(num_tweets_unnecessarily_read_ML=len(df_wasted))\n",
    "    .assign(num_tweets_unnecessarily_read_naive=len(df_wasted_naive))\n",
    "    .assign(\n",
    "        frac_tweets_unnecessarily_read_ML=lambda df: df[\n",
    "            \"num_tweets_unnecessarily_read_ML\"\n",
    "        ]\n",
    "        / len(df_test)\n",
    "    )\n",
    "    .assign(\n",
    "        frac_tweets_unnecessarily_read_naive=lambda df: df[\n",
    "            \"num_tweets_unnecessarily_read_naive\"\n",
    "        ]\n",
    "        / len(df_test)\n",
    "    )\n",
    "    .assign(frac_tweets_missed_ML=lambda df: df[\"num_tweets_missed_ML\"] / len(df_test))\n",
    "    .assign(\n",
    "        frac_tweets_missed_naive=lambda df: df[\"num_tweets_missed_naive\"] / len(df_test)\n",
    "    )\n",
    "    .assign(\n",
    "        response_time_pred_ML=df_test.query(\"pred == 1\")[\"response_time\"].sum() / 60\n",
    "    )\n",
    "    .assign(\n",
    "        response_time_pred_naive=df_test.query(\"pred_naive == 1\")[\"response_time\"].sum()\n",
    "        / 60\n",
    "    )\n",
    "    .T.reset_index()\n",
    ")\n",
    "df_summary[[\"business_metric\", \"model_type\"]] = df_summary[\"index\"].str.rsplit(\n",
    "    \"_\", n=1, expand=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca20763-8519-4d7f-afe8-0fe2dac6614b",
   "metadata": {},
   "source": [
    "Get the combined ML evaluation and business metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c5f2c54-5884-483a-a2e4-18e3edd51001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_572c0\">\n",
       "  <caption>ML and Business Metrics for Latest Batch of Data (batch 1) in Test Split</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >model_type</th>\n",
       "      <th id=\"T_572c0_level0_col0\" class=\"col_heading level0 col0\" >ML</th>\n",
       "      <th id=\"T_572c0_level0_col1\" class=\"col_heading level0 col1\" >naive</th>\n",
       "      <th id=\"T_572c0_level0_col2\" class=\"col_heading level0 col2\" >data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row0\" class=\"row_heading level0 row0\" >batch_num</th>\n",
       "      <td id=\"T_572c0_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_572c0_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_572c0_row0_col2\" class=\"data row0 col2\" >Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row1\" class=\"row_heading level0 row1\" >total_number_tweets</th>\n",
       "      <td id=\"T_572c0_row1_col0\" class=\"data row1 col0\" >600</td>\n",
       "      <td id=\"T_572c0_row1_col1\" class=\"data row1 col1\" >600</td>\n",
       "      <td id=\"T_572c0_row1_col2\" class=\"data row1 col2\" >Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row2\" class=\"row_heading level0 row2\" >num_needs_support</th>\n",
       "      <td id=\"T_572c0_row2_col0\" class=\"data row2 col0\" >236</td>\n",
       "      <td id=\"T_572c0_row2_col1\" class=\"data row2 col1\" >236</td>\n",
       "      <td id=\"T_572c0_row2_col2\" class=\"data row2 col2\" >Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row3\" class=\"row_heading level0 row3\" >accuracy</th>\n",
       "      <td id=\"T_572c0_row3_col0\" class=\"data row3 col0\" >0.805000</td>\n",
       "      <td id=\"T_572c0_row3_col1\" class=\"data row3 col1\" >0.515000</td>\n",
       "      <td id=\"T_572c0_row3_col2\" class=\"data row3 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row4\" class=\"row_heading level0 row4\" >balanced_accuracy</th>\n",
       "      <td id=\"T_572c0_row4_col0\" class=\"data row4 col0\" >0.807250</td>\n",
       "      <td id=\"T_572c0_row4_col1\" class=\"data row4 col1\" >0.516833</td>\n",
       "      <td id=\"T_572c0_row4_col2\" class=\"data row4 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row5\" class=\"row_heading level0 row5\" >precision</th>\n",
       "      <td id=\"T_572c0_row5_col0\" class=\"data row5 col0\" >0.812648</td>\n",
       "      <td id=\"T_572c0_row5_col1\" class=\"data row5 col1\" >0.538858</td>\n",
       "      <td id=\"T_572c0_row5_col2\" class=\"data row5 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row6\" class=\"row_heading level0 row6\" >recall</th>\n",
       "      <td id=\"T_572c0_row6_col0\" class=\"data row6 col0\" >0.805000</td>\n",
       "      <td id=\"T_572c0_row6_col1\" class=\"data row6 col1\" >0.515000</td>\n",
       "      <td id=\"T_572c0_row6_col2\" class=\"data row6 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row7\" class=\"row_heading level0 row7\" >f1</th>\n",
       "      <td id=\"T_572c0_row7_col0\" class=\"data row7 col0\" >0.806673</td>\n",
       "      <td id=\"T_572c0_row7_col1\" class=\"data row7 col1\" >0.520564</td>\n",
       "      <td id=\"T_572c0_row7_col2\" class=\"data row7 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row8\" class=\"row_heading level0 row8\" >f05</th>\n",
       "      <td id=\"T_572c0_row8_col0\" class=\"data row8 col0\" >0.809750</td>\n",
       "      <td id=\"T_572c0_row8_col1\" class=\"data row8 col1\" >0.529993</td>\n",
       "      <td id=\"T_572c0_row8_col2\" class=\"data row8 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row9\" class=\"row_heading level0 row9\" >f2</th>\n",
       "      <td id=\"T_572c0_row9_col0\" class=\"data row9 col0\" >0.805141</td>\n",
       "      <td id=\"T_572c0_row9_col1\" class=\"data row9 col1\" >0.515681</td>\n",
       "      <td id=\"T_572c0_row9_col2\" class=\"data row9 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row10\" class=\"row_heading level0 row10\" >frac_tweets_missed</th>\n",
       "      <td id=\"T_572c0_row10_col0\" class=\"data row10 col0\" >0.071667</td>\n",
       "      <td id=\"T_572c0_row10_col1\" class=\"data row10 col1\" >0.186667</td>\n",
       "      <td id=\"T_572c0_row10_col2\" class=\"data row10 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row11\" class=\"row_heading level0 row11\" >frac_tweets_unnecessarily_read</th>\n",
       "      <td id=\"T_572c0_row11_col0\" class=\"data row11 col0\" >0.123333</td>\n",
       "      <td id=\"T_572c0_row11_col1\" class=\"data row11 col1\" >0.298333</td>\n",
       "      <td id=\"T_572c0_row11_col2\" class=\"data row11 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row12\" class=\"row_heading level0 row12\" >num_tweets_missed</th>\n",
       "      <td id=\"T_572c0_row12_col0\" class=\"data row12 col0\" >43</td>\n",
       "      <td id=\"T_572c0_row12_col1\" class=\"data row12 col1\" >112</td>\n",
       "      <td id=\"T_572c0_row12_col2\" class=\"data row12 col2\" >Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row13\" class=\"row_heading level0 row13\" >num_tweets_unnecessarily_read</th>\n",
       "      <td id=\"T_572c0_row13_col0\" class=\"data row13 col0\" >74</td>\n",
       "      <td id=\"T_572c0_row13_col1\" class=\"data row13 col1\" >179</td>\n",
       "      <td id=\"T_572c0_row13_col2\" class=\"data row13 col2\" >Int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row14\" class=\"row_heading level0 row14\" >pct_reduction_in_time_missed</th>\n",
       "      <td id=\"T_572c0_row14_col0\" class=\"data row14 col0\" >70.825148</td>\n",
       "      <td id=\"T_572c0_row14_col1\" class=\"data row14 col1\" >70.825148</td>\n",
       "      <td id=\"T_572c0_row14_col2\" class=\"data row14 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row15\" class=\"row_heading level0 row15\" >pct_reduction_in_time_wasted</th>\n",
       "      <td id=\"T_572c0_row15_col0\" class=\"data row15 col0\" >50.261778</td>\n",
       "      <td id=\"T_572c0_row15_col1\" class=\"data row15 col1\" >50.261778</td>\n",
       "      <td id=\"T_572c0_row15_col2\" class=\"data row15 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row16\" class=\"row_heading level0 row16\" >response_time_pred</th>\n",
       "      <td id=\"T_572c0_row16_col0\" class=\"data row16 col0\" >240.462956</td>\n",
       "      <td id=\"T_572c0_row16_col1\" class=\"data row16 col1\" >235.699072</td>\n",
       "      <td id=\"T_572c0_row16_col2\" class=\"data row16 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row17\" class=\"row_heading level0 row17\" >response_time_true</th>\n",
       "      <td id=\"T_572c0_row17_col0\" class=\"data row17 col0\" >204.685189</td>\n",
       "      <td id=\"T_572c0_row17_col1\" class=\"data row17 col1\" >204.685189</td>\n",
       "      <td id=\"T_572c0_row17_col2\" class=\"data row17 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row18\" class=\"row_heading level0 row18\" >time_missed</th>\n",
       "      <td id=\"T_572c0_row18_col0\" class=\"data row18 col0\" >28.875000</td>\n",
       "      <td id=\"T_572c0_row18_col1\" class=\"data row18 col1\" >98.972225</td>\n",
       "      <td id=\"T_572c0_row18_col2\" class=\"data row18 col2\" >Float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_572c0_level0_row19\" class=\"row_heading level0 row19\" >time_wasted</th>\n",
       "      <td id=\"T_572c0_row19_col0\" class=\"data row19 col0\" >64.652779</td>\n",
       "      <td id=\"T_572c0_row19_col1\" class=\"data row19 col1\" >129.986108</td>\n",
       "      <td id=\"T_572c0_row19_col2\" class=\"data row19 col2\" >Float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5ae72f7eb0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            # metadata for current test split\n",
    "            pd.DataFrame()\n",
    "            .assign(ML=[test_current_batch_num])\n",
    "            .assign(naive=[test_current_batch_num])\n",
    "            .T.assign(total_number_tweets=len(df_test))\n",
    "            .assign(num_needs_support=len(df_test.query(\"labels == 1\")))\n",
    "            .rename(columns={0: \"batch_num\"})\n",
    "            .T,\n",
    "            # ML evaluation metrics for current test split\n",
    "            pd.concat(\n",
    "                [\n",
    "                    pd.DataFrame.from_dict(metrics_dict, orient=\"index\")\n",
    "                    .squeeze()\n",
    "                    .rename(\"ML\")\n",
    "                    .to_frame()\n",
    "                    .astype(pd.Float32Dtype()),\n",
    "                    pd.DataFrame.from_dict(metrics_dict_naive, orient=\"index\")\n",
    "                    .squeeze()\n",
    "                    .rename(\"naive\")\n",
    "                    .to_frame()\n",
    "                    .astype(pd.Float32Dtype()),\n",
    "                ],\n",
    "                axis=1,\n",
    "            ),\n",
    "            # business metrics for current test split (contd.)\n",
    "            (\n",
    "                df_summary.iloc[:, 1:]\n",
    "                .pivot(index=[\"business_metric\"], columns=[\"model_type\"], values=[0])\n",
    "                .T.assign(\n",
    "                    response_time_true=df_test.query(\"labels == 1\")[\n",
    "                        \"response_time\"\n",
    "                    ].sum()\n",
    "                    / 60\n",
    "                )\n",
    "                .reset_index(level=0, drop=True)\n",
    "                .assign(\n",
    "                    pct_reduction_in_time_wasted=lambda df: 100\n",
    "                    * (df.loc[\"naive\", \"time_wasted\"] - df.loc[\"ML\", \"time_wasted\"])\n",
    "                    / df.loc[\"naive\", \"time_wasted\"]\n",
    "                )\n",
    "                .assign(\n",
    "                    pct_reduction_in_time_missed=lambda df: 100\n",
    "                    * (df.loc[\"naive\", \"time_missed\"] - df.loc[\"ML\", \"time_missed\"])\n",
    "                    / df.loc[\"naive\", \"time_missed\"]\n",
    "                )\n",
    "                .T\n",
    "            ).sort_index(),\n",
    "        ]\n",
    "    )\n",
    "    .T.astype(metrics_dtypes_dict)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"model_type\"})\n",
    ")\n",
    "df_summary.set_index(\"model_type\").T.assign(\n",
    "    data_type=df_summary.set_index(\"model_type\").dtypes.to_frame()\n",
    ").style.set_caption(\n",
    "    f\"ML and Business Metrics for Latest Batch of Data (batch \"\n",
    "    f\"{test_current_batch_num}) in Test Split\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b480186-b16a-4bbb-8e5d-91cbfc42dfbc",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The time missed and time wasted are lower with the ML model approach compared to the non-ML approach. Due to this, the percent reduction in time missed and time wasted are greater than zero, which indicates that the ML approach to predicting whether tweets need support delivers value over the non-ML approach.\n",
    "2. The F2-score meets the required threshold of 80% set in the project scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d4bbb-1fe1-4413-8ffe-fddbcfb9bfb7",
   "metadata": {},
   "source": [
    "## Export Data and Model Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01934059-6319-4e4e-ad32-5acf1b106a74",
   "metadata": {},
   "source": [
    "### Test Split Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b51673ef-988b-4889-9ff7-aa74c41728a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/processed/metrics__inference_starts_20220110_000000__batch_1.parquet.gzip\n",
      "metrics__inference_starts_20220110_000000__batch_1.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "metrics_file_path = os.path.join(\n",
    "    processed_data_dir,\n",
    "    os.path.splitext(os.path.basename(proc_file_test))[0].replace(\"test_nlp\", \"metrics\")\n",
    "    + f\"__batch_{test_current_batch_num}.parquet.gzip\",\n",
    ")\n",
    "metrics_fname = os.path.basename(metrics_file_path)\n",
    "print(metrics_file_path)\n",
    "print(metrics_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "abe2d4b7-aa71-4199-b0f0-5bcad0d13093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to parquet file metrics__inference_starts_20220110_000000__batch_1.parquet.gzip...\n",
      "Done.\n",
      "CPU times: user 95 ms, sys: 4.01 ms, total: 99 ms\n",
      "Wall time: 511 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if upload_to_s3:\n",
    "    storage_options = {\n",
    "        \"key\": os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        \"secret\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    }\n",
    "    prefix = f\"{path_to_folder[1:]}processed/nlp_splits/{metrics_fname}\"\n",
    "    metrics_filepath = f\"s3://{s3_bucket_name}/{prefix}\"\n",
    "else:\n",
    "    storage_options = None\n",
    "    split_preds_filepath = metrics_file_path\n",
    "save_to_parquet(df_summary, metrics_filepath, storage_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa40d43-5097-4fe6-bb80-8cd252319bf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine-Tuned Model to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14acca07-745e-4e58-9c08-925b3eeb1b57",
   "metadata": {},
   "source": [
    "Export the fine-tuned model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d482e7d5-1686-428f-9fc2-9a8dd7918b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../model-fine-tuned\n",
      "Configuration saved in ../model-fine-tuned/config.json\n",
      "Model weights saved in ../model-fine-tuned/pytorch_model.bin\n",
      "tokenizer config file saved in ../model-fine-tuned/tokenizer_config.json\n",
      "Special tokens file saved in ../model-fine-tuned/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a3c5c7-b63a-44b3-a4e3-7dd7fe086d0c",
   "metadata": {},
   "source": [
    "### Test Split with Fine-Tuned Model Predictions and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f4c469a3-fe32-4ca2-8e23-0fa2dec21565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/processed/test_nlp__inference_starts_20220110_000000__batch_1__with_preds.parquet.gzip\n",
      "test_nlp__inference_starts_20220110_000000__batch_1__with_preds.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "test_split_with_preds_path = os.path.join(\n",
    "    processed_data_dir,\n",
    "    os.path.splitext(os.path.basename(proc_file_test))[0]\n",
    "    + f\"__batch_{test_current_batch_num}__with_preds.parquet.gzip\",\n",
    ")\n",
    "test_split_with_preds_fname = os.path.basename(test_split_with_preds_path)\n",
    "print(test_split_with_preds_path)\n",
    "print(test_split_with_preds_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d60e8c41-8d15-49dc-9e4f-6d1487a5b56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to parquet file test_nlp__inference_starts_20220110_000000__batch_1__with_preds.parquet.gzip...\n",
      "Done.\n",
      "CPU times: user 6.88 ms, sys: 7.98 ms, total: 14.9 ms\n",
      "Wall time: 221 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if upload_to_s3:\n",
    "    storage_options = {\n",
    "        \"key\": os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        \"secret\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    }\n",
    "    prefix = f\"{path_to_folder[1:]}processed/nlp_splits/{test_split_with_preds_fname}\"\n",
    "    split_preds_filepath = f\"s3://{s3_bucket_name}/{prefix}\"\n",
    "else:\n",
    "    storage_options = None\n",
    "    split_preds_filepath = test_split_with_preds_path\n",
    "save_to_parquet(df_test, split_preds_filepath, storage_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1329892e-f2e0-4106-81e8-a997969d8b81",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c75075-d3f1-4832-a8ef-18c72c7dbc1e",
   "metadata": {},
   "source": [
    "<span style=\"float:left;\">\n",
    "    <a href=\"./6-split-data/notebooks/6_split_data.ipynb\"><< 6 - Create Data Splits for Model Training and Business Metrics</a>\n",
    "</span>\n",
    "\n",
    "<span style=\"float:right;\">\n",
    "    <a href=\"./8-assess/notebooks/8_assess.ipynb\">8 - Assess Business Metrics on Test Split >></a>\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
